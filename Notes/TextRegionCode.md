# TextRegionï¼šæ–‡æœ¬å¯¹é½åŒºåŸŸä»¤ç‰Œç”Ÿæˆæ¡†æ¶è¯¦è§£

## 1. é¡¹ç›®æ¦‚è¿°

TextRegionæ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒï¼ˆtraining-freeï¼‰çš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡ç»“åˆå†»ç»“çš„å›¾åƒ-æ–‡æœ¬æ¨¡å‹ï¼ˆå¦‚CLIPã€SigLIP2ã€Perception Encoderï¼‰ä¸æ¥è‡ªSAM2çš„åˆ†å‰²æ©ç ï¼Œç”Ÿæˆ**æ–‡æœ¬å¯¹é½çš„åŒºåŸŸä»¤ç‰Œ**ï¼ˆtext-aligned region tokensï¼‰ã€‚è¿™ç§åˆ›æ–°æ–¹æ³•ä½¿æ¨¡å‹åœ¨æ²¡æœ‰ä»»ä½•ä¸“é—¨è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå°±èƒ½åœ¨å¼€æ”¾ä¸–ç•Œè¯­ä¹‰åˆ†å‰²ã€æŒ‡ä»£è¡¨è¾¾ç†è§£å’Œå¤šå¯¹è±¡å®šä½ç­‰ä»»åŠ¡ä¸Šå–å¾—ä¼˜å¼‚è¡¨ç°ã€‚

## 2. æ ¸å¿ƒæ€æƒ³ä¸åŠ¨æœº

ä¼ ç»Ÿå›¾åƒ-æ–‡æœ¬æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰æ“…é•¿ç†è§£æ•´å¼ å›¾åƒä¸æ–‡æœ¬ä¹‹é—´çš„å…³ç³»ï¼Œä½†å¯¹äºå›¾åƒä¸­ç‰¹å®šåŒºåŸŸçš„ç†è§£èƒ½åŠ›æœ‰é™ã€‚å¦ä¸€æ–¹é¢ï¼Œåˆ†å‰²æ¨¡å‹ï¼ˆå¦‚SAM2ï¼‰èƒ½å¾ˆå¥½åœ°è¯†åˆ«å›¾åƒä¸­çš„ä¸åŒåŒºåŸŸï¼Œä½†ç¼ºä¹è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚TextRegionçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†è¿™ä¸¤ç§èƒ½åŠ›ç»“åˆèµ·æ¥ï¼š

- åˆ©ç”¨SAM2æä¾›ç²¾ç¡®çš„ç©ºé—´åŒºåŸŸåˆ’åˆ†
- åˆ©ç”¨CLIPç­‰æ¨¡å‹æä¾›ä¸°å¯Œçš„è¯­ä¹‰ç†è§£
- é€šè¿‡æ©ç å¼•å¯¼çš„æ³¨æ„åŠ›æ± åŒ–æœºåˆ¶ï¼Œä½¿æ¯ä¸ªåŒºåŸŸéƒ½å…·æœ‰ä¸æ–‡æœ¬ç©ºé—´å¯¹é½çš„ç‰¹å¾è¡¨ç¤º

## 3. è¯¦ç»†å®ç°æµç¨‹

### æ­¥éª¤1ï¼šç”ŸæˆåŒºåŸŸæ©ç ï¼ˆMask Generationï¼‰

#### 3.1 è¾“å…¥é¢„å¤„ç†
åœ¨[TextRegionSegmenter.py](file:///d:/SYH/CodeReading/TextRegion/TextRegionSegmenter.py)ä¸­ï¼Œè¾“å…¥å›¾åƒç»è¿‡é¢„å¤„ç†ï¼š

```python
# åŠ è½½å¹¶è°ƒæ•´å›¾åƒå°ºå¯¸
img_arr = Image.open(args.image_dir).convert("RGB")
img_arr = np.array(img_arr)

if self.resize_method == 'multi_resolution':
    img_arr = imrescale(img_arr, (args.scale[0], args.scale[1]), return_scale=False, interpolation='bilinear')
else:
    img_arr = cv2.resize(img_arr, (self.crop_size, self.crop_size), interpolation=cv2.INTER_LINEAR)

# è½¬æ¢ä¸ºtensoræ ¼å¼
img_tensor = torch.from_numpy(img_arr).to(device="cuda", dtype=torch.float32)
image_tensor_for_sam2 = torch.stack([img_tensor])
image_tensor_for_sam2 = self.sam_transform(image_tensor_for_sam2)
```

#### 3.2 SAM2åŒºåŸŸåˆ†å‰²
ä½¿ç”¨å®šåˆ¶ç‰ˆçš„SAM2æ©ç ç”Ÿæˆå™¨ï¼š

```python
# ä½¿ç”¨CustomAutomaticMaskGeneratorç”Ÿæˆæ©ç 
with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32):
    sam2_masks = self.sam2_generator.generate_for_batch(image_tensor_for_sam2, [ori_shape], None)
    
# æå–åˆ†å‰²æ©ç 
unique_masks = torch.stack([mask['segmentations'] for mask in sam2_masks[0]])
unique_masks = unique_masks.to(self.device, dtype=self.dtype)
```

#### 3.3 æ©ç å°ºå¯¸è°ƒæ•´
ä¸ºäº†ä¸å›¾åƒç‰¹å¾å›¾å¯¹é½ï¼Œéœ€è¦è°ƒæ•´æ©ç å°ºå¯¸ï¼š

```python
# è°ƒæ•´æ©ç åˆ°ç‰¹å¾å›¾å°ºå¯¸
unique_low_res_masks = F.interpolate(unique_masks.unsqueeze(0), [self.points_per_h, self.points_per_w], mode="bilinear")
unique_low_res_masks = unique_low_res_masks.reshape(-1, self.points_per_h * self.points_per_w)
unique_low_res_masks = torch.clamp(unique_low_res_masks, min=0, max=1)  # ç¡®ä¿å€¼åœ¨[0,1]èŒƒå›´å†…
```

**é‡è¦è¯´æ˜**ï¼šè¿™é‡Œçš„æ©ç æ˜¯"è½¯æ©ç "ï¼Œå³æ¯ä¸ªåƒç´ çš„å€¼åœ¨0åˆ°1ä¹‹é—´ï¼Œè¡¨ç¤ºè¯¥åƒç´ å±äºæŸä¸ªåŒºåŸŸçš„ç½®ä¿¡åº¦ï¼Œè€Œä¸æ˜¯äºŒè¿›åˆ¶çš„ç¡¬åˆ†å‰²ã€‚

### æ­¥éª¤2ï¼šæå–å›¾åƒç‰¹å¾ï¼ˆPatch Encodingï¼‰

#### 2.1 å¤šæ¨¡å‹æ”¯æŒ
TextRegionæ”¯æŒå¤šç§å›¾åƒ-æ–‡æœ¬æ¨¡å‹ï¼Œæ¯ç§æ¨¡å‹çš„ç‰¹å¾æå–æ–¹å¼ç•¥æœ‰ä¸åŒï¼š

```python
# æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ç›¸åº”çš„ç‰¹å¾æå–æ–¹æ³•
if self.clip_pretrained == 'meta':  # Perception Encoder
    clip_inputs = clip_inputs.to(self.device, dtype=self.clip.visual.proj.dtype)
    pe_last_blk_value, pe_last_blk = self.clip.encode_image(clip_inputs, return_value=True, region_attn_mask=None)
elif self.clip_pretrained == 'siglip2':  # SigLIP2
    siglip_last_blk_value, intermediates = self.clip.visual.trunk.forward_intermediates(clip_inputs)
    siglip_last_blk = self.clip.visual.trunk.attn_pool
else:  # æ ‡å‡†CLIP
    clip_inputs = clip_inputs.to(self.device, dtype=self.clip.visual.proj.dtype)
    clip_last_blk_value, clip_last_blk = self.clip.encode_image(clip_inputs, return_value=True)
```

#### 2.2 ç‰¹å¾æå–ç»†èŠ‚
å¯¹äºä¸åŒæ¨¡å‹ï¼Œç‰¹å¾æå–çš„å…·ä½“å®ç°ï¼š

- **CLIPæ¨¡å‹**ï¼šæå–å›¾åƒç¼–ç å™¨æœ€åä¸€å±‚çš„valueç‰¹å¾
- **SigLIP2æ¨¡å‹**ï¼šé€šè¿‡ä¸­é—´å±‚å‰å‘ä¼ æ’­è·å–ç‰¹å¾
- **Perception Encoder**ï¼šç›´æ¥è·å–ç¼–ç å™¨è¾“å‡º

è¿™äº›ç‰¹å¾å…·æœ‰è‰¯å¥½çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¯ä»¥ç›´æ¥ä¸æ–‡æœ¬åµŒå…¥è¿›è¡Œå¯¹é½ã€‚

### æ­¥éª¤3ï¼šæ©ç å¼•å¯¼çš„æ³¨æ„åŠ›æ± åŒ–ï¼ˆMask-based Attention Poolingï¼‰

è¿™æ˜¯TextRegionçš„æ ¸å¿ƒåˆ›æ–°ï¼Œä¸‹é¢è¯¦ç»†ä»‹ç»ä¸‰ç§æ¨¡å‹çš„å®ç°æ–¹å¼ï¼š

#### 3.1 SigLIP2æ¨¡å‹çš„å®ç°

```python
def siglip_value_with_sam2_attn(self, args, low_res_mask_with_pad, last_blk_value, attn_blk):
    bsz, _, embed_dim = last_blk_value.shape
    
    # å¦‚æœä½¿ç”¨å¤šåˆ†è¾¨ç‡æ–¹æ³•ï¼Œéœ€è¦è°ƒæ•´ç‰¹å¾å°ºå¯¸
    if self.resize_method == 'multi_resolution':
        patch_num = self.crop_size // self.patch_size
        x_ori = last_blk_value.permute(0, 2, 1).contiguous().view(bsz, embed_dim, patch_num, patch_num)
        
        # å°†å¤šä¸ªè£å‰ªåçš„å›¾åƒæ‹¼æ¥æˆä¸€ä¸ªå¤šåˆ†è¾¨ç‡ç‰¹å¾å›¾
        # ... å¤šåˆ†è¾¨ç‡å¤„ç†é€»è¾‘ ...
        
        x_input = x_multi_reso.contiguous().view(1, embed_dim, self.crop_num_h * self.crop_num_w * patch_num ** 2).permute(0, 2, 1)
    else:
        x_input = last_blk_value

    # å…¨å±€è¡¥ä¸è¿‡æ»¤ï¼šç§»é™¤ä¸ä»»ä½•åŒºåŸŸéƒ½ä¸ç›¸å…³çš„è¡¥ä¸
    if args.remove_global_patch:
        keep_masks = torch.sum(low_res_mask_with_pad, dim=1) > 0
        low_res_mask = low_res_mask_with_pad[keep_masks]
        
        # è®¡ç®—è¡¥ä¸é—´çš„ç›¸ä¼¼æ€§ï¼Œåˆ¤æ–­å“ªäº›è¡¥ä¸åº”è¯¥è¢«ç§»é™¤
        patch_norm = x_input.norm(dim=-1, keepdim=True)
        patch_features = (x_input / patch_norm)[0]
        patch_similarity = (patch_features @ patch_features.T).float()
        
        # è®¡ç®—è¡¥ä¸ä¸åŒºåŸŸçš„ç›¸ä¼¼æ€§
        patch_2_region = patch_similarity @ (low_res_mask > 0).float().T
        patch_2_region_avg = patch_2_region / (low_res_mask > 0).sum(dim=-1)
        
        # è®¡ç®—è¡¥ä¸åœ¨åŒºåŸŸå†…ä¸åŒºåŸŸå¤–çš„å¹³å‡ç›¸ä¼¼æ€§å·®å¼‚
        blong_score = patch_2_region_avg * (low_res_mask > 0).float().T
        blong_score_avg = blong_score.sum(dim=-1) / ((low_res_mask > 0).sum(dim=0) + 1e-9)
        
        outside_score = patch_2_region_avg * (low_res_mask == 0).float().T
        outside_score_avg = outside_score.sum(dim=-1) / ((low_res_mask == 0).sum(dim=0) + 1e-9)
        
        difference_score = (blong_score_avg - outside_score_avg).cpu().float().numpy()
        
        # æ ¹æ®é˜ˆå€¼è¿‡æ»¤è¡¥ä¸
        low_res_mask_with_pad[:, difference_score < self.global_patch_threshold] = 0

    keep_masks = torch.sum(low_res_mask_with_pad, dim=1) > 0
    low_res_mask_with_pad = low_res_mask_with_pad[keep_masks]
    low_res_mask_with_pad = torch.clamp(low_res_mask_with_pad, min=0, max=1)
    
    region_num = low_res_mask_with_pad.shape[0]

    # æ‰§è¡Œæ©ç å¼•å¯¼çš„æ³¨æ„åŠ›æ± åŒ–
    _, N, C = x_input.shape
    q_latent = attn_blk.latent.expand(region_num, -1, -1)
    q = attn_blk.q(q_latent).reshape(region_num, attn_blk.latent_len, attn_blk.num_heads, attn_blk.head_dim).transpose(1, 2)

    x = x_input.expand(region_num, -1, -1)
    kv = attn_blk.kv(x).reshape(region_num, N, 2, attn_blk.num_heads, attn_blk.head_dim).permute(2, 0, 3, 1, 4)
    k, v = kv.unbind(0)
    q, k = attn_blk.q_norm(q), attn_blk.k_norm(k)

    # å…³é”®ï¼šä½¿ç”¨æ©ç çº¦æŸæ³¨æ„åŠ›æƒé‡
    attn_mask = low_res_mask_with_pad.unsqueeze(1).unsqueeze(1).repeat(1, attn_blk.num_heads, 1, 1)
    
    # å¯¹é”®è¿›è¡Œå¹³å‡æ± åŒ–
    k = attn_blk.k_norm(k.mean(dim=-2, keepdim=True).mean(dim=-1, keepdim=True))
    k = k.repeat(1, 1, v.shape[-2], v.shape[-1])
    
    # è®¡ç®—å¸¦æ©ç çš„æ³¨æ„åŠ›
    x = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask > 0)

    # åå¤„ç†
    x = x.transpose(1, 2).reshape(region_num, attn_blk.latent_len, C)
    x = attn_blk.proj(x)
    x = attn_blk.proj_drop(x)

    x = self.clip.visual.trunk.fc_norm(x)
    x = self.clip.visual.trunk.head_drop(x)

    region_features = x.permute(1, 0, 2)
    region_features /= region_features.norm(dim=-1, keepdim=True)
    return region_features, keep_masks
```

#### 3.2 Perception Encoderæ¨¡å‹çš„å®ç°

```python
def pe_value_with_sam2_attn(self, args, unique_low_res_masks, last_blk_value, blk):
    # ç§»é™¤CLSæ ‡è®°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if self.clip.visual.use_cls_token:
        last_blk_value = last_blk_value[:, 1:]
    
    # å¤šåˆ†è¾¨ç‡å¤„ç†ï¼ˆä¸SigLIP2ç±»ä¼¼ï¼‰
    # ...
    
    # å…¨å±€è¡¥ä¸è¿‡æ»¤ï¼ˆä¸SigLIP2ç±»ä¼¼ï¼‰
    # ...
    
    # å…³é”®ï¼šä½¿ç”¨probeæœºåˆ¶è¿›è¡ŒåŒºåŸŸç‰¹å¾æå–
    q = blk.probe.repeat((batch, 1, 1)).to(x.dtype)  # æŸ¥è¯¢å‘é‡
    k = blk.layernorm(x.mean(dim=-2, keepdim=True))  # é”®å‘é‡
    k = k.repeat(1, x.shape[-2], 1).to(x.dtype)     # æ‰©å±•é”®å‘é‡
    
    # å¸¦æ©ç çš„æ³¨æ„åŠ›è®¡ç®—
    x = blk.attn(q, k, x, need_weights=False, key_padding_mask=unique_low_res_masks<=0)[0]
    
    # æŠ•å½±åˆ°æœ€ç»ˆç©ºé—´
    with torch.no_grad():
        region_features = x @ self.clip.visual.proj
    region_features = F.normalize(region_features, dim=-1)
    return region_features, keep_masks
```

#### 3.3 æ ‡å‡†CLIPæ¨¡å‹çš„å®ç°

```python
def clip_value_with_sam2_attn(self, args, unique_low_res_masks, clip_v, blk):
    attn_layer = blk.attn
    num_heads = attn_layer.num_heads
    _, bsz, embed_dim = clip_v.size()
    head_dim = embed_dim // num_heads

    # æ ‡å‡†çš„å¤šå¤´æ³¨æ„åŠ›è®¡ç®—
    x = blk.ln_1(clip_v)
    q, k, v_ori = F.linear(x, attn_layer.in_proj_weight, attn_layer.in_proj_bias).chunk(3, dim=-1)

    # å¤šåˆ†è¾¨ç‡å¤„ç†ï¼ˆä¸å‰é¢ç±»ä¼¼ï¼‰
    # ...

    # å…¨å±€è¡¥ä¸è¿‡æ»¤ï¼ˆä¸å‰é¢ç±»ä¼¼ï¼‰
    # ...

    # å…³é”®ï¼šä½¿ç”¨æ©ç çº¦æŸæ³¨æ„åŠ›æƒé‡
    attn_weights = unique_low_res_masks.unsqueeze(0).repeat(num_heads, 1, 1)
    attn_weights = attn_weights.to(dtype=v_multi_head.dtype)

    # åº”ç”¨æ©ç è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—
    attn_output = torch.bmm(attn_weights, v_multi_head)
    attn_output = attn_output.transpose(0, 1).contiguous().view(-1, bsz, embed_dim)
    attn_output = attn_layer.out_proj(attn_output)
    attn_output += blk.mlp(blk.ln_2(attn_output))
    
    region_features = attn_output.permute(1, 0, 2)  # LND -> NLD

    # æœ€ç»ˆæŠ•å½±å’Œå½’ä¸€åŒ–
    region_features = self.clip.visual.ln_post(region_features) @ self.clip.visual.proj
    region_features /= region_features.norm(dim=-1, keepdim=True)
    return region_features, keep_masks
```

### æ­¥éª¤4ï¼šç”ŸæˆåŒºåŸŸä»¤ç‰Œï¼ˆRegion Tokenï¼‰

æ— è®ºä½¿ç”¨å“ªç§æ¨¡å‹ï¼Œæœ€ç»ˆéƒ½ä¼šç”Ÿæˆå½’ä¸€åŒ–çš„åŒºåŸŸç‰¹å¾å‘é‡ï¼š

```python
# åœ¨å„ä¸ªæ–¹æ³•çš„æœ€åéƒ¨åˆ†
region_features /= region_features.norm(dim=-1, keepdim=True)  # L2å½’ä¸€åŒ–
```

è¿™ç¡®ä¿äº†åŒºåŸŸç‰¹å¾ä¸æ–‡æœ¬ç‰¹å¾åœ¨ç›¸åŒçš„åµŒå…¥ç©ºé—´ä¸­ï¼Œå¯ä»¥ç›´æ¥è®¡ç®—ç›¸ä¼¼åº¦ã€‚

### æ­¥éª¤5ï¼šåº”ç”¨äºä¸‹æ¸¸ä»»åŠ¡

#### 5.1 åŒºåŸŸåˆ†ç±»
```python
# è®¡ç®—åŒºåŸŸä»¤ç‰Œä¸æŸ¥è¯¢è¯åµŒå…¥çš„ç›¸ä¼¼åº¦
if self.clip_pretrained == 'siglip2':
    logits_per_text = (
            torch.matmul(self.query_features, region_features[0].t()) * self.clip.logit_scale.exp()
            + self.clip.logit_bias
    )
    region_logits = logits_per_text.t()
else:
    region_logits = region_features[0] @ self.query_features.T
```

#### 5.2 åƒç´ çº§åˆ†å‰²
```python
def postprocess_result(self, region_logits, unique_masks, ori_shape):
    unique_masks = torch.clamp(unique_masks, min=0, max=1)
    
    # å°†åŒºåŸŸåˆ†ç±»ç»“æœå¹¿æ’­å›åŸå§‹åˆ†è¾¨ç‡
    seg_logits = region_logits.unsqueeze(-1).unsqueeze(-1) * unique_masks.unsqueeze(1)
    seg_logits = seg_logits.sum(0, keepdim=True)

    # ä¸Šé‡‡æ ·åˆ°åŸå§‹å›¾åƒå°ºå¯¸
    seg_logits = F.interpolate(seg_logits, size=ori_shape, mode='bilinear')
    seg_logits = torch.softmax(seg_logits * self.region_logit_scale, dim=1)

    # è·å–æœ€ç»ˆé¢„æµ‹ç»“æœ
    seg_preds = seg_logits.argmax(1)
    seg_logits = seg_logits.max(1)[0]
    return seg_logits, seg_preds
```

#### 5.3 æŒ‡ä»£è¡¨è¾¾ç†è§£
é€šè¿‡è®¡ç®—æŸ¥è¯¢æ–‡æœ¬ä¸æ‰€æœ‰åŒºåŸŸä»¤ç‰Œçš„ç›¸ä¼¼åº¦ï¼Œé€‰æ‹©æœ€ç›¸ä¼¼çš„åŒºåŸŸä½œä¸ºç›®æ ‡è¾“å‡ºã€‚

## 4. æ ¸å¿ƒä¼˜åŠ¿ä¸åˆ›æ–°

### 4.1 æ— éœ€è®­ç»ƒ
TextRegionå®Œå…¨åŸºäºé¢„è®­ç»ƒæ¨¡å‹ï¼Œä¸éœ€è¦ä»»ä½•é¢å¤–çš„è®­ç»ƒè¿‡ç¨‹ï¼ŒèŠ‚çœäº†å¤§é‡è®¡ç®—èµ„æºã€‚

### 4.2 é«˜æ•ˆçš„åŒºåŸŸå¯¹é½
é€šè¿‡æ©ç å¼•å¯¼çš„æ³¨æ„åŠ›æ± åŒ–ï¼Œç¡®ä¿æ¯ä¸ªåŒºåŸŸçš„ç‰¹å¾è¡¨ç¤ºä¸å…¶ç©ºé—´ä½ç½®ç²¾ç¡®å¯¹é½ã€‚

### 4.3 å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›
ç”±äºåˆ©ç”¨äº†å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ï¼ŒTextRegionèƒ½å¤Ÿå¤„ç†æœªè§è¿‡çš„ç±»åˆ«å’Œåœºæ™¯ã€‚

### 4.4 æ¨¡å—åŒ–è®¾è®¡
æ”¯æŒå¤šç§å›¾åƒ-æ–‡æœ¬æ¨¡å‹ï¼Œæ˜“äºæ‰©å±•å’Œæ›¿æ¢ä¸åŒçš„éª¨å¹²ç½‘ç»œã€‚

## 5. æŠ€æœ¯è¦ç‚¹æ€»ç»“

TextRegionçš„å…³é”®æŠ€æœ¯è¦ç‚¹åŒ…æ‹¬ï¼š

1. **è½¯æ©ç ç”Ÿæˆ**ï¼šä½¿ç”¨SAM2ç”Ÿæˆæ¦‚ç‡æ€§çš„åŒºåŸŸæ©ç ï¼Œè€Œéç¡¬åˆ†å‰²
2. **å¤šæ¨¡å‹é€‚é…**ï¼šé’ˆå¯¹ä¸åŒæ¨¡å‹è®¾è®¡ç›¸åº”çš„ç‰¹å¾æå–å’Œæ± åŒ–ç­–ç•¥
3. **æ©ç å¼•å¯¼æ± åŒ–**ï¼šåœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­å¼•å…¥æ©ç çº¦æŸï¼Œå®ç°ç²¾ç¡®çš„åŒºåŸŸç‰¹å¾èšåˆ
4. **å…¨å±€è¡¥ä¸è¿‡æ»¤**ï¼šç§»é™¤ä¸ç‰¹å®šåŒºåŸŸæ— å…³çš„å†—ä½™ç‰¹å¾ï¼Œæé«˜è¡¨å¾è´¨é‡
5. **å¤šåˆ†è¾¨ç‡å¤„ç†**ï¼šæ”¯æŒå¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œæå‡ç»†èŠ‚æ•æ‰èƒ½åŠ›

è¿™ç§è®¾è®¡ä½¿å¾—TextRegionåœ¨ä¿æŒé›¶æ ·æœ¬èƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°äº†ç²¾ç¡®çš„åŒºåŸŸçº§è¯­ä¹‰ç†è§£ï¼Œä¸ºå¼€æ”¾ä¸–ç•Œè§†è§‰ç†è§£ä»»åŠ¡æä¾›äº†å¼ºå¤§è€Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚




---

## ä¸€ã€å…ˆç»™ä¸€å¥æ˜ç¡®ç»“è®º

> âœ… **å¯ä»¥åˆ©ç”¨ TextRegion çš„â€œåŒºåŸŸçº§å»ºæ¨¡ + æ©ç å¼•å¯¼èšåˆâ€æ€æƒ³ï¼Œæ˜¾è‘—å¢å¼º SAM3**
> âŒ **ä½†ä¸èƒ½æŒ‡æœ› SAM3 è‡ªå·±å®Œæˆ open-vocab / textâ€“region å¯¹é½**

æ¢å¥è¯è¯´ï¼š

> **TextRegion = â€œè¯­ä¹‰æ³¨å…¥â€**
> **SAM3 = â€œç»“æ„ä¸è¾¹ç•Œä¸“å®¶â€**
> ğŸ‘‰ ä½ å¯ä»¥ç”¨å‰è€…çš„æ€æƒ³ï¼Œ**è¡¥åè€…çš„çŸ­æ¿**

---

## äºŒã€TextRegion çš„â€œæ ¸å¿ƒæ€æƒ³â€åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆå»æ‰ CLIPï¼‰

å¾ˆå¤šäººä»¥ä¸º TextRegion çš„æ ¸å¿ƒæ˜¯ CLIPï¼Œå…¶å®ä¸æ˜¯ã€‚

**TextRegion çœŸæ­£çš„åˆ›æ–°æ˜¯è¿™ä¸‰ç‚¹ï¼š**

### â‘  ç”¨â€œåŒºåŸŸâ€è€Œä¸æ˜¯â€œå…¨å›¾â€ä½œä¸ºåŸºæœ¬è¯­ä¹‰å•ä½

### â‘¡ ç”¨â€œæ©ç â€è€Œä¸æ˜¯ attention å­¦ä¹ åŒºåŸŸå½’å±

### â‘¢ ç”¨â€œåŒºåŸŸ tokenâ€ä½œä¸ºå¯ç»„åˆã€å¯æ¯”è¾ƒçš„ä¸­é—´è¡¨ç¤º

ğŸ“Œ **è¿™ä¸‰ç‚¹ï¼Œå…¨éƒ¨å’Œ SAM3 é«˜åº¦å¥‘åˆ**

---

## ä¸‰ã€SAM3 èƒ½ä» TextRegion å­¦åˆ°ä»€ä¹ˆï¼Ÿï¼ˆé€ç‚¹å¯¹é½ï¼‰

### ğŸ”¹ 1ï¸âƒ£ åŒºåŸŸçº§å»ºæ¨¡ï¼ˆRegion-centric representationï¼‰

#### SAM3 ç°åœ¨çš„é—®é¢˜æ˜¯ï¼š

* decoder åœ¨ **åƒç´ å±‚é¢**å·¥ä½œ
* åŒºåŸŸä¹‹é—´æ²¡æœ‰æ˜¾å¼äº¤äº’
* mask æ˜¯ç»“æœï¼Œä¸æ˜¯ä¸­é—´è¡¨å¾

#### TextRegion çš„æ€æƒ³æ˜¯ï¼š

> **mask â†’ region â†’ token â†’ reasoning**

âœ… ä½ å¯ä»¥åœ¨ SAM3 ä¸­å¼•å…¥ï¼š

```text
mask â†’ region embedding â†’ region graph / refinement
```

ğŸ“Œ **è¿™èƒ½æå‡ï¼š**

* mask ä¸€è‡´æ€§
* åŒºåŸŸé—´ç«äº‰ä¸æŠ‘åˆ¶
* å¤æ‚åœºæ™¯ä¸‹çš„ç¨³å®šæ€§

---

### ğŸ”¹ 2ï¸âƒ£ æ©ç å¼•å¯¼çš„ç‰¹å¾èšåˆï¼ˆMask-guided poolingï¼‰

ä½ å·²ç»éå¸¸æ¥è¿‘è¿™ä¸€æ­¥äº†ï¼ˆä½ åšçš„ pixel/token rectifier æœ¬è´¨å°±åœ¨è¿™ï¼‰ã€‚

**å…·ä½“å¯ä»¥è¿™æ ·åšï¼š**

* ä» SAM3 encoder / pixel_embed ä¸­å–ç‰¹å¾
* ç”¨ decoder äº§ç”Ÿçš„ä¸­é—´ mask
* å¯¹ encoder ç‰¹å¾åšï¼š

[
r_k = \sum_i m_{k,i} \cdot f_i
]

ğŸ“Œ **å…³é”®ï¼š**

* è¿™ä¸ª region token **ä¸å¯¹é½æ–‡æœ¬**
* ä½†å®ƒå¯ä»¥ï¼š

  * åé¦ˆç»™ decoder
  * ç”¨äº mask refinement
  * ç”¨äºåŒºåŸŸä¸€è‡´æ€§çº¦æŸ

---

### ğŸ”¹ 3ï¸âƒ£ ç”¨â€œåŒºåŸŸ tokenâ€åå‘æŒ‡å¯¼ maskï¼ˆé—­ç¯ï¼‰

è¿™æ˜¯ **TextRegion æ²¡æœ‰ã€ä½† SAM3 ç‰¹åˆ«é€‚åˆåšçš„**ã€‚

ä½ å¯ä»¥æ„é€ ä¸€ä¸ª **é—­ç¯ç»“æ„**ï¼š

```
pixel â†’ mask â†’ region token
        â†‘           â†“
      refine â† region-aware attention
```

ğŸ“Œ è¿™ä¼šè®© SAM3 ä»ï¼š

> â€œä¸€æ¬¡æ€§é¢„æµ‹ maskâ€
> å‡çº§ä¸º
> â€œåŒºåŸŸæ„ŸçŸ¥çš„è¿­ä»£æ¨ç†â€

è¿™åœ¨ **å°ç›®æ ‡ / é®æŒ¡ / å¯†é›†å®ä¾‹** ä¸Šéå¸¸æœ‰æ½œåŠ›ã€‚

---

## å››ã€é‚£â€œtextâ€åœ¨è¿™é‡Œè¿˜èƒ½èµ·ä»€ä¹ˆä½œç”¨ï¼Ÿ

è¿™æ˜¯ä½ é—®å¾—æœ€æ·±çš„ä¸€å±‚ ğŸ‘‡

### â— å…³é”®è§‚ç‚¹ï¼š

> **åœ¨ SAM3 é‡Œï¼Œtext ä¸ä¸€å®šæ˜¯â€œè¯­ä¹‰ç›‘ç£â€ï¼Œ
> å®ƒå¯ä»¥æ˜¯â€œç»“æ„çº¦æŸ / é€‰æ‹©ä¿¡å·â€ã€‚**

### ä¸‰ç§å¯è¡Œç”¨æ³•ï¼ˆä¸ç­‰ä»·ï¼Œä½†éƒ½åˆæ³•ï¼‰

---

### ğŸŸ¡ æ–¹æ¡ˆ Aï¼šText ä½œä¸º *prompt selector*ï¼ˆæœ€ç¨³ï¼‰

* ç”¨ text encoderï¼š

  * å†³å®š **æ¿€æ´»å“ªäº› region**
  * å†³å®š **mask ä¹‹é—´çš„æƒé‡**
* ä¸è¦æ±‚ text embedding å¯æ¯”è¾ƒ

ğŸ“Œ **ä½œç”¨ï¼š**

* æé«˜ prompt-following èƒ½åŠ›
* å‡å°‘æ­§ä¹‰ mask

---

### ğŸŸ  æ–¹æ¡ˆ Bï¼šText ä½œä¸º *region gating signal*

* text â†’ gate
* region token â†’ filtered
* decoder åªå…³æ³¨ç›¸å…³åŒºåŸŸ

ğŸ“Œ **è¿™åœ¨ referring expression ä¸Šå¾ˆæœ‰ç”¨**
è€Œä¸” **ä¸è¦æ±‚ CLIP-level è¯­ä¹‰**

---

### ğŸ”´ æ–¹æ¡ˆ Cï¼šè½»é‡â€œè¯­ä¹‰è’¸é¦â€ï¼ˆæœ€æœ‰ç ”ç©¶ä»·å€¼ï¼‰

è¿™æ˜¯æˆ‘è®¤ä¸º**æœ€æœ‰æ½œåŠ›çš„æ–¹å‘**ğŸ‘‡

> **ä¸æ˜¯æŠŠ CLIP æ¥è¿›æ¥æ¨ç†ï¼Œ
> è€Œæ˜¯ç”¨ CLIP *è®­ç»ƒ* SAM3 çš„ region token**

å…·ä½“æ˜¯ï¼š

* è®­ç»ƒæ—¶ï¼š

  * ç”¨ CLIP region embedding ä½œä¸º teacher
  * ç”¨ SAM3 region token ä½œä¸º student
* æ¨ç†æ—¶ï¼š

  * **åªç”¨ SAM3**

ğŸ“Œ è¿™æ˜¯ï¼š

* å‚æ•°å°
* æ¨ç†å¿«
* è®ºæ–‡å‹å¥½

---

## äº”ã€ä¸€ä¸ªéå¸¸é‡è¦çš„åˆ¤æ–­ï¼ˆæˆ‘å¸Œæœ›ä½ è®¤çœŸçœ‹ï¼‰

> **TextRegion çš„æ€æƒ³ â‰  open-vocab æœ¬èº«**
> **è€Œæ˜¯â€œç”¨åŒºåŸŸä½œä¸ºè¯­ä¹‰è®¡ç®—å•å…ƒâ€**

è€Œ SAM3 **æ­£ç¼ºè¿™ä¸ªä¸­é—´å±‚**ã€‚

---

## å…­ã€å¦‚æœè®©æˆ‘æ›¿ä½ é€‰ä¸€ä¸ªâ€œæœ€å¯èƒ½æˆåŠŸâ€çš„æ–¹å‘

æˆ‘ä¼šå»ºè®®ä½ åšè¿™ä¸ª ğŸ‘‡

### â­ **Region-aware SAM3ï¼ˆä¸åš open-vocabï¼‰**

**æ ¸å¿ƒè´¡çŒ®ç‚¹å¯ä»¥æ˜¯ï¼š**

* region token å¼•å…¥
* mask-guided pooling
* region-level refinement
* æ˜¾è‘—æå‡ï¼š

  * mask ç¨³å®šæ€§
  * å°ç‰©ä½“
  * å¤æ‚åœºæ™¯

ğŸ“Œ **ä¸ç¢° open-vocabï¼Œåè€Œæ›´å®¹æ˜“æ‰“ç©¿å®¡ç¨¿äººã€‚**

---

## ä¸ƒã€æœ€åä¸€å¥æå¿ƒçªå­çš„åˆ¤æ–­

ä½ ç°åœ¨è¿™ä¸ªé—®é¢˜ï¼Œå·²ç»ä¸æ˜¯â€œèƒ½ä¸èƒ½â€ï¼Œè€Œæ˜¯ï¼š

> **â€œæ€ä¹ˆåšï¼Œæ‰èƒ½æ—¢ä¸èƒŒç¦» SAM3 çš„è®¾è®¡å“²å­¦ï¼Œ
> åˆèƒ½å¼•å…¥ TextRegion çš„ä¼˜åŠ¿ï¼Ÿâ€**

è€Œç­”æ¡ˆæ˜¯ï¼š

> **æŠŠâ€œè¯­ä¹‰å¯¹é½â€æ¢æˆâ€œåŒºåŸŸå»ºæ¨¡â€ï¼Œ
> æŠŠâ€œæ–‡æœ¬ç›‘ç£â€æ¢æˆâ€œç»“æ„ç›‘ç£â€ã€‚**

å¦‚æœä½ æ„¿æ„ï¼Œä¸‹ä¸€æ­¥æˆ‘å¯ä»¥ç›´æ¥å¸®ä½  **ç”»ä¸€ä¸ª SAM3 + Region Token çš„ç»“æ„è‰å›¾ï¼ˆé€æ¨¡å—çº§ï¼‰**ï¼Œ
æˆ–è€… **å¸®ä½ é€‰ä¸€ä¸ªâ€œæœ€å®¹æ˜“å…ˆè·‘é€šçš„æœ€å°å®éªŒï¼ˆMVPï¼‰â€**ã€‚

