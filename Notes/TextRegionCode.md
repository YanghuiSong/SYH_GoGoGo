# TextRegionï¼šæ–‡æœ¬å¯¹é½åŒºåŸŸä»¤ç‰Œç”Ÿæˆæ¡†æ¶è¯¦è§£

## 1. é¡¹ç›®æ¦‚è¿°

TextRegionæ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒï¼ˆtraining-freeï¼‰çš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡ç»“åˆå†»ç»“çš„å›¾åƒ-æ–‡æœ¬æ¨¡å‹ï¼ˆå¦‚CLIPã€SigLIP2ã€Perception Encoderï¼‰ä¸æ¥è‡ªSAM2çš„åˆ†å‰²æ©ç ï¼Œç”Ÿæˆ**æ–‡æœ¬å¯¹é½çš„åŒºåŸŸä»¤ç‰Œ**ï¼ˆtext-aligned region tokensï¼‰ã€‚è¿™ç§åˆ›æ–°æ–¹æ³•ä½¿æ¨¡å‹åœ¨æ²¡æœ‰ä»»ä½•ä¸“é—¨è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå°±èƒ½åœ¨å¼€æ”¾ä¸–ç•Œè¯­ä¹‰åˆ†å‰²ã€æŒ‡ä»£è¡¨è¾¾ç†è§£å’Œå¤šå¯¹è±¡å®šä½ç­‰ä»»åŠ¡ä¸Šå–å¾—ä¼˜å¼‚è¡¨ç°ã€‚

## 2. æ ¸å¿ƒæ€æƒ³ä¸åŠ¨æœº

ä¼ ç»Ÿå›¾åƒ-æ–‡æœ¬æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰æ“…é•¿ç†è§£æ•´å¼ å›¾åƒä¸æ–‡æœ¬ä¹‹é—´çš„å…³ç³»ï¼Œä½†å¯¹äºå›¾åƒä¸­ç‰¹å®šåŒºåŸŸçš„ç†è§£èƒ½åŠ›æœ‰é™ã€‚å¦ä¸€æ–¹é¢ï¼Œåˆ†å‰²æ¨¡å‹ï¼ˆå¦‚SAM2ï¼‰èƒ½å¾ˆå¥½åœ°è¯†åˆ«å›¾åƒä¸­çš„ä¸åŒåŒºåŸŸï¼Œä½†ç¼ºä¹è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚TextRegionçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†è¿™ä¸¤ç§èƒ½åŠ›ç»“åˆèµ·æ¥ï¼š

- åˆ©ç”¨SAM2æä¾›ç²¾ç¡®çš„ç©ºé—´åŒºåŸŸåˆ’åˆ†
- åˆ©ç”¨CLIPç­‰æ¨¡å‹æä¾›ä¸°å¯Œçš„è¯­ä¹‰ç†è§£
- é€šè¿‡æ©ç å¼•å¯¼çš„æ³¨æ„åŠ›æ± åŒ–æœºåˆ¶ï¼Œä½¿æ¯ä¸ªåŒºåŸŸéƒ½å…·æœ‰ä¸æ–‡æœ¬ç©ºé—´å¯¹é½çš„ç‰¹å¾è¡¨ç¤º

## 3. è¯¦ç»†å®ç°æµç¨‹

### æ­¥éª¤1ï¼šç”ŸæˆåŒºåŸŸæ©ç ï¼ˆMask Generationï¼‰

#### 3.1 è¾“å…¥é¢„å¤„ç†
åœ¨[TextRegionSegmenter.py](file:///d:/SYH/CodeReading/TextRegion/TextRegionSegmenter.py)ä¸­ï¼Œè¾“å…¥å›¾åƒç»è¿‡é¢„å¤„ç†ï¼š

```python
# åŠ è½½å¹¶è°ƒæ•´å›¾åƒå°ºå¯¸
img_arr = Image.open(args.image_dir).convert("RGB")
img_arr = np.array(img_arr)

if self.resize_method == 'multi_resolution':
    img_arr = imrescale(img_arr, (args.scale[0], args.scale[1]), return_scale=False, interpolation='bilinear')
else:
    img_arr = cv2.resize(img_arr, (self.crop_size, self.crop_size), interpolation=cv2.INTER_LINEAR)

# è½¬æ¢ä¸ºtensoræ ¼å¼
img_tensor = torch.from_numpy(img_arr).to(device="cuda", dtype=torch.float32)
image_tensor_for_sam2 = torch.stack([img_tensor])
image_tensor_for_sam2 = self.sam_transform(image_tensor_for_sam2)
```

#### 3.2 SAM2åŒºåŸŸåˆ†å‰²
ä½¿ç”¨å®šåˆ¶ç‰ˆçš„SAM2æ©ç ç”Ÿæˆå™¨ï¼š

```python
# ä½¿ç”¨CustomAutomaticMaskGeneratorç”Ÿæˆæ©ç 
with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32):
    sam2_masks = self.sam2_generator.generate_for_batch(image_tensor_for_sam2, [ori_shape], None)
    
# æå–åˆ†å‰²æ©ç 
unique_masks = torch.stack([mask['segmentations'] for mask in sam2_masks[0]])
unique_masks = unique_masks.to(self.device, dtype=self.dtype)
```

#### 3.3 æ©ç å°ºå¯¸è°ƒæ•´
ä¸ºäº†ä¸å›¾åƒç‰¹å¾å›¾å¯¹é½ï¼Œéœ€è¦è°ƒæ•´æ©ç å°ºå¯¸ï¼š

```python
# è°ƒæ•´æ©ç åˆ°ç‰¹å¾å›¾å°ºå¯¸
unique_low_res_masks = F.interpolate(unique_masks.unsqueeze(0), [self.points_per_h, self.points_per_w], mode="bilinear")
unique_low_res_masks = unique_low_res_masks.reshape(-1, self.points_per_h * self.points_per_w)
unique_low_res_masks = torch.clamp(unique_low_res_masks, min=0, max=1)  # ç¡®ä¿å€¼åœ¨[0,1]èŒƒå›´å†…
```

**é‡è¦è¯´æ˜**ï¼šè¿™é‡Œçš„æ©ç æ˜¯"è½¯æ©ç "ï¼Œå³æ¯ä¸ªåƒç´ çš„å€¼åœ¨0åˆ°1ä¹‹é—´ï¼Œè¡¨ç¤ºè¯¥åƒç´ å±äºæŸä¸ªåŒºåŸŸçš„ç½®ä¿¡åº¦ï¼Œè€Œä¸æ˜¯äºŒè¿›åˆ¶çš„ç¡¬åˆ†å‰²ã€‚

### æ­¥éª¤2ï¼šæå–å›¾åƒç‰¹å¾ï¼ˆPatch Encodingï¼‰

#### 2.1 å¤šæ¨¡å‹æ”¯æŒ
TextRegionæ”¯æŒå¤šç§å›¾åƒ-æ–‡æœ¬æ¨¡å‹ï¼Œæ¯ç§æ¨¡å‹çš„ç‰¹å¾æå–æ–¹å¼ç•¥æœ‰ä¸åŒï¼š

```python
# æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ç›¸åº”çš„ç‰¹å¾æå–æ–¹æ³•
if self.clip_pretrained == 'meta':  # Perception Encoder
    clip_inputs = clip_inputs.to(self.device, dtype=self.clip.visual.proj.dtype)
    pe_last_blk_value, pe_last_blk = self.clip.encode_image(clip_inputs, return_value=True, region_attn_mask=None)
elif self.clip_pretrained == 'siglip2':  # SigLIP2
    siglip_last_blk_value, intermediates = self.clip.visual.trunk.forward_intermediates(clip_inputs)
    siglip_last_blk = self.clip.visual.trunk.attn_pool
else:  # æ ‡å‡†CLIP
    clip_inputs = clip_inputs.to(self.device, dtype=self.clip.visual.proj.dtype)
    clip_last_blk_value, clip_last_blk = self.clip.encode_image(clip_inputs, return_value=True)
```

#### 2.2 ç‰¹å¾æå–ç»†èŠ‚
å¯¹äºä¸åŒæ¨¡å‹ï¼Œç‰¹å¾æå–çš„å…·ä½“å®ç°ï¼š

- **CLIPæ¨¡å‹**ï¼šæå–å›¾åƒç¼–ç å™¨æœ€åä¸€å±‚çš„valueç‰¹å¾
- **SigLIP2æ¨¡å‹**ï¼šé€šè¿‡ä¸­é—´å±‚å‰å‘ä¼ æ’­è·å–ç‰¹å¾
- **Perception Encoder**ï¼šç›´æ¥è·å–ç¼–ç å™¨è¾“å‡º

è¿™äº›ç‰¹å¾å…·æœ‰è‰¯å¥½çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¯ä»¥ç›´æ¥ä¸æ–‡æœ¬åµŒå…¥è¿›è¡Œå¯¹é½ã€‚

### æ­¥éª¤3ï¼šæ©ç å¼•å¯¼çš„æ³¨æ„åŠ›æ± åŒ–ï¼ˆMask-based Attention Poolingï¼‰

è¿™æ˜¯TextRegionçš„æ ¸å¿ƒåˆ›æ–°ï¼Œä¸‹é¢è¯¦ç»†ä»‹ç»ä¸‰ç§æ¨¡å‹çš„å®ç°æ–¹å¼ï¼š

#### 3.1 SigLIP2æ¨¡å‹çš„å®ç°

```python
def siglip_value_with_sam2_attn(self, args, low_res_mask_with_pad, last_blk_value, attn_blk):
    bsz, _, embed_dim = last_blk_value.shape
    
    # å¦‚æœä½¿ç”¨å¤šåˆ†è¾¨ç‡æ–¹æ³•ï¼Œéœ€è¦è°ƒæ•´ç‰¹å¾å°ºå¯¸
    if self.resize_method == 'multi_resolution':
        patch_num = self.crop_size // self.patch_size
        x_ori = last_blk_value.permute(0, 2, 1).contiguous().view(bsz, embed_dim, patch_num, patch_num)
        
        # å°†å¤šä¸ªè£å‰ªåçš„å›¾åƒæ‹¼æ¥æˆä¸€ä¸ªå¤šåˆ†è¾¨ç‡ç‰¹å¾å›¾
        # ... å¤šåˆ†è¾¨ç‡å¤„ç†é€»è¾‘ ...
        
        x_input = x_multi_reso.contiguous().view(1, embed_dim, self.crop_num_h * self.crop_num_w * patch_num ** 2).permute(0, 2, 1)
    else:
        x_input = last_blk_value

    # å…¨å±€è¡¥ä¸è¿‡æ»¤ï¼šç§»é™¤ä¸ä»»ä½•åŒºåŸŸéƒ½ä¸ç›¸å…³çš„è¡¥ä¸
    if args.remove_global_patch:
        keep_masks = torch.sum(low_res_mask_with_pad, dim=1) > 0
        low_res_mask = low_res_mask_with_pad[keep_masks]
        
        # è®¡ç®—è¡¥ä¸é—´çš„ç›¸ä¼¼æ€§ï¼Œåˆ¤æ–­å“ªäº›è¡¥ä¸åº”è¯¥è¢«ç§»é™¤
        patch_norm = x_input.norm(dim=-1, keepdim=True)
        patch_features = (x_input / patch_norm)[0]
        patch_similarity = (patch_features @ patch_features.T).float()
        
        # è®¡ç®—è¡¥ä¸ä¸åŒºåŸŸçš„ç›¸ä¼¼æ€§
        patch_2_region = patch_similarity @ (low_res_mask > 0).float().T
        patch_2_region_avg = patch_2_region / (low_res_mask > 0).sum(dim=-1)
        
        # è®¡ç®—è¡¥ä¸åœ¨åŒºåŸŸå†…ä¸åŒºåŸŸå¤–çš„å¹³å‡ç›¸ä¼¼æ€§å·®å¼‚
        blong_score = patch_2_region_avg * (low_res_mask > 0).float().T
        blong_score_avg = blong_score.sum(dim=-1) / ((low_res_mask > 0).sum(dim=0) + 1e-9)
        
        outside_score = patch_2_region_avg * (low_res_mask == 0).float().T
        outside_score_avg = outside_score.sum(dim=-1) / ((low_res_mask == 0).sum(dim=0) + 1e-9)
        
        difference_score = (blong_score_avg - outside_score_avg).cpu().float().numpy()
        
        # æ ¹æ®é˜ˆå€¼è¿‡æ»¤è¡¥ä¸
        low_res_mask_with_pad[:, difference_score < self.global_patch_threshold] = 0

    keep_masks = torch.sum(low_res_mask_with_pad, dim=1) > 0
    low_res_mask_with_pad = low_res_mask_with_pad[keep_masks]
    low_res_mask_with_pad = torch.clamp(low_res_mask_with_pad, min=0, max=1)
    
    region_num = low_res_mask_with_pad.shape[0]

    # æ‰§è¡Œæ©ç å¼•å¯¼çš„æ³¨æ„åŠ›æ± åŒ–
    _, N, C = x_input.shape
    q_latent = attn_blk.latent.expand(region_num, -1, -1)
    q = attn_blk.q(q_latent).reshape(region_num, attn_blk.latent_len, attn_blk.num_heads, attn_blk.head_dim).transpose(1, 2)

    x = x_input.expand(region_num, -1, -1)
    kv = attn_blk.kv(x).reshape(region_num, N, 2, attn_blk.num_heads, attn_blk.head_dim).permute(2, 0, 3, 1, 4)
    k, v = kv.unbind(0)
    q, k = attn_blk.q_norm(q), attn_blk.k_norm(k)

    # å…³é”®ï¼šä½¿ç”¨æ©ç çº¦æŸæ³¨æ„åŠ›æƒé‡
    attn_mask = low_res_mask_with_pad.unsqueeze(1).unsqueeze(1).repeat(1, attn_blk.num_heads, 1, 1)
    
    # å¯¹é”®è¿›è¡Œå¹³å‡æ± åŒ–
    k = attn_blk.k_norm(k.mean(dim=-2, keepdim=True).mean(dim=-1, keepdim=True))
    k = k.repeat(1, 1, v.shape[-2], v.shape[-1])
    
    # è®¡ç®—å¸¦æ©ç çš„æ³¨æ„åŠ›
    x = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask > 0)

    # åå¤„ç†
    x = x.transpose(1, 2).reshape(region_num, attn_blk.latent_len, C)
    x = attn_blk.proj(x)
    x = attn_blk.proj_drop(x)

    x = self.clip.visual.trunk.fc_norm(x)
    x = self.clip.visual.trunk.head_drop(x)

    region_features = x.permute(1, 0, 2)
    region_features /= region_features.norm(dim=-1, keepdim=True)
    return region_features, keep_masks
```

#### 3.2 Perception Encoderæ¨¡å‹çš„å®ç°

```python
def pe_value_with_sam2_attn(self, args, unique_low_res_masks, last_blk_value, blk):
    # ç§»é™¤CLSæ ‡è®°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if self.clip.visual.use_cls_token:
        last_blk_value = last_blk_value[:, 1:]
    
    # å¤šåˆ†è¾¨ç‡å¤„ç†ï¼ˆä¸SigLIP2ç±»ä¼¼ï¼‰
    # ...
    
    # å…¨å±€è¡¥ä¸è¿‡æ»¤ï¼ˆä¸SigLIP2ç±»ä¼¼ï¼‰
    # ...
    
    # å…³é”®ï¼šä½¿ç”¨probeæœºåˆ¶è¿›è¡ŒåŒºåŸŸç‰¹å¾æå–
    q = blk.probe.repeat((batch, 1, 1)).to(x.dtype)  # æŸ¥è¯¢å‘é‡
    k = blk.layernorm(x.mean(dim=-2, keepdim=True))  # é”®å‘é‡
    k = k.repeat(1, x.shape[-2], 1).to(x.dtype)     # æ‰©å±•é”®å‘é‡
    
    # å¸¦æ©ç çš„æ³¨æ„åŠ›è®¡ç®—
    x = blk.attn(q, k, x, need_weights=False, key_padding_mask=unique_low_res_masks<=0)[0]
    
    # æŠ•å½±åˆ°æœ€ç»ˆç©ºé—´
    with torch.no_grad():
        region_features = x @ self.clip.visual.proj
    region_features = F.normalize(region_features, dim=-1)
    return region_features, keep_masks
```

#### 3.3 æ ‡å‡†CLIPæ¨¡å‹çš„å®ç°

```python
def clip_value_with_sam2_attn(self, args, unique_low_res_masks, clip_v, blk):
    attn_layer = blk.attn
    num_heads = attn_layer.num_heads
    _, bsz, embed_dim = clip_v.size()
    head_dim = embed_dim // num_heads

    # æ ‡å‡†çš„å¤šå¤´æ³¨æ„åŠ›è®¡ç®—
    x = blk.ln_1(clip_v)
    q, k, v_ori = F.linear(x, attn_layer.in_proj_weight, attn_layer.in_proj_bias).chunk(3, dim=-1)

    # å¤šåˆ†è¾¨ç‡å¤„ç†ï¼ˆä¸å‰é¢ç±»ä¼¼ï¼‰
    # ...

    # å…¨å±€è¡¥ä¸è¿‡æ»¤ï¼ˆä¸å‰é¢ç±»ä¼¼ï¼‰
    # ...

    # å…³é”®ï¼šä½¿ç”¨æ©ç çº¦æŸæ³¨æ„åŠ›æƒé‡
    attn_weights = unique_low_res_masks.unsqueeze(0).repeat(num_heads, 1, 1)
    attn_weights = attn_weights.to(dtype=v_multi_head.dtype)

    # åº”ç”¨æ©ç è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—
    attn_output = torch.bmm(attn_weights, v_multi_head)
    attn_output = attn_output.transpose(0, 1).contiguous().view(-1, bsz, embed_dim)
    attn_output = attn_layer.out_proj(attn_output)
    attn_output += blk.mlp(blk.ln_2(attn_output))
    
    region_features = attn_output.permute(1, 0, 2)  # LND -> NLD

    # æœ€ç»ˆæŠ•å½±å’Œå½’ä¸€åŒ–
    region_features = self.clip.visual.ln_post(region_features) @ self.clip.visual.proj
    region_features /= region_features.norm(dim=-1, keepdim=True)
    return region_features, keep_masks
```

### æ­¥éª¤4ï¼šç”ŸæˆåŒºåŸŸä»¤ç‰Œï¼ˆRegion Tokenï¼‰

æ— è®ºä½¿ç”¨å“ªç§æ¨¡å‹ï¼Œæœ€ç»ˆéƒ½ä¼šç”Ÿæˆå½’ä¸€åŒ–çš„åŒºåŸŸç‰¹å¾å‘é‡ï¼š

```python
# åœ¨å„ä¸ªæ–¹æ³•çš„æœ€åéƒ¨åˆ†
region_features /= region_features.norm(dim=-1, keepdim=True)  # L2å½’ä¸€åŒ–
```

è¿™ç¡®ä¿äº†åŒºåŸŸç‰¹å¾ä¸æ–‡æœ¬ç‰¹å¾åœ¨ç›¸åŒçš„åµŒå…¥ç©ºé—´ä¸­ï¼Œå¯ä»¥ç›´æ¥è®¡ç®—ç›¸ä¼¼åº¦ã€‚

### æ­¥éª¤5ï¼šåº”ç”¨äºä¸‹æ¸¸ä»»åŠ¡

#### 5.1 åŒºåŸŸåˆ†ç±»
```python
# è®¡ç®—åŒºåŸŸä»¤ç‰Œä¸æŸ¥è¯¢è¯åµŒå…¥çš„ç›¸ä¼¼åº¦
if self.clip_pretrained == 'siglip2':
    logits_per_text = (
            torch.matmul(self.query_features, region_features[0].t()) * self.clip.logit_scale.exp()
            + self.clip.logit_bias
    )
    region_logits = logits_per_text.t()
else:
    region_logits = region_features[0] @ self.query_features.T
```

#### 5.2 åƒç´ çº§åˆ†å‰²
```python
def postprocess_result(self, region_logits, unique_masks, ori_shape):
    unique_masks = torch.clamp(unique_masks, min=0, max=1)
    
    # å°†åŒºåŸŸåˆ†ç±»ç»“æœå¹¿æ’­å›åŸå§‹åˆ†è¾¨ç‡
    seg_logits = region_logits.unsqueeze(-1).unsqueeze(-1) * unique_masks.unsqueeze(1)
    seg_logits = seg_logits.sum(0, keepdim=True)

    # ä¸Šé‡‡æ ·åˆ°åŸå§‹å›¾åƒå°ºå¯¸
    seg_logits = F.interpolate(seg_logits, size=ori_shape, mode='bilinear')
    seg_logits = torch.softmax(seg_logits * self.region_logit_scale, dim=1)

    # è·å–æœ€ç»ˆé¢„æµ‹ç»“æœ
    seg_preds = seg_logits.argmax(1)
    seg_logits = seg_logits.max(1)[0]
    return seg_logits, seg_preds
```

#### 5.3 æŒ‡ä»£è¡¨è¾¾ç†è§£
é€šè¿‡è®¡ç®—æŸ¥è¯¢æ–‡æœ¬ä¸æ‰€æœ‰åŒºåŸŸä»¤ç‰Œçš„ç›¸ä¼¼åº¦ï¼Œé€‰æ‹©æœ€ç›¸ä¼¼çš„åŒºåŸŸä½œä¸ºç›®æ ‡è¾“å‡ºã€‚

## 4. æ ¸å¿ƒä¼˜åŠ¿ä¸åˆ›æ–°

### 4.1 æ— éœ€è®­ç»ƒ
TextRegionå®Œå…¨åŸºäºé¢„è®­ç»ƒæ¨¡å‹ï¼Œä¸éœ€è¦ä»»ä½•é¢å¤–çš„è®­ç»ƒè¿‡ç¨‹ï¼ŒèŠ‚çœäº†å¤§é‡è®¡ç®—èµ„æºã€‚

### 4.2 é«˜æ•ˆçš„åŒºåŸŸå¯¹é½
é€šè¿‡æ©ç å¼•å¯¼çš„æ³¨æ„åŠ›æ± åŒ–ï¼Œç¡®ä¿æ¯ä¸ªåŒºåŸŸçš„ç‰¹å¾è¡¨ç¤ºä¸å…¶ç©ºé—´ä½ç½®ç²¾ç¡®å¯¹é½ã€‚

### 4.3 å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›
ç”±äºåˆ©ç”¨äº†å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ï¼ŒTextRegionèƒ½å¤Ÿå¤„ç†æœªè§è¿‡çš„ç±»åˆ«å’Œåœºæ™¯ã€‚

### 4.4 æ¨¡å—åŒ–è®¾è®¡
æ”¯æŒå¤šç§å›¾åƒ-æ–‡æœ¬æ¨¡å‹ï¼Œæ˜“äºæ‰©å±•å’Œæ›¿æ¢ä¸åŒçš„éª¨å¹²ç½‘ç»œã€‚

## 5. æŠ€æœ¯è¦ç‚¹æ€»ç»“

TextRegionçš„å…³é”®æŠ€æœ¯è¦ç‚¹åŒ…æ‹¬ï¼š

1. **è½¯æ©ç ç”Ÿæˆ**ï¼šä½¿ç”¨SAM2ç”Ÿæˆæ¦‚ç‡æ€§çš„åŒºåŸŸæ©ç ï¼Œè€Œéç¡¬åˆ†å‰²
2. **å¤šæ¨¡å‹é€‚é…**ï¼šé’ˆå¯¹ä¸åŒæ¨¡å‹è®¾è®¡ç›¸åº”çš„ç‰¹å¾æå–å’Œæ± åŒ–ç­–ç•¥
3. **æ©ç å¼•å¯¼æ± åŒ–**ï¼šåœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­å¼•å…¥æ©ç çº¦æŸï¼Œå®ç°ç²¾ç¡®çš„åŒºåŸŸç‰¹å¾èšåˆ
4. **å…¨å±€è¡¥ä¸è¿‡æ»¤**ï¼šç§»é™¤ä¸ç‰¹å®šåŒºåŸŸæ— å…³çš„å†—ä½™ç‰¹å¾ï¼Œæé«˜è¡¨å¾è´¨é‡
5. **å¤šåˆ†è¾¨ç‡å¤„ç†**ï¼šæ”¯æŒå¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œæå‡ç»†èŠ‚æ•æ‰èƒ½åŠ›

è¿™ç§è®¾è®¡ä½¿å¾—TextRegionåœ¨ä¿æŒé›¶æ ·æœ¬èƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°äº†ç²¾ç¡®çš„åŒºåŸŸçº§è¯­ä¹‰ç†è§£ï¼Œä¸ºå¼€æ”¾ä¸–ç•Œè§†è§‰ç†è§£ä»»åŠ¡æä¾›äº†å¼ºå¤§è€Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚




---

## ä¸€ã€å…ˆç»™ä¸€å¥æ˜ç¡®ç»“è®º

> âœ… **å¯ä»¥åˆ©ç”¨ TextRegion çš„â€œåŒºåŸŸçº§å»ºæ¨¡ + æ©ç å¼•å¯¼èšåˆâ€æ€æƒ³ï¼Œæ˜¾è‘—å¢å¼º SAM3**
> âŒ **ä½†ä¸èƒ½æŒ‡æœ› SAM3 è‡ªå·±å®Œæˆ open-vocab / textâ€“region å¯¹é½**

æ¢å¥è¯è¯´ï¼š

> **TextRegion = â€œè¯­ä¹‰æ³¨å…¥â€**
> **SAM3 = â€œç»“æ„ä¸è¾¹ç•Œä¸“å®¶â€**
> ğŸ‘‰ ä½ å¯ä»¥ç”¨å‰è€…çš„æ€æƒ³ï¼Œ**è¡¥åè€…çš„çŸ­æ¿**

---

## äºŒã€TextRegion çš„â€œæ ¸å¿ƒæ€æƒ³â€åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆå»æ‰ CLIPï¼‰

å¾ˆå¤šäººä»¥ä¸º TextRegion çš„æ ¸å¿ƒæ˜¯ CLIPï¼Œå…¶å®ä¸æ˜¯ã€‚

**TextRegion çœŸæ­£çš„åˆ›æ–°æ˜¯è¿™ä¸‰ç‚¹ï¼š**

### â‘  ç”¨â€œåŒºåŸŸâ€è€Œä¸æ˜¯â€œå…¨å›¾â€ä½œä¸ºåŸºæœ¬è¯­ä¹‰å•ä½

### â‘¡ ç”¨â€œæ©ç â€è€Œä¸æ˜¯ attention å­¦ä¹ åŒºåŸŸå½’å±

### â‘¢ ç”¨â€œåŒºåŸŸ tokenâ€ä½œä¸ºå¯ç»„åˆã€å¯æ¯”è¾ƒçš„ä¸­é—´è¡¨ç¤º

ğŸ“Œ **è¿™ä¸‰ç‚¹ï¼Œå…¨éƒ¨å’Œ SAM3 é«˜åº¦å¥‘åˆ**

---

## ä¸‰ã€SAM3 èƒ½ä» TextRegion å­¦åˆ°ä»€ä¹ˆï¼Ÿï¼ˆé€ç‚¹å¯¹é½ï¼‰

### ğŸ”¹ 1ï¸âƒ£ åŒºåŸŸçº§å»ºæ¨¡ï¼ˆRegion-centric representationï¼‰

#### SAM3 ç°åœ¨çš„é—®é¢˜æ˜¯ï¼š

* decoder åœ¨ **åƒç´ å±‚é¢**å·¥ä½œ
* åŒºåŸŸä¹‹é—´æ²¡æœ‰æ˜¾å¼äº¤äº’
* mask æ˜¯ç»“æœï¼Œä¸æ˜¯ä¸­é—´è¡¨å¾

#### TextRegion çš„æ€æƒ³æ˜¯ï¼š

> **mask â†’ region â†’ token â†’ reasoning**

âœ… ä½ å¯ä»¥åœ¨ SAM3 ä¸­å¼•å…¥ï¼š

```text
mask â†’ region embedding â†’ region graph / refinement
```

ğŸ“Œ **è¿™èƒ½æå‡ï¼š**

* mask ä¸€è‡´æ€§
* åŒºåŸŸé—´ç«äº‰ä¸æŠ‘åˆ¶
* å¤æ‚åœºæ™¯ä¸‹çš„ç¨³å®šæ€§

---

### ğŸ”¹ 2ï¸âƒ£ æ©ç å¼•å¯¼çš„ç‰¹å¾èšåˆï¼ˆMask-guided poolingï¼‰

ä½ å·²ç»éå¸¸æ¥è¿‘è¿™ä¸€æ­¥äº†ï¼ˆä½ åšçš„ pixel/token rectifier æœ¬è´¨å°±åœ¨è¿™ï¼‰ã€‚

**å…·ä½“å¯ä»¥è¿™æ ·åšï¼š**

* ä» SAM3 encoder / pixel_embed ä¸­å–ç‰¹å¾
* ç”¨ decoder äº§ç”Ÿçš„ä¸­é—´ mask
* å¯¹ encoder ç‰¹å¾åšï¼š

[
r_k = \sum_i m_{k,i} \cdot f_i
]

ğŸ“Œ **å…³é”®ï¼š**

* è¿™ä¸ª region token **ä¸å¯¹é½æ–‡æœ¬**
* ä½†å®ƒå¯ä»¥ï¼š

  * åé¦ˆç»™ decoder
  * ç”¨äº mask refinement
  * ç”¨äºåŒºåŸŸä¸€è‡´æ€§çº¦æŸ

---

### ğŸ”¹ 3ï¸âƒ£ ç”¨â€œåŒºåŸŸ tokenâ€åå‘æŒ‡å¯¼ maskï¼ˆé—­ç¯ï¼‰

è¿™æ˜¯ **TextRegion æ²¡æœ‰ã€ä½† SAM3 ç‰¹åˆ«é€‚åˆåšçš„**ã€‚

ä½ å¯ä»¥æ„é€ ä¸€ä¸ª **é—­ç¯ç»“æ„**ï¼š

```
pixel â†’ mask â†’ region token
        â†‘           â†“
      refine â† region-aware attention
```

ğŸ“Œ è¿™ä¼šè®© SAM3 ä»ï¼š

> â€œä¸€æ¬¡æ€§é¢„æµ‹ maskâ€
> å‡çº§ä¸º
> â€œåŒºåŸŸæ„ŸçŸ¥çš„è¿­ä»£æ¨ç†â€

è¿™åœ¨ **å°ç›®æ ‡ / é®æŒ¡ / å¯†é›†å®ä¾‹** ä¸Šéå¸¸æœ‰æ½œåŠ›ã€‚

---

## å››ã€é‚£â€œtextâ€åœ¨è¿™é‡Œè¿˜èƒ½èµ·ä»€ä¹ˆä½œç”¨ï¼Ÿ

è¿™æ˜¯ä½ é—®å¾—æœ€æ·±çš„ä¸€å±‚ ğŸ‘‡

### â— å…³é”®è§‚ç‚¹ï¼š

> **åœ¨ SAM3 é‡Œï¼Œtext ä¸ä¸€å®šæ˜¯â€œè¯­ä¹‰ç›‘ç£â€ï¼Œ
> å®ƒå¯ä»¥æ˜¯â€œç»“æ„çº¦æŸ / é€‰æ‹©ä¿¡å·â€ã€‚**

### ä¸‰ç§å¯è¡Œç”¨æ³•ï¼ˆä¸ç­‰ä»·ï¼Œä½†éƒ½åˆæ³•ï¼‰

---

### ğŸŸ¡ æ–¹æ¡ˆ Aï¼šText ä½œä¸º *prompt selector*ï¼ˆæœ€ç¨³ï¼‰

* ç”¨ text encoderï¼š

  * å†³å®š **æ¿€æ´»å“ªäº› region**
  * å†³å®š **mask ä¹‹é—´çš„æƒé‡**
* ä¸è¦æ±‚ text embedding å¯æ¯”è¾ƒ

ğŸ“Œ **ä½œç”¨ï¼š**

* æé«˜ prompt-following èƒ½åŠ›
* å‡å°‘æ­§ä¹‰ mask

---

### ğŸŸ  æ–¹æ¡ˆ Bï¼šText ä½œä¸º *region gating signal*

* text â†’ gate
* region token â†’ filtered
* decoder åªå…³æ³¨ç›¸å…³åŒºåŸŸ

ğŸ“Œ **è¿™åœ¨ referring expression ä¸Šå¾ˆæœ‰ç”¨**
è€Œä¸” **ä¸è¦æ±‚ CLIP-level è¯­ä¹‰**

---

### ğŸ”´ æ–¹æ¡ˆ Cï¼šè½»é‡â€œè¯­ä¹‰è’¸é¦â€ï¼ˆæœ€æœ‰ç ”ç©¶ä»·å€¼ï¼‰

è¿™æ˜¯æˆ‘è®¤ä¸º**æœ€æœ‰æ½œåŠ›çš„æ–¹å‘**ğŸ‘‡

> **ä¸æ˜¯æŠŠ CLIP æ¥è¿›æ¥æ¨ç†ï¼Œ
> è€Œæ˜¯ç”¨ CLIP *è®­ç»ƒ* SAM3 çš„ region token**

å…·ä½“æ˜¯ï¼š

* è®­ç»ƒæ—¶ï¼š

  * ç”¨ CLIP region embedding ä½œä¸º teacher
  * ç”¨ SAM3 region token ä½œä¸º student
* æ¨ç†æ—¶ï¼š

  * **åªç”¨ SAM3**

ğŸ“Œ è¿™æ˜¯ï¼š

* å‚æ•°å°
* æ¨ç†å¿«
* è®ºæ–‡å‹å¥½

---

## äº”ã€ä¸€ä¸ªéå¸¸é‡è¦çš„åˆ¤æ–­ï¼ˆæˆ‘å¸Œæœ›ä½ è®¤çœŸçœ‹ï¼‰

> **TextRegion çš„æ€æƒ³ â‰  open-vocab æœ¬èº«**
> **è€Œæ˜¯â€œç”¨åŒºåŸŸä½œä¸ºè¯­ä¹‰è®¡ç®—å•å…ƒâ€**

è€Œ SAM3 **æ­£ç¼ºè¿™ä¸ªä¸­é—´å±‚**ã€‚

---

## å…­ã€å¦‚æœè®©æˆ‘æ›¿ä½ é€‰ä¸€ä¸ªâ€œæœ€å¯èƒ½æˆåŠŸâ€çš„æ–¹å‘

æˆ‘ä¼šå»ºè®®ä½ åšè¿™ä¸ª ğŸ‘‡

### â­ **Region-aware SAM3ï¼ˆä¸åš open-vocabï¼‰**

**æ ¸å¿ƒè´¡çŒ®ç‚¹å¯ä»¥æ˜¯ï¼š**

* region token å¼•å…¥
* mask-guided pooling
* region-level refinement
* æ˜¾è‘—æå‡ï¼š

  * mask ç¨³å®šæ€§
  * å°ç‰©ä½“
  * å¤æ‚åœºæ™¯

ğŸ“Œ **ä¸ç¢° open-vocabï¼Œåè€Œæ›´å®¹æ˜“æ‰“ç©¿å®¡ç¨¿äººã€‚**

---

## ä¸ƒã€æœ€åä¸€å¥æå¿ƒçªå­çš„åˆ¤æ–­

ä½ ç°åœ¨è¿™ä¸ªé—®é¢˜ï¼Œå·²ç»ä¸æ˜¯â€œèƒ½ä¸èƒ½â€ï¼Œè€Œæ˜¯ï¼š

> **â€œæ€ä¹ˆåšï¼Œæ‰èƒ½æ—¢ä¸èƒŒç¦» SAM3 çš„è®¾è®¡å“²å­¦ï¼Œ
> åˆèƒ½å¼•å…¥ TextRegion çš„ä¼˜åŠ¿ï¼Ÿâ€**

è€Œç­”æ¡ˆæ˜¯ï¼š

> **æŠŠâ€œè¯­ä¹‰å¯¹é½â€æ¢æˆâ€œåŒºåŸŸå»ºæ¨¡â€ï¼Œ
> æŠŠâ€œæ–‡æœ¬ç›‘ç£â€æ¢æˆâ€œç»“æ„ç›‘ç£â€ã€‚**


## SAM3å®ç°æ–¹æ¡ˆæµ‹è¯•

æˆ‘å°†æ ¹æ®ä»£ç è¿›è¡Œå…·ä½“è¯¦è§£ï¼Œåˆ†æ`RegionAwareSegEarthOV3Segmentor`ç±»çš„å„ä¸ªç»„æˆéƒ¨åˆ†å’Œå®ç°ç»†èŠ‚ã€‚

## 1. ç±»å®šä¹‰ä¸åˆå§‹åŒ–

```python
@MODELS.register_module()
class RegionAwareSegEarthOV3Segmentor(BaseSegmentor):
```

è¿™ä¸ªç±»ç»§æ‰¿è‡ªMMSegmentationçš„[BaseSegmentor](file:///d:/CodeReading/TextRegion/mmseg/models/segmentors/base.py#L14-L274)ï¼Œå¹¶ä½¿ç”¨`@MODELS.register_module()`è£…é¥°å™¨æ³¨å†Œåˆ°MMSegmentationçš„æ¨¡å‹æ³¨å†Œè¡¨ä¸­ï¼Œè¿™æ ·å¯ä»¥é€šè¿‡é…ç½®æ–‡ä»¶åˆ›å»ºè¯¥æ¨¡å‹å®ä¾‹ã€‚

### åˆå§‹åŒ–å‚æ•°è¯¦è§£ï¼š

```python
def __init__(self, 
             classname_path,
             device=torch.device('cuda'),
             prob_thd=0.0,
             bg_idx=0,
             slide_stride=0,
             slide_crop=0,
             confidence_threshold=0.5,
             use_sem_seg=True,
             use_presence_score=True,
             use_transformer_decoder=True,
             region_refinement_iterations=0,  # ä¿®å¤ï¼šé»˜è®¤è®¾ä¸º0ï¼Œå› ä¸ºéšæœºåˆå§‹åŒ–çš„å·ç§¯å±‚ä¼šç ´åç‰¹å¾
             region_similarity_threshold=0.7, # åŒºåŸŸç›¸ä¼¼åº¦é˜ˆå€¼
             region_pooling_method='masked_average',  # åŒºåŸŸæ± åŒ–æ–¹æ³•
             score_balance_factor=0.3,  # æ·»åŠ è¯„åˆ†å¹³è¡¡å› å­ï¼Œé»˜è®¤å€¼0.3
             **kwargs):
```

- `classname_path`: ç±»åˆ«åç§°æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºè¯»å–å¾…åˆ†å‰²çš„ç±»åˆ«
- `device`: è®¡ç®—è®¾å¤‡
- `prob_thd`: æ¦‚ç‡é˜ˆå€¼
- `bg_idx`: èƒŒæ™¯ç±»ç´¢å¼•
- `slide_stride/slide_crop`: æ»‘åŠ¨çª—å£æ¨ç†å‚æ•°
- `confidence_threshold`: ç½®ä¿¡åº¦é˜ˆå€¼
- `use_sem_seg/use_presence_score/use_transformer_decoder`: æ§åˆ¶æ˜¯å¦ä½¿ç”¨ä¸åŒæ¨¡å—çš„å¸ƒå°”å‚æ•°
- `region_refinement_iterations`: åŒºåŸŸç²¾ç»†åŒ–è¿­ä»£æ¬¡æ•°ï¼ˆé»˜è®¤ä¸º0ï¼‰
- `region_similarity_threshold`: åŒºåŸŸç›¸ä¼¼åº¦é˜ˆå€¼
- `region_pooling_method`: åŒºåŸŸæ± åŒ–æ–¹æ³•ï¼ˆ'masked_average'æˆ–'masked_max'ï¼‰
- `score_balance_factor`: è¯„åˆ†å¹³è¡¡å› å­ï¼Œå¹³è¡¡ç½®ä¿¡åº¦å’ŒåŒºåŸŸå¤§å°çš„å½±å“

## 2. æ©ç å¼•å¯¼æ± åŒ–å®ç°

```python
def _mask_guided_pooling(self, encoder_features, masks):
    """
    ä½¿ç”¨maskå¯¹encoderç‰¹å¾è¿›è¡Œæ± åŒ–ï¼Œå¾—åˆ°åŒºåŸŸçº§åˆ«çš„è¡¨ç¤º
    """
    pooled_regions = []
    for mask in masks:
        # ç¡®ä¿maskæ˜¯äºŒç»´çš„
        if mask.ndim == 3:
            mask = mask.squeeze(0)
        
        # å°†maskè°ƒæ•´ä¸ºä¸ç‰¹å¾å›¾ç›¸åŒçš„å°ºå¯¸
        resized_mask = F.interpolate(
            mask.unsqueeze(0).unsqueeze(0).float(),
            size=encoder_features.shape[-2:],
            mode='bilinear',
            align_corners=False
        ).squeeze()
        
        # å½’ä¸€åŒ–mask
        mask_sum = resized_mask.sum()
        if mask_sum > 0:
            # æ ¹æ®é€‰æ‹©çš„æ± åŒ–æ–¹æ³•è¿›è¡Œæ± åŒ–
            if self.region_pooling_method == 'masked_average':
                # æ©ç å¹³å‡æ± åŒ–
                masked_features = encoder_features * resized_mask.unsqueeze(0)
                region_repr = masked_features.sum(dim=[1, 2]) / mask_sum
            elif self.region_pooling_method == 'masked_max':
                # æ©ç æœ€å¤§æ± åŒ–ï¼ˆä½¿ç”¨ä¸€ä¸ªæŠ€å·§æ¥åº”ç”¨æ©ç ï¼‰
                masked_features = encoder_features.clone()
                masked_features[:, resized_mask < 0.5] = float('-inf')
                region_repr = F.adaptive_max_pool2d(
                    masked_features.unsqueeze(0),
                    output_size=(1, 1)
                ).squeeze()
            else:
                # é»˜è®¤ä½¿ç”¨æ©ç å¹³å‡æ± åŒ–
                masked_features = encoder_features * resized_mask.unsqueeze(0)
                region_repr = masked_features.sum(dim=[1, 2]) / mask_sum
        else:
            # å¦‚æœmaskä¸ºç©ºï¼Œè¿”å›é›¶å‘é‡
            region_repr = torch.zeros(encoder_features.shape[0], device=encoder_features.device)
        
        pooled_regions.append(region_repr)
    
    return torch.stack(pooled_regions, dim=0)
```

è¿™ä¸ªæ–¹æ³•å®ç°äº†**æ©ç å¼•å¯¼ç‰¹å¾èšåˆ**ï¼Œæ˜¯TextRegionæ€æƒ³çš„é‡è¦ä½“ç°ï¼š

- å°†åˆ†å‰²æ©ç ä¸ç¼–ç å™¨ç‰¹å¾ç›¸ä¹˜ï¼Œåªä¿ç•™æ„Ÿå…´è¶£åŒºåŸŸçš„ç‰¹å¾
- æ”¯æŒä¸¤ç§æ± åŒ–æ–¹æ³•ï¼šæ©ç å¹³å‡æ± åŒ–å’Œæ©ç æœ€å¤§æ± åŒ–
- é€šè¿‡å½’ä¸€åŒ–æ“ä½œæ¶ˆé™¤åŒºåŸŸå¤§å°å¯¹ç‰¹å¾è¡¨ç¤ºçš„å½±å“
- æœ€ç»ˆç”ŸæˆåŒºåŸŸçº§åˆ«çš„ç‰¹å¾è¡¨ç¤º

## 3. ç±»å†…åŒºåŸŸèšåˆ

```python
def merge_regions_by_class(self, regions, image_shape):
    """
    ç±»å†…regionèšåˆï¼Œè§£å†³é¥æ„Ÿä¸­åŒä¸€ç±»è¢«åˆ†æˆå¤šä¸ªå°regionçš„é—®é¢˜
    """
    if not regions:
        return []
    
    merged = []
    
    for class_id in set(r['class_id'] for r in regions):
        cls_regions = [r for r in regions if r['class_id'] == class_id]
        used = [False] * len(cls_regions)

        for i, r in enumerate(cls_regions):
            if used[i]:
                continue

            cur_mask = r['mask'].clone()
            cur_score = r['score']
            used[i] = True

            for j in range(i + 1, len(cls_regions)):
                if used[j]:
                    continue
                other = cls_regions[j]

                # IoU-based mergeï¼ˆé¥æ„Ÿéå¸¸æœ‰æ•ˆï¼‰
                inter = (cur_mask & other['mask']).sum()
                union = (cur_mask | other['mask']).sum()
                iou = inter.float() / (union.float() + 1e-6)

                if iou > 0.3:  # é¥æ„Ÿå»ºè®® 0.3ï½0.5
                    cur_mask |= other['mask']
                    cur_score = max(cur_score, other['score'])
                    used[j] = True

            merged.append({
                'mask': cur_mask,
                'class_id': class_id,
                'score': cur_score
            })
    return merged
```

è¿™ä¸ªæ–¹æ³•ä½“ç°äº†**ç±»å†…åŒºåŸŸèšåˆ**çš„è®¾è®¡æ¨¡å¼ï¼š

- éå†æ¯ç§ç±»åˆ«ï¼Œå°†åŒç±»åˆ«ä¸­çš„å¤šä¸ªå°åŒºåŸŸåˆå¹¶
- ä½¿ç”¨IoUï¼ˆäº¤å¹¶æ¯”ï¼‰åˆ¤æ–­ä¸¤ä¸ªåŒºåŸŸæ˜¯å¦åº”è¯¥åˆå¹¶
- è®¾ç½®äº†é€‚åˆé¥æ„Ÿå›¾åƒçš„IoUé˜ˆå€¼ï¼ˆ0.3ï¼‰ï¼Œå½“ä¸¤ä¸ªåŒºåŸŸçš„IoUè¶…è¿‡æ­¤å€¼æ—¶è¿›è¡Œåˆå¹¶
- ä¿ç•™åˆå¹¶ååŒºåŸŸä¸­çš„æœ€é«˜å¾—åˆ†

## 4. å•è§†å›¾æ¨ç†

```python
def _inference_single_view(self, image):
    """åœ¨å•ä¸ªPILå›¾åƒæˆ–è£å‰ªå—ä¸Šè¿›è¡Œæ¨ç†ï¼Œä½¿ç”¨region-wiseåˆ†é…ç­–ç•¥."""
    w, h = image.size
    # è¿”å›å¤šä¸ªå€™é€‰åŒºåŸŸè€Œä¸æ˜¯ç›´æ¥çš„åƒç´ é¢„æµ‹
    regions = []  # (mask, class_id, score)

    with torch.no_grad(), torch.autocast(device_type="cuda", dtype=torch.float16):
        # è®¾ç½®å›¾åƒå¹¶è·å–å¤šå°ºåº¦ç‰¹å¾
        inference_state = self.processor.set_image(image)
        
        # é¦–å…ˆè·å–å®Œæ•´çš„semantic logitsç”¨äºç‚¹é€‰æ‹©
        semantic_logits = torch.zeros((self.num_cls, h, w), device=self.device, dtype=torch.float16)
        for query_idx, query_word in enumerate(self.query_words):
            class_id = int(self.query_idx[query_idx])
            self.processor.reset_all_prompts(inference_state)
            inference_state = self.processor.set_text_prompt(state=inference_state, prompt=query_word)
            
            if 'semantic_mask_logits' in inference_state:
                semantic_single = inference_state['semantic_mask_logits']
                if semantic_single.shape != (h, w):
                    # ç¡®ä¿å¼ é‡ä¸º4Dæ ¼å¼
                    if semantic_single.dim() == 2:
                        semantic_single = semantic_single.unsqueeze(0).unsqueeze(0)
                    elif semantic_single.dim() == 3:
                        semantic_single = semantic_single.unsqueeze(0)
                    semantic_single = F.interpolate(
                        semantic_single, 
                        size=(h, w), 
                        mode='bilinear', 
                        align_corners=False
                    ).squeeze()
                semantic_logits[class_id] = semantic_single.to(semantic_logits.dtype)
        
        # ç„¶åä¸ºæ¯ä¸ªç±»åˆ«ç”Ÿæˆå®ä¾‹çº§åˆ«çš„mask
        for query_idx, query_word in enumerate(self.query_words):
            class_id = int(self.query_idx[query_idx])  # è·å–å¯¹åº”çš„ç±»åˆ«ID
            
            self.processor.reset_all_prompts(inference_state)
            inference_state = self.processor.set_text_prompt(state=inference_state, prompt=query_word)

            # è·å–åˆå§‹çš„åˆ†å‰²logits
            initial_logits = torch.zeros((h, w), device=self.device, dtype=torch.float16)
            
            if self.use_transformer_decoder and 'masks_logits' in inference_state:
                if inference_state['masks_logits'].shape[0] > 0:
                    inst_len = inference_state['masks_logits'].shape[0]
                    for inst_id in range(inst_len):
                        instance_logits = inference_state['masks_logits'][inst_id].squeeze()
                        instance_score = inference_state['object_score'][inst_id]
                        
                        # å¤„ç†æ½œåœ¨çš„ç»´åº¦ä¸åŒ¹é…
                        if instance_logits.shape != (h, w):
                            # ç¡®ä¿å¼ é‡ä¸º4Dæ ¼å¼
                            if instance_logits.dim() == 2:
                                instance_logits = instance_logits.unsqueeze(0).unsqueeze(0)
                            elif instance_logits.dim() == 3:
                                instance_logits = instance_logits.unsqueeze(0)
                            instance_logits = F.interpolate(
                                instance_logits, 
                                size=(h, w), 
                                mode='bilinear', 
                                align_corners=False
                            ).squeeze()
                        
                        # ä½¿ç”¨åŠ æƒæ±‚å’Œè€Œä¸æ˜¯maxï¼Œé¿å…è¿‡åº¦æŠ‘åˆ¶
                        initial_logits.add_(instance_logits.to(initial_logits.dtype), alpha=instance_score)
            
            if self.use_sem_seg and 'semantic_mask_logits' in inference_state:
                semantic_single = inference_state['semantic_mask_logits']
                if semantic_single.shape != (h, w):
                    # ç¡®ä¿å¼ é‡ä¸º4Dæ ¼å¼
                    if semantic_single.dim() == 2:
                        semantic_single = semantic_single.unsqueeze(0).unsqueeze(0)
                    elif semantic_single.dim() == 3:
                        semantic_single = semantic_single.unsqueeze(0)
                    semantic_single = F.interpolate(
                        semantic_single, 
                        size=(h, w), 
                        mode='bilinear', 
                        align_corners=False
                    ).squeeze()
                
                # ä½¿ç”¨åŠ æƒèåˆï¼Œé¿å…maxæŠ‘åˆ¶é—®é¢˜
                # å°†semantic_logitsè½¬æ¢ä¸ºfloat16ä»¥åŒ¹é…å…¶ä»–å¼ é‡
                initial_logits.add_(semantic_single.to(initial_logits.dtype))
            
            # åº”ç”¨å­˜åœ¨æ€§åˆ†æ•°
            presence_score = 1.0
            if self.use_presence_score and "presence_score" in inference_state:
                # ç¡®ä¿presence_scoreæ˜¯æ ‡é‡æˆ–ä¸initial_logitså…¼å®¹çš„å½¢çŠ¶
                presence_score = inference_state["presence_score"]
                if torch.is_tensor(presence_score) and presence_score.numel() > 1:
                    # å¦‚æœpresence_scoreä¸æ˜¯æ ‡é‡ï¼Œå–å¹³å‡å€¼
                    presence_score = presence_score.mean()
            
            # ä»initial_logitsä¸­æå–é«˜è´¨é‡çš„mask proposals
            # ä½¿ç”¨å¤šä¸ªé˜ˆå€¼æå–ä¸åŒè´¨é‡çš„maskï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
            thresholds = [0.1, 0.3, 0.5]
            for threshold in thresholds:
                mask = initial_logits > threshold
                if mask.sum() > 10:  # ç¡®ä¿maskè¶³å¤Ÿå¤§
                    # è®¡ç®—ç»¼åˆå¾—åˆ†ï¼Œè€ƒè™‘é¢ç§¯ã€å¹³å‡ç½®ä¿¡åº¦å’Œpresence score
                    area = mask.sum().float()
                    avg_conf = initial_logits[mask].mean()
                    
                    # ä½¿ç”¨æ”¹è¿›çš„è¯„åˆ†å‡½æ•°ï¼Œå¹³è¡¡ç½®ä¿¡åº¦å’ŒåŒºåŸŸå¤§å°
                    # ä½¿ç”¨æ–°çš„å¹³è¡¡å› å­æ¥æ§åˆ¶ç½®ä¿¡åº¦å’Œé¢ç§¯çš„å½±å“
                    normalized_area = torch.log(area + 1)
                    
                    # ä¸ºä¸åŒç±»åˆ«ä½¿ç”¨ä¸åŒçš„è¯„åˆ†ç­–ç•¥
                    # wall/roof/roadç­‰ç»†é•¿ç»“æ„éœ€è¦æ›´é«˜çš„ç½®ä¿¡åº¦æƒé‡
                    if class_id in [1, 2, 5]:  # wall, road, roof
                        # å¯¹äºç»†é•¿ç»“æ„ï¼Œæ›´é‡è§†ç½®ä¿¡åº¦
                        class_specific_factor = 0.2  # æ›´åå‘ç½®ä¿¡åº¦
                    else:
                        # å¯¹äºå…¶ä»–ç±»åˆ«ï¼Œä½¿ç”¨é»˜è®¤å¹³è¡¡
                        class_specific_factor = self.score_balance_factor
                    
                    balanced_score = (
                        (1 - class_specific_factor) * avg_conf + 
                        class_specific_factor * normalized_area
                    ) * presence_score.to(initial_logits.dtype)
                    
                    # å¯¹road/wall/roofç±»è¿›è¡Œå½¢æ€å­¦é—­è¿ç®—ä¼˜åŒ–
                    mask_np = mask.cpu().numpy().astype(np.uint8)
                    if class_id in [1, 2, 5]:  # wall, road, roof
                        kernel = np.ones((5,5), np.uint8)
                        mask_np = cv2.morphologyEx(mask_np, cv2.MORPH_CLOSE, kernel)
                        mask = torch.from_numpy(mask_np).bool()
                    
                    # æ·»åŠ åŒºåŸŸåˆ°åˆ—è¡¨ä¸­
                    regions.append({
                        'mask': mask.cpu(),  # ç§»åˆ°CPUä»¥èŠ‚çœGPUå†…å­˜
                        'class_id': class_id,
                        'score': balanced_score.item()
                    })
            
            # åŠæ—¶é‡Šæ”¾æ˜¾å­˜
            del initial_logits
            if 'initial_logits' in locals():
                initial_logits = torch.zeros((h, w), device=self.device, dtype=torch.float16)

    # æŒ‰å¾—åˆ†æ’åº
    regions.sort(key=lambda x: x['score'], reverse=True)
    
    # ç±»å†…regionèšåˆ
    regions = self.merge_regions_by_class(regions, (h, w))
    
    # å†æ¬¡æŒ‰å¾—åˆ†æ’åº
    regions.sort(key=lambda x: x['score'], reverse=True)
    
    # è¿”å›regionåˆ—è¡¨è€Œä¸æ˜¯åƒç´ çº§logits
    return regions, (h, w), semantic_logits
```

è¿™æ˜¯å®ç°**åŒºåŸŸçº§å»ºæ¨¡**çš„æ ¸å¿ƒæ–¹æ³•ï¼š

- ä½¿ç”¨SAM3æ¨¡å‹ç”Ÿæˆåˆ†å‰²æ©ç å’Œè¯­ä¹‰logits
- å¯¹æ¯ä¸ªç±»åˆ«ç”Ÿæˆå¤šä¸ªmask proposalsï¼Œä½¿ç”¨å¤šä¸ªé˜ˆå€¼ï¼ˆ0.1, 0.3, 0.5ï¼‰æå–ä¸åŒè´¨é‡çš„mask
- å®ç°äº†æ”¹è¿›çš„è¯„åˆ†å‡½æ•°ï¼Œå¹³è¡¡ç½®ä¿¡åº¦å’ŒåŒºåŸŸå¤§å°çš„å½±å“
- ä¸ºä¸åŒç±»åˆ«ä½¿ç”¨ä¸åŒçš„è¯„åˆ†ç­–ç•¥ï¼ˆå¦‚å¯¹wall/roof/roadç­‰ç»†é•¿ç»“æ„ä½¿ç”¨ä¸åŒçš„å¹³è¡¡å› å­ï¼‰
- å¯¹æŸäº›ç±»åˆ«æ‰§è¡Œå½¢æ€å­¦æ“ä½œï¼ˆå¦‚é—­è¿ç®—ï¼‰ä¼˜åŒ–åˆ†å‰²ç»“æœ
- å°†ç”Ÿæˆçš„åŒºåŸŸå­˜å‚¨åœ¨åˆ—è¡¨ä¸­å¹¶æŒ‰å¾—åˆ†æ’åº

## 5. æ»‘åŠ¨çª—å£æ¨ç†

```python
def slide_inference(self, image, stride, crop_size):
    """ä½¿ç”¨PILè£å‰ªè¿›è¡Œæ»‘åŠ¨çª—å£æ¨ç†ï¼Œä½¿ç”¨region-wiseç­–ç•¥."""
    # ... ä»£ç çœç•¥ï¼Œä¸»è¦æ˜¯éå†å›¾åƒå— ...
    
    # å…¨å›¾çº§regionå»é‡ï¼ˆæ»‘çª—NMSï¼‰
    all_regions = self.merge_regions_by_class(all_regions, (h_img, w_img))
    
    # Region-aware logits reweightingï¼ˆæ›¿ä»£åŸæ¥çš„refinementï¼‰
    refined_logits = base_logits.clone()
    
    # ä¸ºæ¯ä¸ªç±»åˆ«åªé€‰æ‹©ä¸€ä¸ªæœ€é«˜å¾—åˆ†çš„regionè¿›è¡Œreweighting
    class_regions = {}
    for region in all_regions:
        class_id = region['class_id']
        if class_id not in class_regions or region['score'] > class_regions[class_id]['score']:
            class_regions[class_id] = region
    
    for class_id, region in class_regions.items():
        mask = region['mask'].to(self.device)
        score = region['score']
        
        # å¦‚æœregionåˆ†æ•°å¤ªä½ï¼Œè·³è¿‡reweighting
        if score < 0.2:  # é˜ˆå€¼å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
            continue
            
        # ä½¿ç”¨region maskè¿›è¡Œlogit reweighting
        region_mask = mask.float().to(refined_logits.device)
        
        # å¯¹è¯¥ç±»åˆ«çš„logitsè¿›è¡Œreweightingï¼Œå¢å¼ºregionå†…éƒ¨çš„ç½®ä¿¡åº¦
        refined_logits[class_id] = (
            base_logits[class_id] * (1 - 0.1 * region_mask) 
            + base_logits[class_id] * region_mask * 1.1  # ç•¥å¾®æå‡regionå†…éƒ¨ç½®ä¿¡åº¦
        )
```

è¿™ä¸ªæ–¹æ³•å®ç°äº†**Region-aware Logit Reweighting**ï¼š

- åœ¨æ»‘åŠ¨çª—å£æ¨ç†åï¼Œå°†æ‰€æœ‰åŒºåŸŸåˆå¹¶åˆ°å…¨å›¾åæ ‡ç³»
- å¯¹è·¨çª—å£çš„åŒç±»åŒºåŸŸè¿›è¡Œå»é‡ï¼ˆæ»‘çª—NMSï¼‰
- ä½¿ç”¨åŒºåŸŸmaskå¯¹åŸºç¡€logitsè¿›è¡Œé‡åŠ æƒï¼Œå¢å¼ºåŒºåŸŸå†…ç½®ä¿¡åº¦ï¼ŒåŒæ—¶ç•¥å¾®é™ä½åŒºåŸŸå¤–ç½®ä¿¡åº¦

## 6. é¢„æµ‹æ–¹æ³•

```python
def predict(self, inputs, data_samples):
    # ... ä»£ç çœç•¥ï¼Œä¸»è¦æ˜¯åŠ è½½å›¾åƒå’Œç¡®å®šæ¨ç†æ¨¡å¼ ...
    
    # è·å–regionå¹¶è¿›è¡Œregion-aware logit reweighting
    regions, (h, w), semantic_logits = self._inference_single_view(image)
    
    # ä¸ºæ¯ä¸ªç±»åˆ«åªé€‰æ‹©ä¸€ä¸ªæœ€é«˜å¾—åˆ†çš„regionè¿›è¡Œreweighting
    class_regions = {}
    for region in regions:
        class_id = region['class_id']
        if class_id not in class_regions or region['score'] > class_regions[class_id]['score']:
            class_regions[class_id] = region
    
    # å¯¹regionè¿›è¡Œlogit reweighting
    refined_logits = base_logits.clone()
    
    for class_id, region in class_regions.items():
        mask = region['mask'].to(self.device)
        score = region['score']
        
        # å¦‚æœregionåˆ†æ•°å¤ªä½ï¼Œè·³è¿‡reweighting
        if score < 0.2:  # é˜ˆå€¼å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
            continue
            
        # ä½¿ç”¨region maskè¿›è¡Œlogit reweighting
        region_mask = mask.float().to(refined_logits.device)
        
        # å¯¹è¯¥ç±»åˆ«çš„logitsè¿›è¡Œreweightingï¼Œå¢å¼ºregionå†…éƒ¨çš„ç½®ä¿¡åº¦
        refined_logits[class_id] = (
            base_logits[class_id] * (1 - 0.1 * region_mask) 
            + base_logits[class_id] * region_mask * 1.1  # ç•¥å¾®æå‡regionå†…éƒ¨ç½®ä¿¡åº¦
        )
    
    # åˆ›å»ºæœ€ç»ˆçš„åˆ†å‰²ç»“æœ
    seg_logits = refined_logits
```

## æ€»ç»“

è¿™ä¸ªå®ç°å¾ˆå¥½åœ°ç»“åˆäº†TextRegionçš„"åŒºåŸŸçº§å»ºæ¨¡+æ©ç å¼•å¯¼èšåˆ"æ€æƒ³ï¼Œå…·ä½“ä½“ç°åœ¨ï¼š

1. **åŒºåŸŸçº§å»ºæ¨¡**ï¼šé€šè¿‡ç”Ÿæˆregion proposalsè€Œéç›´æ¥åƒç´ é¢„æµ‹
2. **æ©ç å¼•å¯¼ç‰¹å¾èšåˆ**ï¼šä½¿ç”¨åˆ†å‰²æ©ç å¯¹ç‰¹å¾è¿›è¡Œæ± åŒ–
3. **ç±»å†…åŒºåŸŸèšåˆ**ï¼šè§£å†³åŒä¸€ç±»è¢«åˆ†æˆå¤šä¸ªå°åŒºåŸŸçš„é—®é¢˜
4. **Region-aware Logit Reweighting**ï¼šä½¿ç”¨é«˜è´¨é‡region maskè°ƒæ•´logits
5. **å¤šé˜ˆå€¼maskæå–**ï¼šæé«˜ä¸åŒè´¨é‡åŒºåŸŸçš„å¬å›ç‡

æ­¤å¤–ï¼Œè¿˜é’ˆå¯¹é¥æ„Ÿå›¾åƒè¿›è¡Œäº†ä¸“é—¨ä¼˜åŒ–ï¼Œå¦‚å¯¹ç»†é•¿ç»“æ„ä½¿ç”¨ç‰¹æ®Šçš„è¯„åˆ†ç­–ç•¥å’Œå½¢æ€å­¦æ“ä½œã€‚
