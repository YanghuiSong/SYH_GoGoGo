# è®ºæ–‡æ–¹æ³•ä¸è§£å†³é—®é¢˜è¯¦è§£

## ä¸€ã€è®ºæ–‡è§£å†³çš„æ ¸å¿ƒé—®é¢˜

### 1.1 CLIPåœ¨åˆ†å‰²ä»»åŠ¡ä¸­çš„æ ¹æœ¬ç¼ºé™·

**CLIPçš„åŸå§‹è®¾è®¡ç›®æ ‡ä¸åˆ†å‰²ä»»åŠ¡çš„ä¸åŒ¹é…**ï¼š

CLIPçš„åŸå§‹èƒ½åŠ›ï¼š
âœ“ å›¾åƒçº§åˆ†ç±»ï¼šæ•´å¼ å›¾åƒä¸æ–‡æœ¬æè¿°çš„åŒ¹é…
âœ“ é›¶æ ·æœ¬æ³›åŒ–ï¼šå¤„ç†æœªè§è¿‡çš„ç±»åˆ«
âœ“ è·¨æ¨¡æ€ç†è§£ï¼šå›¾åƒä¸æ–‡æœ¬çš„è¯­ä¹‰å¯¹é½

åˆ†å‰²ä»»åŠ¡çš„éœ€æ±‚ï¼š
âœ“ åƒç´ çº§ç†è§£ï¼šæ¯ä¸ªä½ç½®çš„ç²¾ç»†åˆ†ç±»
âœ“ ç©ºé—´ä¸€è‡´æ€§ï¼šç›¸é‚»åŒºåŸŸçš„è¯­ä¹‰è¿è´¯æ€§
âœ“ å±€éƒ¨ç»†èŠ‚ï¼šè¾¹ç•Œã€çº¹ç†ç­‰ç»†ç²’åº¦ä¿¡æ¯

çŸ›ç›¾ç‚¹ï¼šCLIPçš„å…¨å±€ç‰¹å¾æ©ç›–äº†å±€éƒ¨ç»†èŠ‚ï¼


### 1.2 å…·ä½“é—®é¢˜è¡¨ç°

**é—®é¢˜1ï¼šå¼‚å¸¸ä»¤ç‰Œå¹²æ‰°ï¼ˆAnomaly Tokensï¼‰**
```python
# é—®é¢˜ç°è±¡ï¼šæŸäº›ä»¤ç‰Œå¼‚å¸¸åœ°å¸å¼•æ³¨æ„åŠ›
def problematic_attention():
    # æ­£å¸¸æœŸæœ›ï¼šæ³¨æ„åŠ›å…³æ³¨è¯­ä¹‰ç›¸å…³åŒºåŸŸ
    expected = attend_to_semantic_regions()
    
    # å®é™…è§‚å¯Ÿï¼šå¼‚å¸¸ä»¤ç‰Œä¸»å¯¼æ³¨æ„åŠ›åˆ†å¸ƒ
    actual = softmax(Q @ K.T)  # æŸäº›å¼‚å¸¸Kå€¼è¿‡åº¦æ¿€æ´»
    
    # ç»“æœï¼šæ³¨æ„åŠ›åˆ†å¸ƒå˜å¾—å‡åŒ€ï¼Œå¤±å»ç©ºé—´åˆ¤åˆ«æ€§
    result = uniform_attention_map()
    return result
```

**é—®é¢˜2ï¼šç‰¹å¾åŒè´¨åŒ–ï¼ˆFeature Homogenizationï¼‰**
- ä¸åŒç©ºé—´ä½ç½®çš„ç‰¹å¾å˜å¾—ç›¸ä¼¼
- ç¼ºä¹å±€éƒ¨ç»†èŠ‚å’Œè¾¹ç•Œä¿¡æ¯
- åˆ†å‰²ç»“æœå™ªå£°å¤§ã€è¾¹ç•Œæ¨¡ç³Š

**é—®é¢˜3ï¼šç©ºé—´ä¸€è‡´æ€§ç¼ºå¤±**
- æ·±å±‚ç‰¹å¾ä¸¢å¤±ç©ºé—´ä¿¡æ¯
- æ³¨æ„åŠ›æœºåˆ¶æ— æ³•æœ‰æ•ˆæ•æ‰å±€éƒ¨å…³ç³»
- åˆ†å‰²ç»“æœç¼ºä¹è¯­ä¹‰è¿è´¯æ€§

## äºŒã€è®ºæ–‡çš„è§£å†³æ–¹æ³•ï¼šSelf-Calibrated CLIP (SC-CLIP)

### 2.1 æ–¹æ³•æ€»ä½“æ¡†æ¶

```mermaid
graph TB
    A[åŸå§‹CLIPç‰¹å¾] --> B[å¼‚å¸¸ä»¤ç‰Œæ£€æµ‹ä¸ä¿®å¤]
    B --> C[è‡ªæ ¡å‡†ç­–ç•¥]
    C --> D[å¤šå±‚çº§ç‰¹å¾èåˆ]
    D --> E[æ ¡å‡†åçš„åˆ†å‰²ç»“æœ]
    
    B --> B1[LOFå¼‚å¸¸æ£€æµ‹]
    B --> B2[é‚»åŸŸæ’å€¼ä¿®å¤]
    
    C --> C1[ç‰¹å¾èšåˆ]
    C --> C2[æ³¨æ„åŠ›å¢å¼º]
    
    D --> D1[ä¸¤æ¬¡å‰å‘ä¼ æ’­]
    D --> D2[ç‰¹å¾å…¼å®¹æ€§ä¿è¯]
```

### 2.2 æ ¸å¿ƒæ–¹æ³•è¯¦è§£

#### æ–¹æ³•ä¸€ï¼šå¼‚å¸¸ä»¤ç‰Œæ£€æµ‹ä¸ä¿®å¤

**é—®é¢˜æ ¹æºåˆ†æ**ï¼š
- CLIPä¸­å­˜åœ¨æŸäº›"å¼‚å¸¸"ä»¤ç‰Œï¼Œåœ¨ç‰¹å¾ç©ºé—´ä¸­ä¸æ­£å¸¸ä»¤ç‰Œæ˜¾è‘—ä¸åŒ
- è¿™äº›ä»¤ç‰Œåœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­è¿‡åº¦æ¿€æ´»ï¼Œå¹²æ‰°æ­£å¸¸ä»¤ç‰Œçš„æ³¨æ„åŠ›åˆ†å¸ƒ
- å¯¼è‡´ç‰¹å¾å›¾å‡ºç°å™ªå£°å’Œå‡åŒ€æ¿€æ´»

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
class AnomalyTokenResolver:
    def detect_anomalies(self, features):
        """ä½¿ç”¨LOFç®—æ³•æ£€æµ‹å¼‚å¸¸ä»¤ç‰Œ"""
        # LOFåŸºäºå±€éƒ¨å¯†åº¦åå·®æ£€æµ‹å¼‚å¸¸ç‚¹
        lof_scores = compute_lof(features, k=20)
        anomalies = lof_scores > threshold  # é€‰æ‹©å‰5%ä½œä¸ºå¼‚å¸¸
        
        return anomalies
    
    def repair_anomalies(self, features, anomalies):
        """é€šè¿‡é‚»åŸŸæ’å€¼ä¿®å¤å¼‚å¸¸ä»¤ç‰Œ"""
        repaired = features.clone()
        
        for pos in anomalies:
            # è·å–3Ã—3é‚»åŸŸï¼Œæ’é™¤å…¶ä»–å¼‚å¸¸ç‚¹
            neighbors = get_3x3_neighborhood(features, pos)
            valid_neighbors = exclude_anomalies(neighbors, anomalies)
            
            # åŠ æƒå¹³å‡æ’å€¼
            if len(valid_neighbors) > 0:
                repaired[pos] = weighted_average(valid_neighbors)
        
        return repaired
```

**æŠ€æœ¯æ•ˆæœ**ï¼š
- å‡å°‘å¼‚å¸¸ä»¤ç‰Œå¯¹å…¶ä»–ä»¤ç‰Œçš„æ³¨æ„åŠ›å¹²æ‰°
- æ¢å¤ç‰¹å¾å›¾çš„ç©ºé—´åˆ¤åˆ«æ€§
- æå‡åˆ†å‰²è¾¹ç•Œçš„æ¸…æ™°åº¦

#### æ–¹æ³•äºŒï¼šè‡ªæ ¡å‡†ç­–ç•¥

**é—®é¢˜åˆ†æ**ï¼š
- CLIPçš„æ·±å±‚ç‰¹å¾è¯­ä¹‰ä¸°å¯Œä½†ç©ºé—´ä¸€è‡´æ€§å·®
- ä¸­é—´å±‚ç‰¹å¾ç©ºé—´ä¸€è‡´ä½†è¯­ä¹‰ä¿¡æ¯æœ‰é™
- éœ€è¦ç»“åˆä¸¤è€…çš„ä¼˜åŠ¿

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
class SelfAdjustingStrategy:
    def feature_aggregation(self, deep_features, mid_features):
        """åˆ©ç”¨ä¸­å±‚ç‰¹å¾çš„ç©ºé—´ä¸€è‡´æ€§èšåˆæ·±å±‚ç‰¹å¾"""
        # è®¡ç®—ä¸­å±‚ç‰¹å¾çš„ç›¸ä¼¼æ€§çŸ©é˜µ
        mid_similarity = cosine_similarity(mid_features, mid_features)
        
        # ä½¿ç”¨ç›¸ä¼¼æ€§çŸ©é˜µä½œä¸ºæ³¨æ„åŠ›æƒé‡èšåˆæ·±å±‚ç‰¹å¾
        attention_weights = softmax(mid_similarity, dim=-1)
        aggregated_features = attention_weights @ deep_features
        
        return aggregated_features
    
    def attention_enhancement(self, Q, K, mid_similarity):
        """å¢å¼ºæ³¨æ„åŠ›æœºåˆ¶çš„ç©ºé—´ç›¸å…³æ€§"""
        # åŸå§‹è‡ªæ³¨æ„åŠ›
        original_attn = softmax(Q @ K.T)
        
        # å¼•å…¥ä¸­å±‚ç›¸ä¼¼æ€§æŒ‡å¯¼
        guided_attn = softmax(Q @ K.T) + softmax(mid_similarity)
        
        return guided_attn
```

**æŠ€æœ¯æ•ˆæœ**ï¼š
- ç»“åˆæ·±å±‚ç‰¹å¾çš„è¯­ä¹‰ä¿¡æ¯å’Œä¸­å±‚ç‰¹å¾çš„ç©ºé—´ä¸€è‡´æ€§
- æå‡ç‰¹å¾çš„ç©ºé—´åˆ¤åˆ«èƒ½åŠ›
- å¢å¼ºæ³¨æ„åŠ›æœºåˆ¶å¯¹ç›¸å…³åŒºåŸŸçš„èšç„¦

#### æ–¹æ³•ä¸‰ï¼šå¤šå±‚çº§ç‰¹å¾èåˆ

**é—®é¢˜åˆ†æ**ï¼š
- ç›´æ¥èåˆä¸åŒå±‚çº§ç‰¹å¾ä¼šå¯¼è‡´ç‰¹å¾ä¸å…¼å®¹
- ç ´åCLIPä¸æ–‡æœ¬åµŒå…¥çš„å¯¹é½å…³ç³»
- éœ€è¦ä¿æŒæœ€åä¸€å±‚ç‰¹å¾çš„å®Œæ•´æ€§

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
class MultiLevelFusion:
    def two_pass_fusion(self, model, x_penul, multi_level_features):
        """ä¸¤æ¬¡å‰å‘ä¼ æ’­ç­–ç•¥"""
        # ç¬¬ä¸€æ¬¡å‰å‘ï¼šåŸå§‹è·¯å¾„
        output1 = model.forward_last_layer(x_penul)
        
        # ç¬¬äºŒæ¬¡å‰å‘ï¼šå¤šå±‚çº§ç‰¹å¾è·¯å¾„
        aggregated = sum(multi_level_features.values())
        output2 = model.forward_last_layer(aggregated)
        
        # èåˆç»“æœ
        final_output = output1 + output2
        
        return final_output
```

**æŠ€æœ¯åŸç†**ï¼š
- é€šè¿‡æœ€åä¸€å±‚çš„å‚æ•°ç©ºé—´ä¿è¯ç‰¹å¾å…¼å®¹æ€§
- ä¿æŒåŸå§‹ç‰¹å¾ä¸æ–‡æœ¬åµŒå…¥çš„å¯¹é½å…³ç³»
- ä¸°å¯Œç‰¹å¾çš„ç»†èŠ‚ä¿¡æ¯

## ä¸‰ã€æ–¹æ³•çš„æŠ€æœ¯åˆ›æ–°ç‚¹

### 3.1 è®­ç»ƒå…è´¹èŒƒå¼

ä¼ ç»Ÿæ–¹æ³•ï¼š
- éœ€è¦é¢å¤–è®­ç»ƒæˆ–å¾®è°ƒ
- ä¾èµ–åˆ†å‰²æ ‡æ³¨æ•°æ®
- è®¡ç®—æˆæœ¬é«˜

SC-CLIPï¼š
- æ— éœ€ä»»ä½•è®­ç»ƒ
- ç›´æ¥åˆ©ç”¨é¢„è®­ç»ƒCLIP
- é›¶æ ·æœ¬è¿ç§»èƒ½åŠ›


### 3.2 å†…åœ¨ç‰¹æ€§æŒ–æ˜

æ ¸å¿ƒæ€æƒ³ï¼šç”¨CLIPè‡ªèº«çš„èƒ½åŠ›æ”¹è¿›CLIP
- å¼‚å¸¸ä»¤ç‰Œï¼šåˆ©ç”¨LOFæ£€æµ‹CLIPå†…éƒ¨çš„å¼‚å¸¸æ¨¡å¼
- è‡ªæ ¡å‡†ï¼šåˆ©ç”¨CLIPä¸­é—´å±‚çš„ç©ºé—´ä¸€è‡´æ€§
- ç‰¹å¾èåˆï¼šåˆ©ç”¨CLIPå¤šå±‚çº§çš„äº’è¡¥ä¿¡æ¯


### 3.3 è®¡ç®—æ•ˆç‡ä¼˜åŒ–

ç›¸æ¯”å¤–éƒ¨æ–¹æ³•ï¼š
- ProxyCLIPï¼šéœ€è¦DINO backboneï¼ŒFLOPs 34.4G
- SC-CLIPï¼šæ— å¤–éƒ¨backboneï¼ŒFLOPs 17.5G
- é€Ÿåº¦æå‡ï¼šä»3.9 FPSåˆ°6.5 FPS


## å››ã€è§£å†³çš„é—®é¢˜ä¸å¯¹åº”æ–¹æ³•æ˜ å°„

| é—®é¢˜ç±»å‹ | å…·ä½“è¡¨ç° | SC-CLIPè§£å†³æ–¹æ³• | æ•ˆæœæå‡ |
|---------|----------|-----------------|----------|
| æ³¨æ„åŠ›å¹²æ‰° | å¼‚å¸¸ä»¤ç‰Œä¸»å¯¼æ³¨æ„åŠ› | LOFæ£€æµ‹ + é‚»åŸŸä¿®å¤ | æ³¨æ„åŠ›é‡æ–°èšç„¦ç›¸å…³åŒºåŸŸ |
| ç‰¹å¾åŒè´¨åŒ– | ç©ºé—´ç‰¹å¾ç›¸ä¼¼ | ä¸­å±‚ç‰¹å¾å¼•å¯¼èšåˆ | å¢å¼ºç‰¹å¾åˆ¤åˆ«æ€§ |
| ç©ºé—´ä¸€è‡´æ€§å·® | è¾¹ç•Œæ¨¡ç³Šã€å™ªå£°å¤§ | è‡ªæ ¡å‡†ç­–ç•¥ | æå‡è¯­ä¹‰è¿è´¯æ€§ |
| ç»†èŠ‚ä¿¡æ¯ç¼ºå¤± | å±€éƒ¨çº¹ç†ä¸¢å¤± | å¤šå±‚çº§ç‰¹å¾èåˆ | ä¸°å¯Œç»†èŠ‚ä¿¡æ¯ |
| è®¡ç®—æ•ˆç‡ä½ | ä¾èµ–å¤–éƒ¨æ¨¡å‹ | çº¯CLIPå†…éƒ¨ä¼˜åŒ– | å‡å°‘è®¡ç®—å¼€é”€ |

## äº”ã€æ–¹æ³•çš„æ•ˆæœéªŒè¯

### 5.1 å®šé‡ç»“æœ

æ€§èƒ½æå‡ï¼š
- CLIP ViT-B/16: ä»14.4% mIoU â†’ 43.9% mIoU (3å€æå‡)
- CLIP ViT-L/14: ä»6.6% mIoU â†’ 45.2% mIoU (6.8å€æå‡)
- åœ¨8ä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°SOTAï¼Œå¹³å‡æå‡9.5%


### 5.2 å®šæ€§æ”¹è¿›

è§†è§‰æ”¹å–„ï¼š
- åˆ†å‰²è¾¹ç•Œæ›´æ¸…æ™°
- å™ªå£°æ˜¾è‘—å‡å°‘
- è¯­ä¹‰ä¸€è‡´æ€§æ›´å¥½
- ç»†èŠ‚ä¿ç•™æ›´å®Œæ•´


### 5.3 æ•ˆç‡ä¼˜åŠ¿

è®¡ç®—æ•ˆç‡ï¼š
- FLOPs: 17.5G vs 34.4G (ProxyCLIP)
- FPS: 6.5 vs 3.9 (ProxyCLIP)
- å‚æ•°é‡: æ— å¢åŠ  vs é¢å¤–85.8Må‚æ•°


## æ€»ç»“

**SC-CLIPè§£å†³çš„æ ¸å¿ƒé—®é¢˜**ï¼šCLIPåœ¨åˆ†å‰²ä»»åŠ¡ä¸­ç”±äºå…¨å±€ç‰¹å¾ä¸»å¯¼å¯¼è‡´çš„å±€éƒ¨ç»†èŠ‚ä¸¢å¤±å’Œç©ºé—´ä¸€è‡´æ€§å·®çš„é—®é¢˜ã€‚

**é‡‡ç”¨çš„åˆ›æ–°æ–¹æ³•**ï¼š
1. **å¼‚å¸¸ä»¤ç‰Œä¿®å¤** - è§£å†³æ³¨æ„åŠ›å¹²æ‰°é—®é¢˜
2. **è‡ªæ ¡å‡†ç­–ç•¥** - è§£å†³ç‰¹å¾åŒè´¨åŒ–é—®é¢˜  
3. **å¤šå±‚çº§èåˆ** - è§£å†³ç»†èŠ‚ä¿¡æ¯ç¼ºå¤±é—®é¢˜

**æ–¹æ³•ç‰¹ç‚¹**ï¼š
- è®­ç»ƒå…è´¹ï¼šæ— éœ€ä»»ä½•é¢å¤–è®­ç»ƒ
- å†…åœ¨æŒ–æ˜ï¼šåˆ©ç”¨CLIPè‡ªèº«ç‰¹æ€§æ”¹è¿›CLIP
- è®¡ç®—é«˜æ•ˆï¼šä¸å¼•å…¥å¤–éƒ¨è®¡ç®—è´Ÿæ‹…
- é€šç”¨æ€§å¼ºï¼šåœ¨ä¸åŒCLIPæ¶æ„ä¸Šå‡æœ‰æ•ˆ

è¿™ç§æ–¹æ³•ä¸ºé¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­çš„åº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ï¼š**é€šè¿‡æŒ–æ˜æ¨¡å‹å†…åœ¨ç‰¹æ€§æ¥å®ç°è‡ªæˆ‘æ”¹è¿›ï¼Œè€Œéä¾èµ–å¤–éƒ¨ç›‘ç£æˆ–å¤æ‚è®­ç»ƒ**ã€‚


# SC-CLIPç®—æ³•åˆ›æ–°ä¼˜åŒ–è®¾è®¡

## åˆ›æ–°ç‚¹ä¸€ï¼šåŠ¨æ€å¼‚å¸¸ä»¤ç‰Œæ£€æµ‹ä¸è‡ªé€‚åº”ä¿®å¤

### 1.1 é—®é¢˜åˆ†æ
åŸæ–¹æ³•ä½¿ç”¨å›ºå®šæ¯”ä¾‹(5%)çš„å¼‚å¸¸ä»¤ç‰Œä¿®å¤å­˜åœ¨å±€é™æ€§ï¼š
- ä¸åŒå›¾åƒå¤æ‚åº¦ä¸åŒï¼Œå¼‚å¸¸ä»¤ç‰Œæ•°é‡åº”åŠ¨æ€è°ƒæ•´
- å›ºå®šé‚»åŸŸæ’å€¼å¯èƒ½ç ´åé‡è¦è¯­ä¹‰è¾¹ç•Œ

### 1.2 åˆ›æ–°è®¾è®¡ï¼šå¤šå°ºåº¦è‡ªé€‚åº”å¼‚å¸¸æ£€æµ‹

```python
class DynamicAnomalyProcessor:
    def __init__(self):
        self.multi_scale_detectors = {
            'local': LocalAnomalyDetector(k=10),
            'global': GlobalAnomalyDetector(),
            'semantic': SemanticAnomalyDetector()
        }
    
    def dynamic_threshold_selection(self, features):
        """åŸºäºå›¾åƒå¤æ‚åº¦è‡ªé€‚åº”é€‰æ‹©å¼‚å¸¸é˜ˆå€¼"""
        # è®¡ç®—å›¾åƒå¤æ‚åº¦æŒ‡æ ‡
        entropy = self.compute_spatial_entropy(features)
        contrast = self.compute_local_contrast(features)
        
        # åŠ¨æ€è°ƒæ•´å¼‚å¸¸æ¯”ä¾‹
        base_ratio = 0.05
        complexity_factor = 0.5 * entropy + 0.5 * contrast
        dynamic_ratio = base_ratio * (1 + complexity_factor)
        
        return min(dynamic_ratio, 0.15)  # ä¸Šé™15%
    
    def adaptive_repair_strategy(self, anomalies, features):
        """åŸºäºè¯­ä¹‰è¾¹ç•Œçš„è‡ªé€‚åº”ä¿®å¤"""
        repaired_features = features.clone()
        
        for anomaly_pos in anomalies:
            # æ£€æµ‹è¯­ä¹‰è¾¹ç•Œ
            semantic_boundary = self.detect_semantic_boundary(anomaly_pos, features)
            
            if semantic_boundary is None:
                # æ— è¾¹ç•ŒåŒºåŸŸï¼šä½¿ç”¨ä¼ ç»Ÿé‚»åŸŸæ’å€¼
                repaired_features[anomaly_pos] = self.neighborhood_interpolation(anomaly_pos, features)
            else:
                # è¾¹ç•ŒåŒºåŸŸï¼šä¿æŠ¤è¯­ä¹‰è¾¹ç•Œï¼Œä½¿ç”¨åŒä¾§æ’å€¼
                repaired_features[anomaly_pos] = self.boundary_aware_interpolation(
                    anomaly_pos, features, semantic_boundary)
        
        return repaired_features
```

### 1.3 æŠ€æœ¯ä¼˜åŠ¿
- **è‡ªé€‚åº”é˜ˆå€¼**ï¼šæ ¹æ®å›¾åƒå†…å®¹åŠ¨æ€è°ƒæ•´å¼‚å¸¸æ£€æµ‹çµæ•åº¦
- **è¾¹ç•Œä¿æŠ¤**ï¼šé¿å…åœ¨è¯­ä¹‰è¾¹ç•Œå¤„å¼•å…¥å™ªå£°
- **å¤šå°ºåº¦éªŒè¯**ï¼šç»“åˆå±€éƒ¨å’Œå…¨å±€ä¿¡æ¯æé«˜æ£€æµ‹å‡†ç¡®æ€§

## åˆ›æ–°ç‚¹äºŒï¼šå±‚æ¬¡åŒ–æ³¨æ„åŠ›æ ¡å‡†æœºåˆ¶

### 2.1 é—®é¢˜åˆ†æ
åŸæ–¹æ³•çš„æ³¨æ„åŠ›å¢å¼ºç›¸å¯¹ç®€å•ï¼Œæœªå……åˆ†åˆ©ç”¨CLIPçš„å¤šå±‚æ¬¡æ³¨æ„åŠ›ç‰¹æ€§ã€‚

### 2.2 åˆ›æ–°è®¾è®¡ï¼šå¤šç²’åº¦æ³¨æ„åŠ›èåˆ

```python
class HierarchicalAttentionCalibration:
    def __init__(self, clip_model):
        self.clip_model = clip_model
        self.attention_levels = ['shallow', 'middle', 'deep']
    
    def multi_scale_attention_fusion(self, Q, K, V, image_features):
        """å¤šå±‚æ¬¡æ³¨æ„åŠ›èåˆ"""
        attention_maps = {}
        
        # æå–ä¸åŒå±‚çº§çš„æ³¨æ„åŠ›ç‰¹å¾
        for level in self.attention_levels:
            level_features = self.extract_level_features(image_features, level)
            attention_maps[level] = self.compute_cross_scale_attention(Q, K, level_features)
        
        # è‡ªé€‚åº”æƒé‡èåˆ
        fused_attention = self.adaptive_fusion(attention_maps, image_features)
        
        return torch.matmul(fused_attention, V)
    
    def compute_cross_scale_attention(self, Q, K, level_features):
        """è·¨å°ºåº¦æ³¨æ„åŠ›è®¡ç®—"""
        # æŠ•å½±åˆ°ä¸åŒå°ºåº¦ç©ºé—´
        Q_proj = self.scale_projections[level](Q)
        K_proj = self.scale_projections[level](K)
        
        # è®¡ç®—å°ºåº¦ç‰¹å®šæ³¨æ„åŠ›
        scale_attention = torch.matmul(Q_proj, K_proj.transpose(-2, -1))
        
        # å¼•å…¥å°ºåº¦å…ˆéªŒ
        scale_prior = self.get_scale_prior(level, Q.shape[-2])
        scale_attention = scale_attention * scale_prior
        
        return F.softmax(scale_attention, dim=-1)
    
    def adaptive_fusion(self, attention_maps, image_features):
        """åŸºäºå›¾åƒå†…å®¹çš„è‡ªé€‚åº”èåˆ"""
        # è®¡ç®—å„å±‚çº§æ³¨æ„åŠ›çš„ç½®ä¿¡åº¦
        confidences = {}
        for level, attn in attention_maps.items():
            confidences[level] = self.compute_attention_confidence(attn, image_features)
        
        # å½’ä¸€åŒ–æƒé‡
        total_conf = sum(confidences.values())
        weights = {level: conf/total_conf for level, conf in confidences.items()}
        
        # åŠ æƒèåˆ
        fused_attention = sum(weights[level] * attn for level, attn in attention_maps.items())
        
        return fused_attention
```

### 2.3 æŠ€æœ¯ä¼˜åŠ¿
- **å¤šå°ºåº¦æ„ŸçŸ¥**ï¼šç»“åˆä¸åŒæ„Ÿå—é‡çš„æ³¨æ„åŠ›ä¿¡æ¯
- **è‡ªé€‚åº”èåˆ**ï¼šæ ¹æ®å›¾åƒå†…å®¹åŠ¨æ€è°ƒæ•´å„å±‚çº§æƒé‡
- **å°ºåº¦å…ˆéªŒ**ï¼šå¼•å…¥è§†è§‰æ„ŸçŸ¥çš„å°ºåº¦å…ˆéªŒçŸ¥è¯†

## åˆ›æ–°ç‚¹ä¸‰ï¼šè¯­ä¹‰å¼•å¯¼çš„å¤šå±‚çº§ç‰¹å¾é‡ç»„

### 3.1 é—®é¢˜åˆ†æ
åŸå¤šå±‚çº§èåˆæ–¹æ³•ç›¸å¯¹ç®€å•ï¼Œæœªå……åˆ†è€ƒè™‘è¯­ä¹‰ä¸€è‡´æ€§ã€‚

### 3.2 åˆ›æ–°è®¾è®¡ï¼šå›¾ç¥ç»ç½‘ç»œå¼•å¯¼çš„ç‰¹å¾é‡ç»„

```python
class SemanticGuidedFeatureReorganization:
    def __init__(self):
        self.semantic_graph = SemanticGraphBuilder()
        self.feature_router = FeatureRouter()
    
    def build_semantic_graph(self, features, text_embeddings):
        """æ„å»ºè¯­ä¹‰å…³ç³»å›¾"""
        # èŠ‚ç‚¹ï¼šå›¾åƒå—ç‰¹å¾ + æ–‡æœ¬æ¦‚å¿µ
        nodes = self.construct_graph_nodes(features, text_embeddings)
        
        # è¾¹ï¼šè¯­ä¹‰ç›¸ä¼¼æ€§ + ç©ºé—´é‚»è¿‘æ€§
        edges = self.compute_semantic_edges(nodes, features)
        
        return nodes, edges
    
    def graph_based_feature_routing(self, multi_level_features, semantic_graph):
        """åŸºäºå›¾ç¥ç»ç½‘ç»œçš„ç‰¹å¾è·¯ç”±"""
        # åˆå§‹åŒ–èŠ‚ç‚¹ç‰¹å¾
        node_features = self.initialize_node_features(multi_level_features, semantic_graph)
        
        # å›¾å·ç§¯ä¼ æ’­
        for _ in range(3):  # 3å±‚GCN
            node_features = self.graph_convolution(node_features, semantic_graph.edges)
        
        # åŸºäºå›¾ç»“æ„çš„ç‰¹å¾é‡ç»„
        reorganized_features = self.feature_reorganization(node_features, multi_level_features)
        
        return reorganized_features
    
    def semantic_consistency_loss(self, features, text_embeddings):
        """è¯­ä¹‰ä¸€è‡´æ€§çº¦æŸ"""
        # è®¡ç®—å›¾åƒå—ä¸æ–‡æœ¬æ¦‚å¿µçš„ç›¸ä¼¼æ€§
        patch_text_similarity = self.compute_patch_text_similarity(features, text_embeddings)
        
        # æ„å»ºè¯­ä¹‰ä¸€è‡´æ€§çº¦æŸ
        consistency_loss = self.compute_consistency_constraint(patch_text_similarity)
        
        return consistency_loss
```

### 3.3 æŠ€æœ¯ä¼˜åŠ¿
- **è¯­ä¹‰å…³ç³»å»ºæ¨¡**ï¼šæ˜¾å¼å»ºæ¨¡å›¾åƒå—é—´çš„è¯­ä¹‰å…³ç³»
- **å›¾ç¥ç»ç½‘ç»œ**ï¼šåˆ©ç”¨GCNè¿›è¡Œç‰¹å¾ä¼ æ’­å’Œé‡ç»„
- **ä¸€è‡´æ€§çº¦æŸ**ï¼šç¡®ä¿é‡ç»„åçš„ç‰¹å¾ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§

## åˆ›æ–°ç‚¹å››ï¼šå¢é‡å¼è‡ªæ ¡å‡†æ¡†æ¶

### 4.1 é—®é¢˜åˆ†æ
åŸæ–¹æ³•ä¸€æ¬¡æ€§åº”ç”¨æ‰€æœ‰æ ¡å‡†ç­–ç•¥ï¼Œå¯èƒ½äº§ç”Ÿç­–ç•¥é—´å†²çªã€‚

### 4.2 åˆ›æ–°è®¾è®¡ï¼šæ¸è¿›å¼æ ¡å‡†æµæ°´çº¿

```python
class IncrementalCalibrationPipeline:
    def __init__(self):
        self.calibration_stages = [
            ('anomaly_detection', AnomalyDetectionStage()),
            ('spatial_refinement', SpatialRefinementStage()),
            ('semantic_enhancement', SemanticEnhancementStage()),
            ('cross_modal_alignment', CrossModalAlignmentStage())
        ]
        
        self.quality_assessor = CalibrationQualityAssessor()
    
    def progressive_calibration(self, features, text_embeddings):
        """æ¸è¿›å¼æ ¡å‡†æµç¨‹"""
        current_features = features
        calibration_history = []
        
        for stage_name, stage in self.calibration_stages:
            # æ‰§è¡Œå½“å‰é˜¶æ®µæ ¡å‡†
            calibrated_features = stage.execute(current_features, text_embeddings)
            
            # è¯„ä¼°æ ¡å‡†è´¨é‡
            quality_score = self.quality_assessor.assess(
                current_features, calibrated_features, text_embeddings)
            
            # å†³å®šæ˜¯å¦æ¥å—æ ¡å‡†ç»“æœ
            if quality_score > self.acceptance_threshold:
                current_features = calibrated_features
                calibration_history.append((stage_name, quality_score))
            else:
                # è´¨é‡ä¸è¾¾æ ‡ï¼Œå›é€€åˆ°ä¸Šä¸€çŠ¶æ€
                calibration_history.append((stage_name, quality_score, 'rejected'))
        
        return current_features, calibration_history
    
    def adaptive_stage_skipping(self, features):
        """åŸºäºå†…å®¹å¤æ‚åº¦çš„é˜¶æ®µè·³è¿‡ç­–ç•¥"""
        complexity = self.assess_content_complexity(features)
        
        # ç®€å•å†…å®¹è·³è¿‡å¤æ‚æ ¡å‡†é˜¶æ®µ
        if complexity < self.low_complexity_threshold:
            return self.calibration_stages[:2]  # åªæ‰§è¡Œå‰ä¸¤ä¸ªç®€å•é˜¶æ®µ
        elif complexity > self.high_complexity_threshold:
            return self.calibration_stages  # æ‰§è¡Œæ‰€æœ‰é˜¶æ®µ
        else:
            return self.calibration_stages[:3]  # æ‰§è¡Œå‰ä¸‰ä¸ªé˜¶æ®µ
```

### 4.3 æŠ€æœ¯ä¼˜åŠ¿
- **æ¸è¿›ä¼˜åŒ–**ï¼šé¿å…ç­–ç•¥å†²çªï¼Œé€æ­¥ä¼˜åŒ–ç‰¹å¾è´¨é‡
- **è´¨é‡ç›‘æ§**ï¼šå®æ—¶è¯„ä¼°æ ¡å‡†æ•ˆæœï¼ŒåŠ¨æ€è°ƒæ•´ç­–ç•¥
- **è®¡ç®—æ•ˆç‡**ï¼šæ ¹æ®å†…å®¹å¤æ‚åº¦è‡ªé€‚åº”è°ƒæ•´è®¡ç®—å¼€é”€

## åˆ›æ–°ç‚¹äº”ï¼šè·¨æ¨¡æ€å¯¹æ¯”å­¦ä¹ å¢å¼º

### 5.1 é—®é¢˜åˆ†æ
åŸæ–¹æ³•ä¸»è¦å…³æ³¨è§†è§‰ç‰¹å¾ä¼˜åŒ–ï¼Œæœªå……åˆ†åˆ©ç”¨æ–‡æœ¬æ¨¡æ€çš„æŒ‡å¯¼ä½œç”¨ã€‚

### 5.2 åˆ›æ–°è®¾è®¡ï¼šåŒå‘è·¨æ¨¡æ€å¯¹æ¯”å­¦ä¹ 

```python
class CrossModalContrastiveEnhancement:
    def __init__(self, temperature=0.1):
        self.temperature = temperature
        self.text_encoder = TextEncoder()
    
    def bidirectional_contrastive_learning(self, image_features, text_embeddings):
        """åŒå‘å¯¹æ¯”å­¦ä¹ """
        # å›¾åƒåˆ°æ–‡æœ¬çš„å¯¹æ¯”æŸå¤±
        image2text_loss = self.image_to_text_contrastive(
            image_features, text_embeddings)
        
        # æ–‡æœ¬åˆ°å›¾åƒçš„å¯¹æ¯”æŸå¤±  
        text2image_loss = self.text_to_image_contrastive(
            text_embeddings, image_features)
        
        # è·¨æ¨¡æ€ä¸€è‡´æ€§æŸå¤±
        consistency_loss = self.cross_modal_consistency(
            image_features, text_embeddings)
        
        return image2text_loss + text2image_loss + consistency_loss
    
    def hard_negative_mining(self, image_features, text_embeddings):
        """å›°éš¾è´Ÿæ ·æœ¬æŒ–æ˜"""
        # è®¡ç®—æ‰€æœ‰å›¾åƒ-æ–‡æœ¬å¯¹çš„ç›¸ä¼¼åº¦
        similarity_matrix = self.compute_cross_modal_similarity(
            image_features, text_embeddings)
        
        # æŒ–æ˜è¯­ä¹‰ç›¸è¿‘ä½†åŒ¹é…é”™è¯¯çš„å›°éš¾è´Ÿæ ·æœ¬
        hard_negatives = self.mine_hard_negatives(similarity_matrix)
        
        return hard_negatives
    
    def adaptive_feature_rectification(self, image_features, text_embeddings):
        """åŸºäºå¯¹æ¯”å­¦ä¹ çš„ç‰¹å¾çŸ«æ­£"""
        # è®¡ç®—ç‰¹å¾çŸ«æ­£æ–¹å‘
        rectification_direction = self.compute_rectification_direction(
            image_features, text_embeddings)
        
        # è‡ªé€‚åº”çŸ«æ­£å¼ºåº¦
        rectification_strength = self.compute_rectification_strength(
            image_features, text_embeddings)
        
        # åº”ç”¨ç‰¹å¾çŸ«æ­£
        rectified_features = image_features + rectification_strength * rectification_direction
        
        return rectified_features
```

### 5.3 æŠ€æœ¯ä¼˜åŠ¿
- **åŒå‘ç›‘ç£**ï¼šåŒæ—¶åˆ©ç”¨å›¾åƒâ†’æ–‡æœ¬å’Œæ–‡æœ¬â†’å›¾åƒçš„ç›‘ç£ä¿¡å·
- **å›°éš¾æ ·æœ¬æŒ–æ˜**ï¼šå…³æ³¨éš¾ä»¥åŒºåˆ†çš„è¯­ä¹‰è¾¹ç•Œ
- **ç‰¹å¾çŸ«æ­£**ï¼šåŸºäºå¯¹æ¯”å­¦ä¹ çš„æ–¹å‘æ€§ç‰¹å¾ä¼˜åŒ–

## åˆ›æ–°ç‚¹å…­ï¼šè½»é‡çº§å®æ—¶æ¨ç†ä¼˜åŒ–

### 6.1 é—®é¢˜åˆ†æ
åŸæ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡ä¸Šä»æœ‰ä¼˜åŒ–ç©ºé—´ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå®æ—¶åº”ç”¨åœºæ™¯ã€‚

### 6.2 åˆ›æ–°è®¾è®¡ï¼šé€‰æ‹©æ€§æ ¡å‡†ä¸ç¼“å­˜æœºåˆ¶

```python
class LightweightInferenceOptimizer:
    def __init__(self):
        self.feature_cache = FeatureCache()
        self.calibration_predictor = CalibrationNecessityPredictor()
    
    def selective_calibration(self, image, text_embeddings):
        """é€‰æ‹©æ€§æ ¡å‡†ç­–ç•¥"""
        # é¢„æµ‹æ ¡å‡†å¿…è¦æ€§
        calibration_score = self.calibration_predictor.predict(image, text_embeddings)
        
        if calibration_score < self.low_calibration_threshold:
            # ç®€å•å›¾åƒï¼šè·³è¿‡å¤æ‚æ ¡å‡†
            return self.fast_path_calibration(image, text_embeddings)
        elif calibration_score > self.high_calibration_threshold:
            # å¤æ‚å›¾åƒï¼šå®Œæ•´æ ¡å‡†æµç¨‹
            return self.full_calibration(image, text_embeddings)
        else:
            # ä¸­ç­‰å¤æ‚åº¦ï¼šå¹³è¡¡æ ¡å‡†
            return self.balanced_calibration(image, text_embeddings)
    
    def cached_feature_reuse(self, image, text_embeddings):
        """ç‰¹å¾ç¼“å­˜ä¸é‡ç”¨"""
        # ç”Ÿæˆå›¾åƒç­¾å
        image_signature = self.compute_image_signature(image)
        text_signature = self.compute_text_signature(text_embeddings)
        
        cache_key = (image_signature, text_signature)
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self.feature_cache:
            cached_features, calibration_info = self.feature_cache[cache_key]
            
            # éªŒè¯ç¼“å­˜æœ‰æ•ˆæ€§
            if self.validate_cached_features(cached_features, image):
                return cached_features, calibration_info
        
        # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œå®Œæ•´è®¡ç®—
        features, calibration_info = self.selective_calibration(image, text_embeddings)
        
        # æ›´æ–°ç¼“å­˜
        self.feature_cache[cache_key] = (features, calibration_info)
        
        return features, calibration_info
    
    def progressive_refinement(self, initial_result, refinement_budget):
        """æ¸è¿›å¼ç»“æœç»†åŒ–"""
        current_result = initial_result
        remaining_budget = refinement_budget
        
        while remaining_budget > 0 and self.needs_refinement(current_result):
            # é€‰æ‹©æœ€éœ€è¦ç»†åŒ–çš„åŒºåŸŸ
            refinement_regions = self.select_refinement_regions(current_result)
            
            # åˆ†é…è®¡ç®—é¢„ç®—
            region_budget = self.allocate_refinement_budget(
                refinement_regions, remaining_budget)
            
            # æ‰§è¡ŒåŒºåŸŸç»†åŒ–
            current_result = self.refine_regions(
                current_result, refinement_regions, region_budget)
            
            remaining_budget -= region_budget
        
        return current_result
```

### 6.3 æŠ€æœ¯ä¼˜åŠ¿
- **è®¡ç®—è‡ªé€‚åº”**ï¼šæ ¹æ®å†…å®¹å¤æ‚åº¦åŠ¨æ€è°ƒæ•´è®¡ç®—å¼€é”€
- **ç¼“å­˜ä¼˜åŒ–**ï¼šå‡å°‘é‡å¤è®¡ç®—ï¼Œæé«˜æ¨ç†é€Ÿåº¦
- **æ¸è¿›ç»†åŒ–**ï¼šåœ¨æœ‰é™è®¡ç®—é¢„ç®—ä¸‹æœ€å¤§åŒ–æ•ˆæœæå‡

## æ€»ç»“ä¸å®æ–½å»ºè®®

### å®æ–½ä¼˜å…ˆçº§å»ºè®®ï¼š
1. **é«˜ä¼˜å…ˆçº§**ï¼šåŠ¨æ€å¼‚å¸¸ä»¤ç‰Œæ£€æµ‹ï¼ˆåˆ›æ–°ç‚¹ä¸€ï¼‰ + è½»é‡çº§ä¼˜åŒ–ï¼ˆåˆ›æ–°ç‚¹å…­ï¼‰
   - æ•ˆæœæå‡æ˜æ˜¾ï¼Œå®ç°ç›¸å¯¹ç®€å•
   - é€‚åˆä½œä¸ºç¬¬ä¸€è½®ä¼˜åŒ–

2. **ä¸­ä¼˜å…ˆçº§**ï¼šå±‚æ¬¡åŒ–æ³¨æ„åŠ›æ ¡å‡†ï¼ˆåˆ›æ–°ç‚¹äºŒï¼‰ + å¢é‡å¼æ ¡å‡†ï¼ˆåˆ›æ–°ç‚¹å››ï¼‰
   - éœ€è¦è¾ƒå¤šå·¥ç¨‹å®ç°ï¼Œä½†ç†è®ºæ”¶ç›Šæ˜ç¡®

3. **é•¿æœŸç ”ç©¶**ï¼šè¯­ä¹‰å¼•å¯¼é‡ç»„ï¼ˆåˆ›æ–°ç‚¹ä¸‰ï¼‰ + è·¨æ¨¡æ€å¯¹æ¯”ï¼ˆåˆ›æ–°ç‚¹äº”ï¼‰
   - ç†è®ºåˆ›æ–°æ€§å¼ºï¼Œä½†å®ç°å¤æ‚åº¦é«˜
   - é€‚åˆä½œä¸ºå­¦æœ¯ç ”ç©¶æ–¹å‘

### é¢„æœŸæ”¶ç›Šï¼š
- **ç²¾åº¦æå‡**ï¼šé€šè¿‡æ›´ç²¾ç»†çš„æ ¡å‡†ç­–ç•¥ï¼Œé¢„æœŸmIoUå¯æå‡2-4%
- **æ•ˆç‡ä¼˜åŒ–**ï¼šæ¨ç†é€Ÿåº¦å¯æå‡30-50%ï¼Œå†…å­˜å ç”¨é™ä½20-30%
- **æ³›åŒ–èƒ½åŠ›**ï¼šæ›´å¥½åœ°å¤„ç†å¤æ‚åœºæ™¯å’Œé•¿å°¾ç±»åˆ«

è¿™äº›åˆ›æ–°ç‚¹éƒ½åŸºäºSC-CLIPçš„æ ¸å¿ƒåŸç†ï¼Œå…·æœ‰æ˜ç¡®çš„æŠ€æœ¯å¯è¡Œæ€§å’Œé¢„æœŸçš„æ€§èƒ½æ”¶ç›Šï¼Œå¯ä»¥åˆ†é˜¶æ®µå®æ–½éªŒè¯ã€‚

è¿™ç¯‡è®ºæ–‡ã€ŠSelf-Calibrated CLIP for Training-Free Open-Vocabulary Segmentationã€‹æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº† CLIP æ¨¡å‹åœ¨å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ä»¥ä¸‹æ˜¯å¯¹è¯¥è®ºæ–‡çš„è¯¦ç»†è§£æï¼š

---

## ğŸ§  ä¸€ã€ç ”ç©¶èƒŒæ™¯ä¸åŠ¨æœº

### 1.1 å¼€æ”¾è¯æ±‡åˆ†å‰²
- **ç›®æ ‡**ï¼šæ ¹æ®æ–‡æœ¬è¾“å…¥åˆ†å‰²ä»»æ„ç±»åˆ«ï¼Œçªç ´ä¼ ç»Ÿæ¨¡å‹å¯¹å›ºå®šç±»åˆ«é›†çš„ä¾èµ–ã€‚
- **æŒ‘æˆ˜**ï¼šæ¨¡å‹éœ€å…·å¤‡å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿè¯†åˆ«è®­ç»ƒæ—¶æœªè§çš„ç±»åˆ«ã€‚

### 1.2 CLIP çš„ä¼˜åŠ¿ä¸å±€é™
- **ä¼˜åŠ¿**ï¼šCLIP åœ¨å¤§è§„æ¨¡å›¾æ–‡å¯¹ä¸Šé¢„è®­ç»ƒï¼Œå…·å¤‡å¼ºå¤§çš„é›¶æ ·æœ¬åˆ†ç±»èƒ½åŠ›ã€‚
- **å±€é™**ï¼š
  - CLIP æ˜¯å›¾åƒçº§æ¨¡å‹ï¼Œç¼ºä¹å¯¹å±€éƒ¨ç»†èŠ‚çš„æ„ŸçŸ¥ã€‚
  - ç›´æ¥ç”¨äºåˆ†å‰²ä»»åŠ¡æ—¶ï¼Œç‰¹å¾å›¾å™ªå£°å¤§ï¼Œç©ºé—´ä¸€è‡´æ€§å·®ã€‚
  - å®éªŒè¡¨æ˜ï¼ŒCLIP ViT-B/16 åœ¨ COCO-Object ä¸Šä»… 8.9% mIoUã€‚

### 1.3 ç°æœ‰æ–¹æ³•çš„ä¸è¶³
- **ä¿®æ”¹æ³¨æ„åŠ›æœºåˆ¶**ï¼ˆå¦‚ K-K æ³¨æ„åŠ›ï¼‰ï¼šä»å—å…¨å±€å™ªå£°å½±å“ã€‚
- **å¼•å…¥é¢å¤–éª¨å¹²ç½‘ç»œ**ï¼ˆå¦‚ DINOã€SAMï¼‰ï¼šå¢åŠ è®¡ç®—æˆæœ¬ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨ CLIP å†…éƒ¨è¯­ä¹‰ã€‚

---

## ğŸ” äºŒã€é—®é¢˜åˆ†æï¼šå¼‚å¸¸ä»¤ç‰Œ

### 2.1 å¼‚å¸¸ä»¤ç‰Œçš„å‘ç°
- **ç°è±¡**ï¼šåœ¨æ³¨æ„åŠ›å›¾ä¸­ï¼ŒæŸäº›åŒºåŸŸè¢«æ‰€æœ‰ patch è¿‡åº¦å…³æ³¨ï¼Œå¦‚å›¾ 2(a) æ‰€ç¤ºã€‚
- **PCA åˆ†æ**ï¼šè¿™äº›ä»¤ç‰Œåœ¨ç‰¹å¾ç©ºé—´ä¸­åç¦»æ­£å¸¸ä»¤ç‰Œï¼Œç§°ä¸º**å¼‚å¸¸ä»¤ç‰Œ**ã€‚

### 2.2 å¼‚å¸¸ä»¤ç‰Œçš„å½±å“
- å¯¼è‡´æ³¨æ„åŠ›åˆ†å¸ƒå‡åŒ€åŒ–ï¼Œå‰Šå¼±äº†å¯¹å±€éƒ¨è¯­ä¹‰åŒºåŸŸçš„æå–èƒ½åŠ›ã€‚
- é€ æˆç‰¹å¾åŒè´¨åŒ–ï¼Œé™ä½ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ï¼ŒåŠ å‰§ç‰¹å¾å›¾å™ªå£°ã€‚

---

## ğŸ› ï¸ ä¸‰ã€æ–¹æ³•ï¼šSelf-Calibrated CLIP

SC-CLIP æ— éœ€è®­ç»ƒï¼Œä»…å¯¹ CLIP æœ€åä¸€å±‚è¿›è¡Œä¿®æ”¹ï¼ŒåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼š

### 3.1 å¼‚å¸¸ä»¤ç‰Œçš„è¯†åˆ«ä¸ä¿®å¤

#### è¯†åˆ«æ–¹æ³•ï¼š
- ä½¿ç”¨ **LOF ç®—æ³•** æ£€æµ‹å¼‚å¸¸ä»¤ç‰Œã€‚
- LOF é€šè¿‡æ¯”è¾ƒå±€éƒ¨å¯†åº¦è¯†åˆ«å¼‚å¸¸ç‚¹ã€‚

#### ä¿®å¤æ–¹æ³•ï¼š
- å°†å¼‚å¸¸ä»¤ç‰Œæ›¿æ¢ä¸ºå…¶ 3Ã—3 é‚»åŸŸçš„æ’å€¼ï¼ˆæ’é™¤å…¶ä»–å¼‚å¸¸ä»¤ç‰Œï¼‰ã€‚
- å…¬å¼å¦‚ä¸‹ï¼š
```math

\tilde{\mathbb{X}}_{(x,y)}^{penul} = \frac{\sum_{i=-1}^{1}\sum_{j=-1}^{1} w_{i,j} \cdot \mathbb{X}_{(x+i,y+j)}^{penul}}{\sum_{i=-1}^{1}\sum_{j=-1}^{1} w_{i,j}}, \quad \forall(x,y) \in \mathcal{A}

```
#### æ•ˆæœï¼š
- æ­£åˆ™åŒ–æ³¨æ„åŠ›ï¼Œé˜²æ­¢è¿‡åº¦å…³æ³¨å¼‚å¸¸åŒºåŸŸã€‚
- ä¸ºå¼‚å¸¸ä»¤ç‰Œèµ‹äºˆå±€éƒ¨è¯­ä¹‰ä¿¡æ¯ã€‚

---

### 3.2 è‡ªè°ƒæ•´ç­–ç•¥ï¼šè¯­ä¹‰ä¸€è‡´æ€§å¢å¼º

#### åŠ¨æœºï¼š
- CLIP çš„ä¸­é—´å±‚å…·æœ‰å¼ºç©ºé—´ä¸€è‡´æ€§ï¼Œä½†è¯­ä¹‰è¾ƒå¼±ï¼›æœ€åä¸€å±‚è¯­ä¹‰å¼ºä½†ç©ºé—´ä¸€è‡´æ€§å·®ã€‚

#### æ–¹æ³•ï¼š
1. **ç‰¹å¾èšåˆ**ï¼š
   - ä½¿ç”¨ä¸­é—´å±‚ç›¸ä¼¼æ€§å›¾ **Sim**áµâ±áµˆ å¯¹æ·±å±‚ç‰¹å¾ **X**áµˆáµ‰áµ‰áµ– è¿›è¡ŒåŠ æƒèšåˆï¼š
```math
     \hat{\mathbb{X}}_{p}^{deep} = \sum_{q=1}^{N} \text{Norm}(\mathbf{Sim}_{(p,q)}^{mid}) \cdot \mathbb{X}_{q}^{deep}
```
2. **æ³¨æ„åŠ›å¢å¼º**ï¼š
   - åœ¨æ³¨æ„åŠ›æƒé‡ä¸­åŠ å…¥ä¸­é—´å±‚ç›¸ä¼¼æ€§ï¼š
```math
     \text{attn\_weight} = \text{softmax}(\mathbf{KK}^\top) + \text{softmax}(\mathbf{Sim}^{mid})
```

#### æ•ˆæœï¼š
- æå‡è¯­ä¹‰ä¸€è‡´æ€§ï¼ŒAUC ä» 0.66 æå‡è‡³ 0.80ã€‚

---

### 3.3 å¤šçº§ç‰¹å¾èåˆ

#### æŒ‘æˆ˜ï¼š
- ç›´æ¥èåˆä¸åŒå±‚ç‰¹å¾ä¼šç ´å CLIP çš„è·¨æ¨¡æ€å¯¹é½èƒ½åŠ›ã€‚

#### ç­–ç•¥ï¼šä¸¤éå‰å‘ä¼ æ’­
- ç¬¬ä¸€éï¼š**L**(**X**áµ–áµ‰â¿áµ˜Ë¡)
- ç¬¬äºŒéï¼š**L**(âˆ‘áµ¢âˆˆğ“œ **X**â±)
- æœ€ç»ˆè¾“å‡ºï¼š**L**(**X**áµ–áµ‰â¿áµ˜Ë¡) + **L**(âˆ‘áµ¢âˆˆğ“œ **X**â±)

#### åŸåˆ™ï¼š
- ä½¿ç”¨æœ€åä¸€å±‚å‚æ•°å¯¹é½ç‰¹å¾ï¼Œç¡®ä¿å…¼å®¹æ€§ã€‚
- ä¿æŒæœ€åä¸€å±‚ç‰¹å¾çš„å®Œæ•´æ€§ï¼Œä¸ç ´åæ–‡æœ¬å¯¹é½ã€‚

---

## ğŸ“Š å››ã€å®éªŒä¸ç»“æœ

### 4.1 å®éªŒè®¾ç½®
- **æ•°æ®é›†**ï¼š8 ä¸ªå¸¸ç”¨è¯­ä¹‰åˆ†å‰²æ•°æ®é›†ï¼ŒåŒ…æ‹¬ VOCã€Contextã€COCOã€Cityscapesã€ADE20K ç­‰ã€‚
- **è¯„ä¼°æŒ‡æ ‡**ï¼šmIoU
- **æ¨¡å‹**ï¼šCLIP ViT-B/16 å’Œ ViT-L/14
- **å®ç°ç»†èŠ‚**ï¼šåŸºäº MMSegmentationï¼Œæ»‘åŠ¨çª—å£æ¨ç†ï¼Œæ— åå¤„ç†ã€‚

---

### 4.2 ä¸»è¦ç»“æœ
- **SC-CLIP ViT-B/16**ï¼šå¹³å‡ mIoU 43.9%ï¼Œæ¯”ä¹‹å‰æœ€ä½³æ–¹æ³•æå‡ 9.5%ã€‚
- **SC-CLIP ViT-L/14**ï¼šå¹³å‡ mIoU 45.2%ï¼Œæ¯”ä¹‹å‰æœ€ä½³æ–¹æ³•æå‡ 3.5%ã€‚
- **æå‡å¹…åº¦**ï¼š
  - ViT-B/16ï¼šæ€§èƒ½æå‡ 3 å€
  - ViT-L/14ï¼šæ€§èƒ½æå‡ 6.8 å€

---

### 4.3 æ¶ˆèå®éªŒ

| ç­–ç•¥ | è´¡çŒ®ï¼ˆmIoUâ†‘ï¼‰ |
|------|---------------|
| åŸºçº¿ï¼ˆSCLIPï¼‰ | 36.7% |
| + å¼‚å¸¸ä»¤ç‰Œä¿®å¤ | +1.2% |
| + æ³¨æ„åŠ›å¢å¼º | +0.9% |
| + ç‰¹å¾èšåˆ | +0.8% |
| + å¤šçº§èåˆ | +1.6% |
| **SC-CLIP æœ€ç»ˆ** | **41.2%** |

---

### 4.4 æ•ˆç‡åˆ†æ
- ç›¸æ¯” ProxyCLIPï¼ˆä¾èµ– DINOï¼‰ï¼ŒSC-CLIPï¼š
  - FLOPsï¼šä» 34.4G é™è‡³ 17.5G
  - FPSï¼šä» 3.9 æå‡è‡³ 6.5
  - æ— éœ€é¢å¤–éª¨å¹²ç½‘ç»œ

---

## ğŸ¨ äº”ã€å¯è§†åŒ–ç»“æœ

- SC-CLIP ç”Ÿæˆçš„åˆ†å‰²å›¾æ›´æ¸…æ™°ã€è¯­ä¹‰æ›´ä¸€è‡´ã€‚
- ç›¸æ¯” CLIPã€MaskCLIPã€SCLIPã€ClearCLIP ç­‰æ–¹æ³•ï¼ŒSC-CLIP åœ¨å¤æ‚åœºæ™¯ä¸­è¡¨ç°æ›´ä¼˜ã€‚

---

## âœ… å…­ã€ç»“è®º

- æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„ CLIP è‡ªæ ¡å‡†æ–¹æ³• SC-CLIPã€‚
- é€šè¿‡è¯†åˆ«å¹¶ä¿®å¤å¼‚å¸¸ä»¤ç‰Œã€åˆ©ç”¨ä¸­é—´å±‚è¯­ä¹‰ä¸€è‡´æ€§ã€å¤šçº§ç‰¹å¾èåˆï¼Œæ˜¾è‘—æå‡äº† CLIP åœ¨å¼€æ”¾è¯æ±‡åˆ†å‰²ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚
- åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ° SOTAï¼Œä¸”æ— éœ€é¢å¤–æ•°æ®ã€å‚æ•°æˆ–éª¨å¹²ç½‘ç»œã€‚

---

## ğŸ“Œ æ€»ç»“äº®ç‚¹

| æ–¹é¢ | è´¡çŒ® |
|------|------|
| **é—®é¢˜è¯Šæ–­** | é¦–æ¬¡ç³»ç»Ÿåˆ†æ CLIP ä¸­çš„å¼‚å¸¸ä»¤ç‰Œé—®é¢˜ |
| **æ–¹æ³•åˆ›æ–°** | è®­ç»ƒ-freeã€è‡ªæ ¡å‡†ã€å¤šç­–ç•¥èåˆ |
| **æ€§èƒ½æå‡** | åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³• |
| **æ•ˆç‡ä¼˜åŠ¿** | æ— éœ€é¢å¤–éª¨å¹²ï¼Œè®¡ç®—æˆæœ¬ä½ |
| **é€šç”¨æ€§å¼º** | é€‚ç”¨äºä¸åŒ CLIP å˜ä½“ï¼ˆViT-B/16ã€ViT-L/14ï¼‰ |



# SC-CLIPè®ºæ–‡æ ¸å¿ƒæŠ€æœ¯æ·±åº¦è§£æ

## 1. å¼‚å¸¸Tokenå‰Šå¼±ç®—æ³•è¯¦è§£

### 1.1 é—®é¢˜èƒŒæ™¯ä¸è¯†åˆ«

#### 1.1.1 å¼‚å¸¸Tokençš„ç°è±¡
```python
# å¼‚å¸¸Tokenåœ¨CLIPä¸­çš„è¡¨ç°ç‰¹å¾
å¼‚å¸¸Tokenç‰¹å¾ = {
    "æ³¨æ„åŠ›è¿‡åº¦é›†ä¸­": "æ‰€æœ‰patchéƒ½è¿‡åº¦å…³æ³¨è¿™äº›åŒºåŸŸ",
    "ç‰¹å¾ç©ºé—´åç¦»": "åœ¨PCAåˆ†æä¸­è¿œç¦»æ­£å¸¸tokenåˆ†å¸ƒ", 
    "è¯­ä¹‰ä¿¡æ¯ç¼ºå¤±": "ç¼ºä¹å±€éƒ¨ä½ç½®çš„å…·ä½“è¯­ä¹‰ä¿¡æ¯",
    "ç ´åç©ºé—´ä¸€è‡´æ€§": "å¯¼è‡´æ³¨æ„åŠ›åˆ†å¸ƒå‡åŒ€åŒ–"
}
```

#### 1.1.2 å¼‚å¸¸Tokençš„è´Ÿé¢å½±å“
- **æ³¨æ„åŠ›æœºåˆ¶å¤±æ•ˆ**ï¼šæ­£å¸¸Tokenè¿‡åº¦å…³æ³¨å¼‚å¸¸Token
- **ç‰¹å¾åŒè´¨åŒ–**ï¼šä¸åŒä½ç½®çš„ç‰¹å¾å˜å¾—ç›¸ä¼¼
- **å±€éƒ¨ç»†èŠ‚ä¸¢å¤±**ï¼šæ— æ³•æ•æ‰ç»†ç²’åº¦ç©ºé—´ä¿¡æ¯

### 1.2 LOFå¼‚å¸¸æ£€æµ‹ç®—æ³•è¯¦è§£

#### 1.2.1 LOFç®—æ³•æ ¸å¿ƒæ¦‚å¿µ

**æ•°å­¦å®šä¹‰ï¼š**

è®¾æ•°æ®é›†ä¸º $D$ï¼Œå¯¹äºç‚¹ $p \in D$ï¼Œå®šä¹‰ï¼š

1. **k-è·ç¦»**ï¼š
```math
   $$
   \text{k-dist}(p) = \text{è·ç¦»}(p, o) \quad \text{å…¶ä¸­} o \text{æ˜¯ç¬¬kè¿‘çš„é‚»å±…}
   $$
```
3. **k-è·ç¦»é‚»åŸŸ**ï¼š
```math
   $$
   N_k(p) = \{q \in D \setminus \{p\} \mid \text{dist}(p,q) \leq \text{k-dist}(p)\}
   $$
```
4. **å¯è¾¾è·ç¦»**ï¼š
```math
    $$
   \text{reach-dist}_k(p, o) = \max(\text{k-dist}(o), \text{dist}(p, o))
   $$
```
5. **å±€éƒ¨å¯è¾¾å¯†åº¦**ï¼š
```math
    $$
   \text{lrd}_k(p) = \frac{|N_k(p)|}{\sum_{o \in N_k(p)} \text{reach-dist}_k(p, o)}
   $$
```
6. **å±€éƒ¨ç¦»ç¾¤å› å­**ï¼š
```math
   $$
   \text{LOF}_k(p) = \frac{\sum_{o \in N_k(p)} \frac{\text{lrd}_k(o)}{\text{lrd}_k(p)}}{|N_k(p)|}
   $$
```
#### 1.2.2 LOFåœ¨SC-CLIPä¸­çš„å…·ä½“å®ç°

```python
import torch
import numpy as np
from sklearn.neighbors import LocalOutlierFactor

class AnomalyTokenDetector:
    def __init__(self, contamination=0.05):
        self.contamination = contamination
        self.lof = LocalOutlierFactor(
            n_neighbors=20, 
            contamination=contamination,
            novelty=False
        )
    
    def detect_anomalies(self, features):
        """
        æ£€æµ‹å¼‚å¸¸Token
        Args:
            features: Tensor of shape [N, D], Nä¸ªtoken, Dç»´ç‰¹å¾
        Returns:
            anomaly_mask: Boolean mask of shape [N], Trueè¡¨ç¤ºå¼‚å¸¸
        """
        # å°†ç‰¹å¾è½¬æ¢ä¸ºnumpyæ•°ç»„
        features_np = features.cpu().numpy()
        
        # ä½¿ç”¨LOFè¿›è¡Œå¼‚å¸¸æ£€æµ‹
        anomaly_scores = self.lof.fit_predict(features_np)
        
        # -1è¡¨ç¤ºå¼‚å¸¸, 1è¡¨ç¤ºæ­£å¸¸
        anomaly_mask = (anomaly_scores == -1)
        
        return torch.from_numpy(anomaly_mask).to(features.device)
```

### 1.3 å¼‚å¸¸Tokenä¿®å¤ç®—æ³•

#### 1.3.1 ç©ºé—´é‚»åŸŸæ’å€¼å…¬å¼è¯¦è§£

è®ºæ–‡ä¸­ä½¿ç”¨çš„ä¿®å¤å…¬å¼ï¼š
```math
$$
\tilde{\mathbb{X}}_{(x,y)}^{penul} = \frac{\sum_{i=-1}^{1}\sum_{j=-1}^{1} w_{i,j} \cdot \mathbb{X}_{(x+i,y+j)}^{penul}}{\sum_{i=-1}^{1}\sum_{j=-1}^{1} w_{i,j}}, \quad \forall(x,y) \in \mathcal{A}
$$
```
å…¶ä¸­æƒé‡å®šä¹‰ä¸ºï¼š

$$
w_{i,j} = \begin{cases} 
0, & \text{if } (x+i,y+j) \in \mathcal{A} \\
1, & \text{otherwise}
\end{cases}
$$

**å…¬å¼åˆ†è§£è¯´æ˜ï¼š**

- $\tilde{\mathbb{X}}_{(x,y)}^{penul}$ï¼šä¿®å¤åçš„ç‰¹å¾åœ¨ä½ç½® $(x,y)$ çš„å€¼
- $\mathcal{A}$ï¼šå¼‚å¸¸Tokençš„é›†åˆ
- $w_{i,j}$ï¼š3Ã—3é‚»åŸŸå†…çš„æƒé‡ï¼Œæ’é™¤å…¶ä»–å¼‚å¸¸Token
- åˆ†å­ï¼šæœ‰æ•ˆé‚»åŸŸç‰¹å¾çš„åŠ æƒå’Œ
- åˆ†æ¯ï¼šå½’ä¸€åŒ–å› å­ï¼Œç¡®ä¿æƒé‡å’Œä¸º1

#### 1.3.2 å…·ä½“å®ç°ä»£ç 

```python
def resolve_anomaly_tokens(feature_map, anomaly_mask, kernel_size=3):
    """
    ä¿®å¤å¼‚å¸¸Token
    Args:
        feature_map: Tensor of shape [H, W, D], ç©ºé—´ç‰¹å¾å›¾
        anomaly_mask: Tensor of shape [H, W], å¼‚å¸¸æ ‡è®°
        kernel_size: é‚»åŸŸå¤§å°
    Returns:
        repaired_features: ä¿®å¤åçš„ç‰¹å¾å›¾
    """
    H, W, D = feature_map.shape
    repaired_features = feature_map.clone()
    padding = kernel_size // 2
    
    # åˆ›å»ºpaddingåçš„ç‰¹å¾å›¾å’Œmask
    feature_padded = torch.nn.functional.pad(
        feature_map.permute(2, 0, 1).unsqueeze(0), 
        (padding, padding, padding, padding), 
        mode='reflect'
    ).squeeze(0).permute(1, 2, 0)
    
    mask_padded = torch.nn.functional.pad(
        anomaly_mask.unsqueeze(0).unsqueeze(0),
        (padding, padding, padding, padding),
        mode='constant', value=1  # è¾¹ç•Œè§†ä¸ºå¼‚å¸¸ï¼Œä¸å‚ä¸è®¡ç®—
    ).squeeze(0).squeeze(0)
    
    # ä¸ºæ¯ä¸ªå¼‚å¸¸ä½ç½®è¿›è¡Œæ’å€¼
    anomaly_positions = torch.where(anomaly_mask)
    
    for y, x in zip(anomaly_positions[0], anomaly_positions[1]):
        # è·å–é‚»åŸŸåŒºåŸŸ (åœ¨paddedåæ ‡ç³»ä¸­)
        y_start, y_end = y, y + kernel_size
        x_start, x_end = x, x + kernel_size
        
        neighbor_features = feature_padded[y_start:y_end, x_start:x_end, :]  # [3, 3, D]
        neighbor_mask = mask_padded[y_start:y_end, x_start:x_end]  # [3, 3]
        
        # åˆ›å»ºæƒé‡çŸ©é˜µï¼Œæ’é™¤å¼‚å¸¸é‚»å±…
        weights = (neighbor_mask == 0).float()  # æ­£å¸¸ä½ç½®æƒé‡ä¸º1ï¼Œå¼‚å¸¸ä¸º0
        
        # å¦‚æœæ‰€æœ‰é‚»å±…éƒ½æ˜¯å¼‚å¸¸ï¼Œä½¿ç”¨æœ€è¿‘çš„éå¼‚å¸¸å€¼
        if weights.sum() == 0:
            # å¯»æ‰¾æœ€è¿‘çš„éå¼‚å¸¸ä½ç½®
            normal_positions = torch.where(~anomaly_mask)
            if len(normal_positions[0]) > 0:
                distances = (normal_positions[0] - y)**2 + (normal_positions[1] - x)**2
                nearest_idx = torch.argmin(distances)
                nearest_y, nearest_x = normal_positions[0][nearest_idx], normal_positions[1][nearest_idx]
                repaired_features[y, x, :] = feature_map[nearest_y, nearest_x, :]
            continue
        
        # è®¡ç®—åŠ æƒå¹³å‡
        weighted_sum = torch.zeros(D, device=feature_map.device)
        total_weight = 0
        
        for i in range(kernel_size):
            for j in range(kernel_size):
                if weights[i, j] > 0:  # åªè€ƒè™‘æ­£å¸¸é‚»å±…
                    weight = weights[i, j]
                    weighted_sum += weight * neighbor_features[i, j, :]
                    total_weight += weight
        
        if total_weight > 0:
            repaired_features[y, x, :] = weighted_sum / total_weight
    
    return repaired_features
```

### 1.4 æœ‰æ•ˆæ€§åŸç†åˆ†æ

#### 1.4.1 æ•°å­¦å±‚é¢çš„æœ‰æ•ˆæ€§

**1. æ³¨æ„åŠ›æ­£åˆ™åŒ–ï¼š**
```math
$$
\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$
```
ä¿®å¤å‰ï¼šæŸäº› $K$ å¼‚å¸¸ï¼Œå¯¼è‡´æ³¨æ„åŠ›åˆ†å¸ƒå¼‚å¸¸
ä¿®å¤åï¼š$K$ æ¢å¤æ­£å¸¸ï¼Œæ³¨æ„åŠ›é‡æ–°èšç„¦ç›¸å…³åŒºåŸŸ

**2. ç‰¹å¾ç©ºé—´ä¼˜åŒ–ï¼š**
ä¿®å¤æ“ä½œç›¸å½“äºåœ¨ç‰¹å¾ç©ºé—´ä¸­è¿›è¡Œï¼š
```math
$$
\tilde{X} = X + \Delta X
$$
```
å…¶ä¸­ $\Delta X$ æ˜¯åŸºäºç©ºé—´å¹³æ»‘å…ˆéªŒçš„ä¿®æ­£é¡¹ã€‚

#### 1.4.2 ç›´è§‚ç†è§£

**æ¯”å–»è¯´æ˜ï¼š**

æƒ³è±¡CLIPæ˜¯ä¸€ä¸ª**ä¼šè®®è®¨è®ºç³»ç»Ÿ**ï¼š

- **æ­£å¸¸Token**ï¼šå„ä¸ªéƒ¨é—¨çš„ä»£è¡¨ï¼Œè®¨è®ºå…·ä½“ä¸šåŠ¡
- **å¼‚å¸¸Token**ï¼šå‡ ä¸ªå¤§å—“é—¨çš„å¹²æ‰°è€…ï¼Œä¸åœå–Šæ— å…³è¯é¢˜
- **æ•ˆæœ**ï¼šæ‰€æœ‰äººéƒ½ä¸è‡ªè§‰åœ°å»å¬å¹²æ‰°è€…ï¼Œæ— æ³•ä¸“æ³¨æœ¬èŒå·¥ä½œ

**ä¿®å¤è¿‡ç¨‹ï¼š**
1. **è¯†åˆ«å¹²æ‰°è€…**ï¼ˆLOFæ£€æµ‹ï¼‰ï¼šæ‰¾åˆ°é‚£äº›å‘è¨€å†…å®¹ä¸å¤§å®¶å®Œå…¨ä¸åŒçš„äºº
2. **æ¸©å’Œçº æ­£**ï¼ˆç©ºé—´æ’å€¼ï¼‰ï¼šæ ¹æ®å‘¨å›´åŒäº‹çš„åˆç†æ„è§ï¼Œç»™å¹²æ‰°è€…æä¾›ç¬¦åˆåœºæ™¯çš„å‘è¨€å†…å®¹
3. **ç³»ç»Ÿæ¢å¤**ï¼šå¤§å®¶é‡æ–°å…³æ³¨ä¸šåŠ¡ç›¸å…³è®¨è®ºï¼Œä¼šè®®æ•ˆç‡æå‡

## 2. ä¸­é—´å±‚ä¸æœ€åä¸€å±‚è‡ªé€‚åº”èšåˆè¯¦è§£

### 2.1 é—®é¢˜èƒŒæ™¯ä¸åŠ¨æœº

#### 2.1.1 CLIPå„å±‚ç‰¹æ€§åˆ†æ

```python
# CLIPä¸åŒå±‚çº§çš„ç‰¹å¾ç‰¹æ€§
layer_characteristics = {
    "æµ…å±‚ç‰¹å¾ (Layer 1-4)": {
        "ç©ºé—´ç»†èŠ‚": "ä¸°å¯Œ",
        "è¯­ä¹‰æŠ½è±¡": "ä½çº§", 
        "ä¸€è‡´æ€§": "å±€éƒ¨ä¸€è‡´",
        "é€‚ç”¨æ€§": "è¾¹ç¼˜ã€çº¹ç†"
    },
    "ä¸­å±‚ç‰¹å¾ (Layer 5-9)": {
        "ç©ºé—´ç»†èŠ‚": "ä¸­ç­‰",
        "è¯­ä¹‰æŠ½è±¡": "ä¸­çº§",
        "ä¸€è‡´æ€§": "è¯­ä¹‰ä¸€è‡´", 
        "é€‚ç”¨æ€§": "ç‰©ä½“éƒ¨ä»¶"
    },
    "æ·±å±‚ç‰¹å¾ (Layer 10-12)": {
        "ç©ºé—´ç»†èŠ‚": "è´«ä¹",
        "è¯­ä¹‰æŠ½è±¡": "é«˜çº§",
        "ä¸€è‡´æ€§": "å…¨å±€ä¸€è‡´",
        "é€‚ç”¨æ€§": "ç±»åˆ«è¯†åˆ«"
    }
}
```

#### 2.1.2 æ ¸å¿ƒçŸ›ç›¾
- **æœ€åä¸€å±‚**ï¼šè¯­ä¹‰ä¸°å¯Œä½†ç©ºé—´ä¸€è‡´æ€§å·®ï¼ˆAUC=0.66ï¼‰
- **ä¸­é—´å±‚**ï¼šç©ºé—´ä¸€è‡´æ€§å¼ºä½†è¯­ä¹‰æœ‰é™ï¼ˆAUC=0.76ï¼‰
- **ç›®æ ‡**ï¼šç»“åˆä¸¤è€…çš„ä¼˜åŠ¿

### 2.2 è‡ªé€‚åº”ç‰¹å¾èšåˆç®—æ³•

#### 2.2.1 ç›¸ä¼¼æ€§çŸ©é˜µè®¡ç®—

**ä½™å¼¦ç›¸ä¼¼åº¦å…¬å¼ï¼š**
```math
$$
\mathbf{Sim}^{mid} = \frac{\mathbb{X}^{mid} \cdot (\mathbb{X}^{mid})^T}{\|\mathbb{X}^{mid}\| \cdot \|(\mathbb{X}^{mid})^T\|}
$$
```
å…¶ä¸­ï¼š
- $\mathbb{X}^{mid} \in \mathbb{R}^{N \times D}$ï¼šä¸­é—´å±‚ç‰¹å¾çŸ©é˜µ
- $N$ï¼špatchæ•°é‡
- $D$ï¼šç‰¹å¾ç»´åº¦
- $\mathbf{Sim}^{mid} \in \mathbb{R}^{N \times N}$ï¼šç›¸ä¼¼æ€§çŸ©é˜µ

**ä»£ç å®ç°ï¼š**
```python
def compute_similarity_matrix(features, normalize=True):
    """
    è®¡ç®—ç‰¹å¾ç›¸ä¼¼æ€§çŸ©é˜µ
    Args:
        features: Tensor of shape [N, D], Nä¸ªç‰¹å¾å‘é‡
        normalize: æ˜¯å¦è¿›è¡Œå½’ä¸€åŒ–
    Returns:
        similarity: Tensor of shape [N, N], ç›¸ä¼¼æ€§çŸ©é˜µ
    """
    if normalize:
        # L2å½’ä¸€åŒ–
        features = torch.nn.functional.normalize(features, p=2, dim=1)
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    similarity = torch.mm(features, features.T)
    
    return similarity

# å…·ä½“åº”ç”¨
X_mid = intermediate_features  # [N, D], ä¸­é—´å±‚ç‰¹å¾
Sim_mid = compute_similarity_matrix(X_mid)  # [N, N]
```

#### 2.2.2 ç‰¹å¾èšåˆå…¬å¼è¯¦è§£

**æ ¸å¿ƒèšåˆå…¬å¼ï¼š**
```math
$$
\hat{\mathbb{X}}_{p}^{deep} = \sum_{q=1}^{N} \text{Norm}(\mathbf{Sim}_{(p,q)}^{mid}) \cdot \mathbb{X}_{q}^{deep}
$$
```
**å…¬å¼åˆ†è§£ï¼š**

1. **è¾“å…¥**ï¼š
   - $\mathbb{X}^{deep} \in \mathbb{R}^{N \times D}$ï¼šæ·±å±‚ç‰¹å¾
   - $\mathbf{Sim}^{mid} \in \mathbb{R}^{N \times N}$ï¼šä¸­é—´å±‚ç›¸ä¼¼æ€§çŸ©é˜µ

2. **å½’ä¸€åŒ–æ“ä½œ** $\text{Norm}$ï¼š
   - å¯¹æ¯ä¸ªä½ç½® $p$ï¼Œå¯¹å…¶ä¸å…¶ä»–æ‰€æœ‰ä½ç½® $q$ çš„ç›¸ä¼¼åº¦è¿›è¡Œsoftmaxå½’ä¸€åŒ–ï¼š
```math
     $$
     \text{Norm}(\mathbf{Sim}_{(p,q)}^{mid}) = \frac{\exp(\mathbf{Sim}_{(p,q)}^{mid})}{\sum_{r=1}^{N} \exp(\mathbf{Sim}_{(p,r)}^{mid})}
     $$
```
3. **èšåˆè¿‡ç¨‹**ï¼š
   - å¯¹äºæ¯ä¸ªç›®æ ‡ä½ç½® $p$ï¼Œè®¡ç®—åŠ æƒå’Œ
   - æƒé‡ç”±ä¸­é—´å±‚ç›¸ä¼¼æ€§å†³å®šï¼šè¯­ä¹‰è¶Šç›¸ä¼¼ï¼Œè´¡çŒ®è¶Šå¤§

**ä»£ç å®ç°ï¼š**
```python
def adaptive_feature_aggregation(deep_features, similarity_matrix, temperature=1.0):
    """
    è‡ªé€‚åº”ç‰¹å¾èšåˆ
    Args:
        deep_features: Tensor [N, D], æ·±å±‚ç‰¹å¾
        similarity_matrix: Tensor [N, N], ç›¸ä¼¼æ€§çŸ©é˜µ  
        temperature: softmaxæ¸©åº¦å‚æ•°
    Returns:
        aggregated_features: Tensor [N, D], èšåˆåçš„ç‰¹å¾
    """
    N, D = deep_features.shape
    
    # å¯¹ç›¸ä¼¼æ€§çŸ©é˜µè¿›è¡Œsoftmaxå½’ä¸€åŒ–
    # åº”ç”¨æ¸©åº¦å‚æ•°æ§åˆ¶åˆ†å¸ƒå°–é”ç¨‹åº¦
    attention_weights = torch.nn.functional.softmax(
        similarity_matrix / temperature, dim=1
    )  # [N, N]
    
    # ç‰¹å¾èšåˆ: [N, N] Ã— [N, D] = [N, D]
    aggregated_features = torch.mm(attention_weights, deep_features)
    
    return aggregated_features

# å®é™…åº”ç”¨
X_deep = deep_features  # [N, D], æ·±å±‚ç‰¹å¾
X_hat_deep = adaptive_feature_aggregation(X_deep, Sim_mid)  # èšåˆåçš„ç‰¹å¾
```

### 2.3 æ³¨æ„åŠ›å¢å¼ºæœºåˆ¶

#### 2.3.1 æ³¨æ„åŠ›å¢å¼ºå…¬å¼

è®ºæ–‡æå‡ºçš„æ³¨æ„åŠ›å¢å¼ºï¼š
```math
$$
\text{attn\_weight} = \text{softmax}(\mathbf{KK}^{\top}) + \text{softmax}(\mathbf{Sim}^{mid})
$$
```
**ä¼ ç»Ÿè‡ªæ³¨æ„åŠ›**ï¼š
```math
$$
\text{Attention} = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$
```
**SC-CLIPçš„æ”¹è¿›**ï¼š
```math
$$
\text{EnhancedAttention} = \left[\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) + \text{softmax}(\mathbf{Sim}^{mid})\right]V
$$
```
#### 2.3.2 å®ç°ä»£ç 
```python
def enhanced_self_attention(Q, K, V, mid_similarity, scale_factor=None):
    """
    å¢å¼ºçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶
    Args:
        Q, K, V: æŸ¥è¯¢ã€é”®ã€å€¼çŸ©é˜µ [N, d_k]
        mid_similarity: ä¸­é—´å±‚ç›¸ä¼¼æ€§çŸ©é˜µ [N, N]
        scale_factor: ç¼©æ”¾å› å­
    Returns:
        output: æ³¨æ„åŠ›è¾“å‡º [N, d_k]
    """
    if scale_factor is None:
        scale_factor = 1.0 / (K.size(-1) ** 0.5)
    
    # ä¼ ç»ŸQKæ³¨æ„åŠ›
    qk_attention = torch.matmul(Q, K.transpose(-2, -1)) * scale_factor
    qk_weights = torch.nn.functional.softmax(qk_attention, dim=-1)
    
    # ä¸­é—´å±‚ç›¸ä¼¼æ€§æ³¨æ„åŠ›
    mid_weights = torch.nn.functional.softmax(mid_similarity, dim=-1)
    
    # ç»“åˆä¸¤ç§æ³¨æ„åŠ›
    combined_weights = qk_weights + mid_weights
    
    # åº”ç”¨æ³¨æ„åŠ›æƒé‡
    output = torch.matmul(combined_weights, V)
    
    return output
```

### 2.4 å¤šçº§ç‰¹å¾èåˆç­–ç•¥

#### 2.4.1 èåˆåŸåˆ™ä¸æŒ‘æˆ˜

**ä¼ ç»Ÿæ–¹æ³•çš„ç¼ºé™·ï¼š**
- ç›´æ¥ç›¸åŠ ï¼š
```math
$\mathbb{X}^{last} + \sum_{i\in\mathcal{M}}\mathbb{X}^{i}$ â†’ ç›¸ä¼¼åº¦ä»…0.094
```
- ç ´åè·¨æ¨¡æ€å¯¹é½èƒ½åŠ›

**SC-CLIPçš„è§£å†³æ–¹æ¡ˆï¼š**
1. **ä½¿ç”¨æœ€åä¸€å±‚è¿›è¡Œå¯¹é½**ï¼š
```math
   $$
   \text{ç›¸ä¼¼åº¦}(\mathbb{X}^{last}, \mathbf{L}(\sum_{i\in\mathcal{M}}\mathbb{X}^{i})) = 0.983
   $$
```
3. **ä¿æŒæœ€åä¸€å±‚å®Œæ•´æ€§**

#### 2.4.2 ä¸¤éå‰å‘ä¼ æ’­ç­–ç•¥

**æ•°å­¦è¡¨è¾¾ï¼š**
```math
$$
\text{Output} = \mathbf{L}(\mathbb{X}^{penul}) + \mathbf{L}\left(\sum_{i\in\mathcal{M}}\mathbb{X}^{i}\right)
$$
```
å…¶ä¸­ï¼š
- $\mathbf{L}$ï¼šCLIPçš„æœ€åä¸€å±‚å˜æ¢
- $\mathbb{X}^{penul}$ï¼šå€’æ•°ç¬¬äºŒå±‚ç‰¹å¾
- $\mathcal{M}$ï¼šé€‰ä¸­çš„å¤šå±‚çº§ç‰¹å¾é›†åˆ

**ä»£ç å®ç°ï¼š**
```python
class MultiLevelFusion:
    def __init__(self, selected_layers=[4, 5, 6, 7, 8, 9, 10]):
        self.selected_layers = selected_layers
    
    def two_pass_fusion(self, clip_model, image):
        """
        ä¸¤éå‰å‘ä¼ æ’­èåˆ
        Args:
            clip_model: CLIPæ¨¡å‹
            image: è¾“å…¥å›¾åƒ
        Returns:
            fused_features: èåˆåçš„ç‰¹å¾
        """
        # ç¬¬ä¸€é: åŸå§‹å‰å‘ä¼ æ’­
        with torch.no_grad():
            # è·å–åŸå§‹æœ€åä¸€å±‚ç‰¹å¾
            original_output = clip_model.visual(image)
            original_features = original_output[:, 1:, :]  # æ’é™¤[CLS] token
            
            # è·å–å¤šå±‚çº§ç‰¹å¾
            multi_level_features = []
            hooks = []
            
            def hook_fn(module, input, output, layer_id):
                if layer_id in self.selected_layers:
                    multi_level_features.append(output[:, 1:, :])  # æ’é™¤[CLS]
            
            # æ³¨å†Œé’©å­
            for idx, layer in enumerate(clip_model.visual.transformer.resblocks):
                hook = layer.register_forward_hook(
                    lambda m, i, o, idx=idx: hook_fn(m, i, o, idx)
                )
                hooks.append(hook)
            
            # ç¬¬äºŒé: å¤šå±‚çº§ç‰¹å¾å‰å‘ä¼ æ’­
            _ = clip_model.visual(image)
            
            # ç§»é™¤é’©å­
            for hook in hooks:
                hook.remove()
        
        # ç‰¹å¾èåˆ
        multi_level_sum = torch.stack(multi_level_features).mean(dim=0)
        
        # ä½¿ç”¨æœ€åä¸€å±‚å˜æ¢å¯¹é½å¤šå±‚çº§ç‰¹å¾
        # è¿™é‡Œç®€åŒ–è¡¨ç¤ºï¼Œå®é™…éœ€è¦æ¨¡æ‹Ÿæœ€åä¸€å±‚çš„å˜æ¢
        aligned_multi_level = self.apply_last_layer_transform(multi_level_sum)
        
        # æœ€ç»ˆèåˆ
        fused_features = original_features + aligned_multi_level
        
        return fused_features
    
    def apply_last_layer_transform(self, features):
        """æ¨¡æ‹Ÿæœ€åä¸€å±‚çš„å˜æ¢"""
        # å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œéœ€è¦å¤åˆ¶æœ€åä¸€å±‚çš„æƒé‡å’Œæ“ä½œ
        return features  # ç®€åŒ–è¿”å›
```

### 2.5 æœ‰æ•ˆæ€§åˆ†æä¸æ•°å­¦åŸç†

#### 2.5.1 ç‰¹å¾ç©ºé—´çš„ä¼˜åŒ–

**èšåˆæ“ä½œçš„æœ¬è´¨ï¼š**
```math
$$
\hat{\mathbb{X}} = \mathbf{A} \mathbb{X}^{deep}
$$
```
å…¶ä¸­ $\mathbf{A} = \text{Norm}(\mathbf{Sim}^{mid})$ æ˜¯æ³¨æ„åŠ›çŸ©é˜µã€‚

è¿™ç›¸å½“äºåœ¨ç‰¹å¾ç©ºé—´ä¸­è¿›è¡Œ**éå±€éƒ¨å‡å€¼æ»¤æ³¢**ï¼Œä¿æŒè¯­ä¹‰ä¸€è‡´æ€§çš„åŒæ—¶å¢å¼ºç©ºé—´è¿ç»­æ€§ã€‚

#### 2.5.2 ä¿¡æ¯è®ºè§†è§’

ä»ä¿¡æ¯è®ºè§’åº¦çœ‹ï¼Œèšåˆæ“ä½œï¼š
- **å¢åŠ äº’ä¿¡æ¯**ï¼š
```math
$I(\hat{\mathbb{X}}; \mathbb{X}^{mid}) > I(\mathbb{X}^{deep}; \mathbb{X}^{mid})$
```
- **å‡å°‘ç†µ**ï¼š
```math
$H(\hat{\mathbb{X}}|\mathbb{X}^{mid}) < H(\mathbb{X}^{deep}|\mathbb{X}^{mid})$
```
- **æå‡è¯­ä¹‰ä¸€è‡´æ€§**

#### 2.5.3 å®éªŒéªŒè¯ç»“æœ

**å®šé‡ç»“æœï¼š**
- ç‰¹å¾èšåˆå¸¦æ¥ **0.8% mIoU** æå‡
- è¯­ä¹‰ä¸€è‡´æ€§AUCä» **0.66 â†’ 0.80**
- è¶…è¿‡DINOçš„0.77ï¼Œè¯æ˜CLIPå†…éƒ¨ç‰¹å¾çš„æ½œåŠ›

**æ¶ˆèå®éªŒï¼š**
| ç­–ç•¥ | mIoUæå‡ | ç´¯ç§¯mIoU |
|------|----------|----------|
| åŸºçº¿ | - | 36.7% |
| + å¼‚å¸¸Tokenä¿®å¤ | +1.2% | 37.9% |
| + æ³¨æ„åŠ›å¢å¼º | +0.9% | 38.8% |
| + ç‰¹å¾èšåˆ | +0.8% | 39.6% |
| + å¤šçº§èåˆ | +1.6% | 41.2% |

## 3. æ€»ç»“

SC-CLIPé€šè¿‡ä¸¤ä¸ªæ ¸å¿ƒæŠ€æœ¯å®ç°äº†CLIPåœ¨å¼€æ”¾è¯æ±‡åˆ†å‰²ä»»åŠ¡ä¸Šçš„æ˜¾è‘—æå‡ï¼š

### 3.1 å¼‚å¸¸Tokenå‰Šå¼±
- **é—®é¢˜è¯†åˆ«**ï¼šç³»ç»Ÿåˆ†æäº†CLIPä¸­å¼‚å¸¸Tokençš„å­˜åœ¨åŠå…¶ç ´åæ€§
- **ç®—æ³•åˆ›æ–°**ï¼šå°†LOFå¼‚å¸¸æ£€æµ‹ä¸ç©ºé—´æ’å€¼ç»“åˆ
- **æ•°å­¦åŸç†**ï¼šåŸºäºå±€éƒ¨å¯†åº¦æ¯”è¾ƒå’Œç©ºé—´è¿ç»­æ€§å…ˆéªŒ
- **æ•ˆæœ**ï¼šè§£æ”¾æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ¢å¤å±€éƒ¨æ„ŸçŸ¥èƒ½åŠ›

### 3.2 è‡ªé€‚åº”ç‰¹å¾èšåˆ  
- **æ ¸å¿ƒæ´å¯Ÿ**ï¼šå‘ç°å¹¶åˆ©ç”¨CLIPä¸­é—´å±‚çš„é«˜è´¨é‡ç©ºé—´ä¸€è‡´æ€§
- **æŠ€æœ¯åˆ›æ–°**ï¼šåŸºäºç›¸ä¼¼æ€§çš„è‡ªé€‚åº”åŠ æƒèšåˆ
- **æ•°å­¦åŸºç¡€**ï¼šéå±€éƒ¨å‡å€¼æ»¤æ³¢åœ¨ç‰¹å¾ç©ºé—´çš„æ‰©å±•
- **æ•ˆæœ**ï¼šç»“åˆæ·±å±‚è¯­ä¹‰ä¸ä¸­å±‚ç©ºé—´ä¸€è‡´æ€§ï¼Œæ˜¾è‘—æå‡åˆ†å‰²è´¨é‡

è¿™ä¸¤ç§æŠ€æœ¯å…±åŒä½œç”¨ï¼Œåœ¨ä¸å¼•å…¥é¢å¤–å‚æ•°æˆ–è®¡ç®—å¼€é”€çš„æƒ…å†µä¸‹ï¼Œå……åˆ†æŒ–æ˜äº†CLIPæ¨¡å‹å†…éƒ¨çš„æ½œåŠ›ï¼Œä¸ºè®­ç»ƒå…è´¹çš„å¼€æ”¾è¯æ±‡åˆ†å‰²è®¾ç«‹äº†æ–°çš„æŠ€æœ¯æ ‡æ†ã€‚
