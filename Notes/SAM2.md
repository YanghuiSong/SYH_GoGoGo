# SAM2ï¼ˆSegment Anything Model 2ï¼‰æŠ€æœ¯è¯¦è§£

## 1. è®ºæ–‡æ¦‚è¿°

### 1.1 æ ¸å¿ƒè´¡çŒ®
SAM2æ˜¯Meta FAIRå›¢é˜Ÿæ¨å‡ºçš„ç¬¬äºŒä»£"åˆ†å‰²ä¸€åˆ‡"æ¨¡å‹ï¼Œä¸»è¦åˆ›æ–°åŒ…æ‹¬ï¼š

- **ç»Ÿä¸€æ¶æ„**ï¼šåŒæ—¶æ”¯æŒå›¾åƒå’Œè§†é¢‘åˆ†å‰²ä»»åŠ¡
- **æµå¼å†…å­˜æœºåˆ¶**ï¼šå®ç°å®æ—¶è§†é¢‘å¤„ç†èƒ½åŠ›
- **å¤§è§„æ¨¡æ•°æ®é›†**ï¼šæ„å»ºäº†åŒ…å«3550ä¸‡ä¸ªæ©ç çš„SA-Væ•°æ®é›†
- **é«˜æ•ˆæ€§èƒ½**ï¼šæ¯”SAMå¿«6å€ï¼Œè§†é¢‘åˆ†å‰²äº¤äº’æ¬¡æ•°å‡å°‘3å€

### 1.2 æ ¸å¿ƒæŒ‡æ ‡å¯¹æ¯”
| æŒ‡æ ‡ | SAM | SAM2 | æå‡ |
|------|-----|------|------|
| å›¾åƒåˆ†å‰²é€Ÿåº¦ | 21.7 FPS | 130.1 FPS | 6Ã— |
| è§†é¢‘åˆ†å‰²äº¤äº’æ¬¡æ•° | - | å‡å°‘3å€ | - |
| æ•°æ®é›†è§„æ¨¡ | 11äº¿æ©ç  | 3550ä¸‡è§†é¢‘æ©ç  | - |

## 2. ä»»åŠ¡å®šä¹‰ï¼šPromptable Visual Segmentation (PVS)

### 2.1 ä»»åŠ¡å½¢å¼åŒ–å®šä¹‰
PVSä»»åŠ¡å¯ä»¥å½¢å¼åŒ–è¡¨ç¤ºä¸ºï¼š

ç»™å®šè§†é¢‘åºåˆ— $V = \{I_1, I_2, ..., I_T\}$ï¼Œå…¶ä¸­ $I_t$ æ˜¯ç¬¬tå¸§å›¾åƒï¼Œæ¨¡å‹æ¥æ”¶åœ¨ä»»æ„å¸§ä¸Šçš„æç¤ºé›†åˆ $P = \{p_1, p_2, ..., p_K\}$ï¼Œæ¯ä¸ªæç¤º $p_i = (t_i, type_i, location_i)$ åŒ…å«ï¼š
- å¸§ç´¢å¼• $t_i$
- æç¤ºç±»å‹ï¼ˆç‚¹ã€æ¡†ã€æ©ç ï¼‰
- ç©ºé—´ä½ç½®ä¿¡æ¯

æ¨¡å‹è¾“å‡ºä¸ºæ—¶ç©ºæ©ç åºåˆ— $M = \{m_1, m_2, ..., m_T\}$ï¼Œå…¶ä¸­ $m_t$ æ˜¯ç›®æ ‡å¯¹è±¡åœ¨ç¬¬tå¸§çš„åˆ†å‰²æ©ç ã€‚

### 2.2 ä¸ä¼ ç»Ÿä»»åŠ¡çš„å…³ç³»
```
PVSä»»åŠ¡ âŠ‡ {
    SAä»»åŠ¡ï¼ˆå•å¸§è§†é¢‘æƒ…å†µï¼‰,
    åŠç›‘ç£VOSï¼ˆä»…åœ¨é¦–å¸§æä¾›æ©ç æç¤ºï¼‰,
    äº¤äº’å¼VOSï¼ˆå¤šå¸§æ¶‚é¸¦æç¤ºï¼‰
}
```

## 3. æ¨¡å‹æ¶æ„è¯¦è§£

### 3.1 æ•´ä½“æ¶æ„å›¾
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   å›¾åƒç¼–ç å™¨     â”‚    â”‚   å†…å­˜æ³¨æ„åŠ›     â”‚    â”‚   æ©ç è§£ç å™¨     â”‚
â”‚   (Hiera)       â”‚â”€â”€â”€â–¶â”‚   (Transformer)  â”‚â”€â”€â”€â–¶â”‚   (Two-way)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                      â”‚                       â”‚
         â”‚                      â”‚                       â”‚
         â–¼                      â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œ  â”‚    â”‚    å†…å­˜é“¶è¡Œ      â”‚    â”‚  å¤šå°ºåº¦ç‰¹å¾èåˆ  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 æ ¸å¿ƒç»„ä»¶æ•°å­¦å»ºæ¨¡

#### 3.2.1 å›¾åƒç¼–ç å™¨
ä½¿ç”¨Hieraæ¶æ„ï¼ŒåŸºäºMAEé¢„è®­ç»ƒï¼Œé‡‡ç”¨åˆ†å±‚è®¾è®¡ï¼š

è®¾è¾“å…¥å¸§ $I_t âˆˆ â„^{HÃ—WÃ—3}$ï¼Œå›¾åƒç¼–ç å™¨è¾“å‡ºå¤šå°ºåº¦ç‰¹å¾ï¼š

math```
F_t = Encoder(I_t) = {f_tâ´, f_tâ¸, f_tÂ¹â¶, f_tÂ³Â²}
```
å…¶ä¸­ä¸Šæ ‡è¡¨ç¤ºä¸‹é‡‡æ ·å€æ•°ã€‚

#### 3.2.2 å†…å­˜æ³¨æ„åŠ›æœºåˆ¶
å†…å­˜æ³¨æ„åŠ›æ¨¡å—æ˜¯Transformeræ¶æ„ï¼Œè®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ï¼š

**è‡ªæ³¨æ„åŠ›**ï¼š
math```
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$
```
**äº¤å‰æ³¨æ„åŠ›**ï¼š
å½“å‰å¸§ç‰¹å¾ $F_t$ ä¸å†…å­˜é“¶è¡Œ $M_{bank}$ è¿›è¡Œäº¤å‰æ³¨æ„åŠ›ï¼š
math```
$$
F_t' = \text{CrossAttn}(F_t, M_{bank}) = \text{Attention}(F_tW_Q, M_{bank}W_K, M_{bank}W_V)
$$
```
#### 3.2.3 å†…å­˜é“¶è¡Œè®¾è®¡
å†…å­˜é“¶è¡Œç»´æŠ¤ä¸¤ç§ç±»å‹çš„ä¿¡æ¯ï¼š

1. **ç©ºé—´è®°å¿†**ï¼šå­˜å‚¨æœ€è¿‘Nå¸§çš„ç‰¹å¾å›¾
math```
   $$
   M_{spatial} = \{F_{t-N}', F_{t-N+1}', ..., F_{t-1}'\}
   $$
```
2. **å¯¹è±¡æŒ‡é’ˆ**ï¼šè½»é‡çº§å‘é‡ï¼Œç¼–ç é«˜çº§è¯­ä¹‰ä¿¡æ¯
math```
   $$
   P_{object} = \{p_{t_1}, p_{t_2}, ..., p_{t_M}\}
   $$
```
#### 3.2.4 æ©ç è§£ç å™¨
åŸºäºSAMçš„ä¸¤è·¯Transformerè®¾è®¡ï¼Œä½†å¢åŠ äº†æ”¹è¿›ï¼š

**å¤šå°ºåº¦ç‰¹å¾èåˆ**ï¼š
math```
$$
\text{Output} = \text{Decoder}(F_t' âŠ• \text{Upsample}(f_tâ´) âŠ• \text{Upsample}(f_tâ¸))
$$
```
**é®æŒ¡é¢„æµ‹å¤´**ï¼š
math```
$$
o_t = \sigma(W_o Â· h_t + b_o)
$$
```
å…¶ä¸­ $o_t âˆˆ [0,1]$ è¡¨ç¤ºç›®æ ‡åœ¨å½“å‰å¸§çš„å¯è§æ€§æ¦‚ç‡ã€‚

## 4. è®­ç»ƒç­–ç•¥è¯¦è§£

### 4.1 é¢„è®­ç»ƒé˜¶æ®µ
åœ¨SA-1Bæ•°æ®é›†ä¸Šè¿›è¡Œå›¾åƒåˆ†å‰²é¢„è®­ç»ƒï¼š

**æŸå¤±å‡½æ•°**ï¼š
math```
$$
\mathcal{L} = Î»_1\mathcal{L}_{focal} + Î»_2\mathcal{L}_{dice} + Î»_3\mathcal{L}_{IoU} + Î»_4\mathcal{L}_{occlusion}
$$
```
å…·ä½“æƒé‡ï¼š$\mathcal{L}_{focal}: \mathcal{L}_{dice}: \mathcal{L}_{IoU}: \mathcal{L}_{occlusion} = 20:1:1:1$

### 4.2 å…¨è®­ç»ƒé˜¶æ®µ
é‡‡ç”¨å›¾åƒå’Œè§†é¢‘äº¤æ›¿è®­ç»ƒç­–ç•¥ï¼š

**æ‰¹é‡é‡‡æ ·æ¦‚ç‡**ï¼š
math```
$$
P(\text{image}) = \frac{N_{image}}{N_{total}}, \quad P(\text{video}) = \frac{N_{video}}{N_{total}}
$$
```
**äº¤äº’æ¨¡æ‹Ÿ**ï¼š
- é‡‡æ ·8å¸§åºåˆ—
- éšæœºé€‰æ‹©æœ€å¤š2å¸§è¿›è¡Œæç¤º
- åˆå§‹æç¤ºï¼š50%æ©ç ï¼Œ25%ç‚¹å‡»ï¼Œ25%è¾¹ç•Œæ¡†

### 4.3 æ•°æ®å¢å¼ºç­–ç•¥
```python
# è§†é¢‘æ•°æ®å¢å¼ºæµæ°´çº¿
augmentation_pipeline = [
    RandomHorizontalFlip(p=0.5),
    RandomAffine(degrees=25, shear=20),
    ColorJitter(brightness=0.1, contrast=0.03, saturation=0.03),
    RandomGrayscale(p=0.05),
    Mosaic2x2(p=0.1)  # æ¨¡æ‹Ÿç›¸ä¼¼å¯¹è±¡æŒ‘æˆ˜
]
```

## 5. æ•°æ®å¼•æ“ä¸SA-Væ•°æ®é›†

### 5.1 ä¸‰é˜¶æ®µæ•°æ®å¼•æ“

#### é˜¶æ®µ1ï¼šé€å¸§SAMæ ‡æ³¨
- ä½¿ç”¨SAMé€å¸§æ ‡æ³¨
- è´¨é‡é«˜ä½†æ•ˆç‡ä½ï¼š37.8ç§’/å¸§
- æ”¶é›†16Kæ©ç åºåˆ—

#### é˜¶æ®µ2ï¼šSAM + SAM2æ©ç ä¼ æ’­
- é¦–å¸§ä½¿ç”¨SAMï¼Œåç»­å¸§ä½¿ç”¨SAM2ä¼ æ’­
- æ•ˆç‡æå‡5.1å€ï¼š7.4ç§’/å¸§
- æ”¶é›†63.5Kæ©ç åºåˆ—

#### é˜¶æ®µ3ï¼šå®Œæ•´SAM2äº¤äº’
- ä½¿ç”¨å®Œæ•´SAM2è¿›è¡Œäº¤äº’å¼æ ‡æ³¨
- æ•ˆç‡æå‡8.4å€ï¼š4.5ç§’/å¸§
- æ”¶é›†197Kæ©ç åºåˆ—

### 5.2 SA-Væ•°æ®é›†ç»Ÿè®¡
| æŒ‡æ ‡ | æ•°å€¼ | å¯¹æ¯” |
|------|------|------|
| è§†é¢‘æ•°é‡ | 50.9K | - |
| æ‰‹åŠ¨æ©ç åºåˆ— | 190.9K | - |
| è‡ªåŠ¨æ©ç åºåˆ— | 451.7K | - |
| æ€»æ©ç æ•°é‡ | 35.5M | æ¯”æœ€å¤§VOSæ•°æ®é›†å¤š53Ã— |
| æ¶ˆå¤±é‡ç°ç‡ | 42.5% | å…·æœ‰æŒ‘æˆ˜æ€§ |

## 6. å…³é”®æŠ€æœ¯åˆ›æ–°ç‚¹

### 6.1 æµå¼å¤„ç†æ¶æ„
```python
class StreamingProcessor:
    def process_frame(self, frame_t, prompts_t=None):
        # 1. æå–å›¾åƒç‰¹å¾
        features_t = self.image_encoder(frame_t)
        
        # 2. å†…å­˜æ³¨æ„åŠ› conditioning
        conditioned_features = self.memory_attention(
            features_t, self.memory_bank
        )
        
        # 3. æ©ç è§£ç 
        mask_t, iou_t, occlusion_t = self.mask_decoder(
            conditioned_features, prompts_t
        )
        
        # 4. æ›´æ–°å†…å­˜
        memory_t = self.memory_encoder(mask_t, features_t)
        self.memory_bank.update(memory_t)
        
        return mask_t, iou_t, occlusion_t
```

### 6.2 å†…å­˜ç®¡ç†ç­–ç•¥
- **FIFOé˜Ÿåˆ—**ï¼šç»´æŠ¤æœ€è¿‘Nå¸§è®°å¿†
- **å¯¹è±¡æŒ‡é’ˆ**ï¼šè½»é‡çº§è¯­ä¹‰è¡¨ç¤º
- **æ—¶é—´ä½ç½®ç¼–ç **ï¼šæ•æ‰çŸ­æœŸè¿åŠ¨æ¨¡å¼

### 6.3 å¤šå¯¹è±¡å¤„ç†
è™½ç„¶SAM2ç‹¬ç«‹å¤„ç†æ¯ä¸ªå¯¹è±¡ï¼Œä½†é€šè¿‡å…±äº«å›¾åƒç¼–ç ç‰¹å¾å®ç°æ•ˆç‡ä¼˜åŒ–ï¼š
```python
# å¤šå¯¹è±¡æ¨ç†ä¼ªä»£ç 
def segment_multiple_objects(video, object_prompts):
    # å…±äº«å›¾åƒç¼–ç 
    frame_features = image_encoder(video_frames)
    
    results = {}
    for obj_id, prompts in object_prompts.items():
        # ç‹¬ç«‹å†…å­˜å’Œè§£ç å™¨
        obj_memory = MemoryBank()
        obj_results = []
        
        for t in range(len(video)):
            conditioned_feat = memory_attention(
                frame_features[t], obj_memory
            )
            mask_t = mask_decoder(conditioned_feat, prompts.get(t))
            obj_results.append(mask_t)
            obj_memory.update(encode_memory(mask_t, frame_features[t]))
        
        results[obj_id] = obj_results
    
    return results
```

## 7. å®éªŒè®¾è®¡ä¸ç»“æœåˆ†æ

### 7.1 é›¶æ ·æœ¬è¯„ä¼°è®¾ç½®

#### 7.1.1 è§†é¢‘æ•°æ®é›†
è¯„ä¼°è¦†ç›–17ä¸ªé›¶æ ·æœ¬è§†é¢‘æ•°æ®é›†ï¼ŒåŒ…æ‹¬ï¼š
- **åŒ»ç–—**ï¼šEndoVis 2018
- **é•¿è§†é¢‘**ï¼šLVOSv2
- **å¼€æ”¾è¯æ±‡**ï¼šLV-VIS, UVO
- **ç‰¹æ®Šå˜æ¢**ï¼šVOST
- **é©¾é©¶**ï¼šVirtual KITTI 2

#### 7.1.2 è¯„ä¼°åè®®
**ç¦»çº¿è¯„ä¼°**ï¼š
- å¤šè½®é€šè¿‡è§†é¢‘
- æ¯è½®é€‰æ‹©æœ€å·®å¸§æ·»åŠ æç¤º
- æ¨¡æ‹Ÿç²¾ç¡®æ ‡æ³¨åœºæ™¯

**åœ¨çº¿è¯„ä¼°**ï¼š
- å•æ¬¡å‰å‘ä¼ æ’­
- é‡åˆ°ä½è´¨é‡å¸§æ—¶æš‚åœæ·»åŠ æç¤º
- æ¨¡æ‹Ÿå®æ—¶äº¤äº’åœºæ™¯

### 7.2 ä¸»è¦å®éªŒç»“æœ

#### 7.2.1 è§†é¢‘åˆ†å‰²æ€§èƒ½
| æ–¹æ³• | ç¦»çº¿è¯„ä¼° (ğ’¥&â„±) | åœ¨çº¿è¯„ä¼° (ğ’¥&â„±) | äº¤äº’æ¬¡æ•° |
|------|----------------|----------------|----------|
| SAM+XMem++ | 68.4 | 67.6 | åŸºå‡† |
| SAM+Cutie | 70.1 | 69.4 | åŸºå‡† |
| SAM2 | **75.3** | **74.4** | **å‡å°‘3Ã—** |

#### 7.2.2 å›¾åƒåˆ†å‰²æ€§èƒ½
| æ¨¡å‹ | æ•°æ® | 1-click mIoU | 5-click mIoU | FPS |
|------|------|--------------|--------------|-----|
| SAM (ViT-H) | SA-1B | 58.1 | 81.3 | 21.7 |
| SAM2 (Hiera-B+) | SA-1B | 58.9 | 81.7 | 130.1 |
| SAM2 (Hiera-B+) | å®Œæ•´æ··åˆ | **61.9** | **83.5** | **130.1** |

## 8. æ¶ˆèç ”ç©¶ä¸æŠ€æœ¯åˆ†æ

### 8.1 å†…å­˜æ¶æ„æ¶ˆè
**å…³é”®å‘ç°**ï¼š
- ä½¿ç”¨å¯¹è±¡æŒ‡é’ˆæ˜¾è‘—æå‡é•¿è§†é¢‘æ€§èƒ½ï¼ˆLVOSv2 +4.6%ï¼‰
- GRUè®°å¿†æœºåˆ¶å¸¦æ¥æœ‰é™æ”¹è¿›ä½†å¢åŠ å¤æ‚åº¦
- 6å¸§å†…å­˜å¤§å°åœ¨é€Ÿåº¦å’Œç²¾åº¦é—´è¾¾åˆ°æœ€ä½³å¹³è¡¡

### 8.2 ä½ç½®ç¼–ç ä¼˜åŒ–
é€šè¿‡ç§»é™¤ç›¸å¯¹ä½ç½®åç½®(RPB)å¹¶é‡‡ç”¨2D-RoPEï¼š
- é€Ÿåº¦æå‡ï¼šåœ¨1024åˆ†è¾¨ç‡ä¸‹æå‡4%
- ç²¾åº¦ä¿æŒï¼šåœ¨ä¸»è¦åŸºå‡†ä¸Šæ— æ€§èƒ½æŸå¤±
- æ”¯æŒFlashAttention-2åŠ é€Ÿ

### 8.3 æ•°æ®è§„æ¨¡æ•ˆåº”
è§‚å¯Ÿåˆ°æ˜æ˜¾çš„å¹‚å¾‹å…³ç³»ï¼š
math```
$$
\text{Performance} âˆ (\text{Data Size})^Î±
$$
```
å…¶ä¸­ $Î± â‰ˆ 0.3-0.4$ï¼Œè¡¨æ˜ç»§ç»­æ‰©å¤§æ•°æ®è§„æ¨¡ä»èƒ½å¸¦æ¥æ”¶ç›Šã€‚

## 9. å±€é™æ€§ä¸æœªæ¥æ–¹å‘

### 9.1 å½“å‰é™åˆ¶
1. **é•œå¤´åˆ‡æ¢å¤„ç†**ï¼šåœ¨è§†é¢‘é•œå¤´åˆ‡æ¢æ—¶å¯èƒ½ä¸¢å¤±è·Ÿè¸ª
2. **æ‹¥æŒ¤åœºæ™¯**ï¼šç›¸ä¼¼å¤–è§‚å¯¹è±¡å®¹æ˜“æ··æ·†
3. **ç²¾ç»†ç»“æ„**ï¼šå¿«é€Ÿç§»åŠ¨çš„ç»†å°ç»“æ„è·Ÿè¸ªä¸å‡†ç¡®
4. **å¤šå¯¹è±¡äº¤äº’**ï¼šç¼ºä¹å¯¹è±¡é—´çš„æ˜¾å¼å…³ç³»å»ºæ¨¡

### 9.2 æ”¹è¿›æ–¹å‘
1. **æ˜¾å¼è¿åŠ¨å»ºæ¨¡**ï¼šé›†æˆå…‰æµæˆ–è¿åŠ¨ä¼°è®¡
2. **å¯¹è±¡å…³ç³»å»ºæ¨¡**ï¼šå¼•å…¥å¯¹è±¡é—´çš„æ³¨æ„åŠ›æœºåˆ¶
3. **è‡ªåŠ¨åŒ–æ•°æ®å¼•æ“**ï¼šå‡å°‘äººå·¥æ ‡æ³¨ä¾èµ–
4. **é•¿åºåˆ—ä¼˜åŒ–**ï¼šæ”¹è¿›å†…å­˜ç®¡ç†åº”å¯¹è¶…é•¿è§†é¢‘

## 10. å®é™…åº”ç”¨ä¸å½±å“

### 10.1 åº”ç”¨åœºæ™¯
- **è§†é¢‘ç¼–è¾‘**ï¼šç²¾ç¡®çš„å¯¹è±¡è·Ÿè¸ªå’Œåˆ†å‰²
- **AR/VR**ï¼šå®æ—¶åœºæ™¯ç†è§£
- **æœºå™¨äººæŠ€æœ¯**ï¼šç¯å¢ƒæ„ŸçŸ¥å’Œæ“ä½œ
- **è‡ªåŠ¨é©¾é©¶**ï¼šåŠ¨æ€å¯¹è±¡è·Ÿè¸ª
- **åŒ»ç–—å½±åƒ**ï¼šæ‰‹æœ¯å·¥å…·å’Œå™¨å®˜åˆ†å‰²

### 10.2 å¼€æºè´¡çŒ®
- **æ¨¡å‹æƒé‡**ï¼šApache 2.0è®¸å¯
- **SA-Væ•°æ®é›†**ï¼šCC BY 4.0è®¸å¯
- **è®­ç»ƒä»£ç **ï¼šå®Œæ•´å¤ç°æ”¯æŒ
- **åœ¨çº¿Demo**ï¼šäº¤äº’å¼ä½“éªŒ

## æ€»ç»“

SAM2ä»£è¡¨äº†è§†è§‰åˆ†å‰²é¢†åŸŸçš„é‡è¦è¿›æ­¥ï¼Œé€šè¿‡ç»Ÿä¸€çš„æµå¼æ¶æ„æˆåŠŸå°†"åˆ†å‰²ä¸€åˆ‡"çš„èƒ½åŠ›ä»å›¾åƒæ‰©å±•åˆ°è§†é¢‘é¢†åŸŸã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºå†…å­˜æ³¨æ„åŠ›æœºåˆ¶å’Œå¤§è§„æ¨¡æ•°æ®å¼•æ“ï¼Œåœ¨ä¿æŒé«˜æ•ˆç‡çš„åŒæ—¶æ˜¾è‘—æå‡äº†åˆ†å‰²ç²¾åº¦ã€‚è¿™é¡¹å·¥ä½œä¸ºè§†é¢‘ç†è§£å’Œç¼–è¾‘åº”ç”¨å¥ å®šäº†åšå®åŸºç¡€ï¼Œé¢„è®¡å°†æ¨åŠ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚

# SAM 2 ç®—æ³•åŸç†è¯¦è§£

## ğŸ¯ æ ¸å¿ƒä»»åŠ¡å®šä¹‰

### Promptable Visual Segmentation (PVS)

```python
class PromptableVisualSegmentation:
    def __init__(self):
        self.supports = ['points', 'boxes', 'masks']
        self.domain = 'images_and_videos'
    
    def process_prompt(self, video_frames, prompts):
        """
        è¾“å…¥: 
        - video_frames: è§†é¢‘å¸§åºåˆ— [T, H, W, 3]
        - prompts: åœ¨ä»»æ„å¸§ä¸Šçš„æç¤º {(frame_idx, prompt_type, prompt_data)}
        
        è¾“å‡º:
        - masklet: æ•´ä¸ªè§†é¢‘ä¸­çš„æ—¶ç©ºæ©ç åºåˆ— [T, H, W]
        """
        # å®æ—¶å“åº”è¢«æç¤ºå¸§
        # ä¼ æ’­åˆ°æ•´ä¸ªè§†é¢‘
        # æ”¯æŒè¿­ä»£ç»†åŒ–
```

**ä»»åŠ¡ç‰¹æ€§**ï¼š
- **è·¨å¸§äº¤äº’**ï¼šæç¤ºå¯å‡ºç°åœ¨ä»»æ„å¸§ï¼Œä¸é™äºç¬¬ä¸€å¸§
- **å®æ—¶åé¦ˆ**ï¼šåœ¨è¢«æç¤ºå¸§ç«‹å³ç”Ÿæˆæ©ç 
- **æ—¶ç©ºä¸€è‡´æ€§**ï¼šåœ¨æ•´ä¸ªè§†é¢‘ä¸­ä¿æŒåˆ†å‰²ä¸€è‡´æ€§
- **å¤šè½®ç»†åŒ–**ï¼šæ”¯æŒé€šè¿‡é¢å¤–æç¤ºä¿®æ­£åˆ†å‰²ç»“æœ

---

## ğŸ—ï¸ æ¨¡å‹æ¶æ„è¯¦è§£

### æ•´ä½“æ¶æ„æ¦‚è§ˆ

```
SAM 2 Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Image Encoder  â”‚ -> â”‚ Memory Attention  â”‚ -> â”‚  Mask Decoder   â”‚
â”‚   (Hiera)       â”‚    â”‚   (Transformer)   â”‚    â”‚  (Two-Way)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Multi-scale    â”‚    â”‚   Memory Bank    â”‚    â”‚  Object Pointer â”‚
â”‚   Features      â”‚    â”‚ (FIFO Queues)    â”‚    â”‚   Vectors       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1. å›¾åƒç¼–ç å™¨ (Image Encoder)

```python
class HieraImageEncoder(nn.Module):
    def __init__(self, model_size='B+'):
        super().__init__()
        # åŸºäºMAEé¢„è®­ç»ƒçš„Hieraæ¶æ„
        self.backbone = HieraBackbone(model_size)
        self.fpn = FeaturePyramidNetwork()
        
    def forward(self, frame):
        # æå–å¤šå°ºåº¦ç‰¹å¾
        features = self.backbone(frame)  # [stride4, stride8, stride16, stride32]
        fused_features = self.fpn(features[stride16], features[stride32])
        return fused_features  # ç”¨äºè®°å¿†æ³¨æ„åŠ›
```

**å…³é”®æŠ€æœ¯ç‚¹**ï¼š
- **åˆ†å±‚ç‰¹å¾æå–**ï¼šstride 16å’Œ32ç‰¹å¾èåˆç”¨äºè®°å¿†æ³¨æ„åŠ›
- **å¤šå°ºåº¦è¿æ¥**ï¼šstride 4å’Œ8ç‰¹å¾ç›´æ¥è¿æ¥åˆ°æ©ç è§£ç å™¨ï¼ˆç»•è¿‡è®°å¿†ï¼‰
- **ä½ç½®ç¼–ç **ï¼šä½¿ç”¨çª—å£åŒ–ç»å¯¹ä½ç½®ç¼–ç ï¼Œç§»é™¤ç›¸å¯¹ä½ç½®åç½®(RPB)

### 2. è®°å¿†æ³¨æ„åŠ›æœºåˆ¶ (Memory Attention)

```python
class MemoryAttention(nn.Module):
    def __init__(self, L=4, dim=256):
        super().__init__()
        self.layers = nn.ModuleList([
            MemoryAttentionBlock(dim) for _ in range(L)
        ])
        
    def forward(self, current_frame_emb, memory_bank):
        """
        current_frame_emb: å½“å‰å¸§ç‰¹å¾ [B, H, W, C]
        memory_bank: è®°å¿†åº“åŒ…å«:
            - spatial_memories: æœ€è¿‘Nå¸§ç©ºé—´ç‰¹å¾
            - prompted_memories: æç¤ºå¸§è®°å¿†  
            - object_pointers: ç‰©ä½“æŒ‡é’ˆå‘é‡
        """
        x = current_frame_emb
        for layer in self.layers:
            # è‡ªæ³¨æ„åŠ› + è®°å¿†äº¤å‰æ³¨æ„åŠ› + MLP
            x = layer(x, memory_bank)
        return x
```

**è®°å¿†åº“è®¾è®¡**ï¼š
```python
class MemoryBank:
    def __init__(self, N=6, M=10):
        self.recent_frames = deque(maxlen=N)    # æœ€è¿‘å¸§è®°å¿†(FIFO)
        self.prompted_frames = deque(maxlen=M)  # æç¤ºå¸§è®°å¿†(FIFO)
        self.object_pointers = []               # ç‰©ä½“è¯­ä¹‰å‘é‡
        
    def add_memory(self, frame_emb, mask_pred, is_prompted=False):
        memory = self.memory_encoder(frame_emb, mask_pred)
        if is_prompted:
            self.prompted_frames.append(memory)
        else:
            self.recent_frames.append(memory)
        
        # æ›´æ–°ç‰©ä½“æŒ‡é’ˆ
        obj_ptr = self.extract_object_pointer(mask_pred)
        self.object_pointers.append(obj_ptr)
```

### 3. æ©ç è§£ç å™¨æ”¹è¿› (Enhanced Mask Decoder)

```python
class SAM2MaskDecoder(nn.Module):
    def __init__(self):
        super().__init__()
        # ç»§æ‰¿SAMçš„åŒå‘Transformerè®¾è®¡
        self.two_way_transformer = TwoWayTransformer()
        
        # æ–°å¢ç»„ä»¶
        self.occlusion_head = nn.Linear(dim, 1)  # é®æŒ¡é¢„æµ‹
        self.high_res_skip = HighResolutionSkip() # é«˜åˆ†è¾¨ç‡è·³è·ƒè¿æ¥
        
    def forward(self, conditioned_emb, prompts, image_features):
        # å¤„ç†æç¤º
        prompt_emb = self.prompt_encoder(prompts)
        
        # åŒå‘æ³¨æ„åŠ›æ›´æ–°
        mask_tokens, image_emb = self.two_way_transformer(
            prompt_emb, conditioned_emb
        )
        
        # å¤šæ©ç é¢„æµ‹å¤„ç†æ­§ä¹‰
        multi_masks = self.predict_multiple_masks(mask_tokens, image_emb)
        
        # é®æŒ¡é¢„æµ‹
        occlusion_score = self.occlusion_head(mask_tokens)
        
        # é«˜åˆ†è¾¨ç‡ä¸Šé‡‡æ ·ï¼ˆä½¿ç”¨stride4/8ç‰¹å¾ï¼‰
        upsampled_masks = self.high_res_skip(multi_masks, image_features)
        
        return upsampled_masks, occlusion_score, mask_tokens
```

**å…³é”®æ”¹è¿›**ï¼š
- **é®æŒ¡é¢„æµ‹**ï¼šè¯†åˆ«ç›®æ ‡ç‰©ä½“æ˜¯å¦åœ¨å½“å‰å¸§å¯è§
- **å¤šæ©ç è¾“å‡º**ï¼šå¤„ç†è·¨å¸§æ­§ä¹‰ï¼ˆå¦‚éƒ¨ä»¶vsæ•´ä½“ï¼‰
- **é«˜åˆ†è¾¨ç‡è·³è·ƒ**ï¼šç›´æ¥ä»å›¾åƒç¼–ç å™¨å¼•å…¥ç»†èŠ‚ç‰¹å¾

---

## ğŸ”„ è®­ç»ƒç­–ç•¥è¯¦è§£

### 1. é¢„è®­ç»ƒé˜¶æ®µ (SA-1B)

```python
def pre_training_pipeline():
    # åˆå§‹åŒ–: MAEé¢„è®­ç»ƒçš„Hiera
    model = SAM2(image_encoder='hiera_mae_pretrained')
    
    # æ•°æ®: SA-1Bæ•°æ®é›†
    dataset = SA1BDataset()
    
    # è®­ç»ƒé…ç½®:
    optimizer = AdamW(lr=4e-4, weight_decay=0.1)
    scheduler = ReciprocalSqrtSchedule(timescale=1000)
    
    # æŸå¤±å‡½æ•°:
    losses = {
        'mask': LinearCombination([FocalLoss(20), DiceLoss(1)]),
        'iou': L1Loss(1),  # æ›´æ¿€è¿›çš„IoUç›‘ç£
        'occlusion': CrossEntropyLoss(1)
    }
```

### 2. è”åˆè®­ç»ƒé˜¶æ®µ

```python
def joint_training():
    # æ•°æ®æ··åˆç­–ç•¥
    data_mix = {
        'SA-1B': 15.2%,      # å›¾åƒæ•°æ®
        'SA-V': 70.0%,       # è§†é¢‘æ•°æ®
        'Internal': 14.8%    # å†…éƒ¨è§†é¢‘æ•°æ®
    }
    
    # äº¤æ›¿è®­ç»ƒç­–ç•¥
    for iteration in total_iterations:
        if random() < image_prob:
            batch = sample_image_batch()  # å•å¸§è®­ç»ƒ
            loss = image_task_loss(batch)
        else:
            batch = sample_video_batch()  # 8å¸§åºåˆ—
            loss = video_task_loss(batch)
```

### 3. è§†é¢‘è®­ç»ƒæ¨¡æ‹Ÿ

```python
def simulate_interactive_training(video_sequence, gt_masklet):
    """æ¨¡æ‹Ÿäº¤äº’å¼è®­ç»ƒè¿‡ç¨‹"""
    
    # é‡‡æ ·8å¸§åºåˆ—
    frames = sample_8_frames(video_sequence)
    
    # éšæœºé€‰æ‹©æœ€å¤š2ä¸ªæç¤ºå¸§
    prompt_frames = random.sample(range(8), k=min(2, 8))
    
    predictions = []
    memory_bank = MemoryBank()
    
    for t in range(8):
        # å½“å‰å¸§å¤„ç†
        current_frame = frames[t]
        
        if t in prompt_frames:
            # æ¨¡æ‹Ÿç”¨æˆ·æç¤º (50%æ©ç , 25%ç‚¹å‡», 25%æ¡†)
            prompt_type = sample_prompt_type()
            prompt = generate_prompt(gt_masklet[t], prompt_type)
        else:
            prompt = None
            
        # æ¨¡å‹é¢„æµ‹ï¼ˆä½¿ç”¨è®°å¿†ï¼‰
        mask_pred, occlusion = model(
            current_frame, prompt, memory_bank
        )
        
        # æ›´æ–°è®°å¿†åº“
        memory_bank.add_memory(
            model.image_encoder(current_frame), 
            mask_pred, 
            is_prompted=(t in prompt_frames)
        )
        
        predictions.append(mask_pred)
    
    return compute_loss(predictions, gt_masklet)
```

**æ•°æ®å¢å¼ºç­–ç•¥**ï¼š
- **Mosaicå¢å¼º**ï¼š2Ã—2æ‹¼æ¥ç›¸åŒè§†é¢‘ï¼Œæ¨¡æ‹Ÿç›¸ä¼¼ç‰©ä½“åœºæ™¯
- **æ—¶åºåè½¬**ï¼š50%æ¦‚ç‡åå‘å¤„ç†åºåˆ—
- **é¢œè‰²æŠ–åŠ¨**ï¼šæ¯å¸§ç‹¬ç«‹é¢œè‰²å˜æ¢
- **ä»¿å°„å˜æ¢**ï¼šæ—‹è½¬ã€å‰ªåˆ‡ç­‰ç©ºé—´å˜æ¢

---

## ğŸª æ¨ç†æµç¨‹è¯¦è§£

### æµå¼å¤„ç†ç®—æ³•

```python
class StreamingInference:
    def __init__(self, model):
        self.model = model
        self.memory_bank = MemoryBank()
        self.current_object_pointers = []
        
    def process_frame(self, frame, prompts=None):
        """å¤„ç†å•å¸§"""
        
        # 1. æå–å›¾åƒç‰¹å¾ï¼ˆåªè¿è¡Œä¸€æ¬¡ï¼‰
        frame_emb = self.model.image_encoder(frame)
        
        # 2. è®°å¿†æ³¨æ„åŠ›æ¡ä»¶åŒ–
        conditioned_emb = self.model.memory_attention(
            frame_emb, self.memory_bank
        )
        
        # 3. æ©ç è§£ç ï¼ˆå¯é€‰æç¤ºï¼‰
        masks, occlusion, obj_ptr = self.model.mask_decoder(
            conditioned_emb, prompts, frame_emb
        )
        
        # 4. å¤„ç†å¤šæ©ç æ­§ä¹‰
        if prompts is None and len(masks) > 1:
            # æ— æ–°æç¤ºæ—¶é€‰æ‹©æœ€é«˜IoUæ©ç 
            selected_mask = select_mask_by_iou(masks)
        else:
            selected_mask = masks[0]  # æç¤ºå·²è§£å†³æ­§ä¹‰
            
        # 5. æ›´æ–°è®°å¿†
        self.memory_bank.add_memory(
            frame_emb, selected_mask, 
            is_prompted=(prompts is not None)
        )
        
        return selected_mask, occlusion
```

### å¤šç‰©ä½“å¤„ç†

```python
def process_multiple_objects(video_frames, object_prompts):
    """å¤„ç†è§†é¢‘ä¸­çš„å¤šä¸ªç‰©ä½“"""
    
    # å…±äº«å›¾åƒç¼–ç ï¼ˆè®¡ç®—æ•ˆç‡ï¼‰
    frame_embeddings = [
        model.image_encoder(frame) for frame in video_frames
    ]
    
    results = {}
    for obj_id, prompts in object_prompts.items():
        # æ¯ä¸ªç‰©ä½“ç‹¬ç«‹çš„å†…å­˜åº“å’Œå¤„ç†å™¨
        obj_processor = StreamingInference(model)
        obj_masks = []
        
        for t, frame_emb in enumerate(frame_embeddings):
            frame_prompts = prompts.get(t, None)
            mask, _ = obj_processor.process_frame(
                frame_emb, frame_prompts
            )
            obj_masks.append(mask)
            
        results[obj_id] = obj_masks
    
    return results
```

---

## âš¡ å…³é”®æŠ€æœ¯åˆ›æ–°

### 1. è®°å¿†æœºåˆ¶è®¾è®¡

**ç©ºé—´è®°å¿†**ï¼š
- å­˜å‚¨æœ€è¿‘Nå¸§çš„ç‰¹å¾å›¾
- ä½¿ç”¨2D-RoPEä½ç½®ç¼–ç æ•æ‰çŸ­æœŸè¿åŠ¨
- é€šé“ç»´åº¦å‹ç¼©ï¼ˆ64ç»´ï¼‰å‡å°‘å†…å­˜å ç”¨

**ç‰©ä½“æŒ‡é’ˆ**ï¼š
- ä»æ©ç è§£ç å™¨è¾“å‡ºtokenæå–
- ç¼–ç é«˜çº§è¯­ä¹‰ä¿¡æ¯
- å¢å¼ºé•¿æ—¶åºä¸€è‡´æ€§

### 2. é«˜æ•ˆæ³¨æ„åŠ›ä¼˜åŒ–

```python
# ä½¿ç”¨FlashAttention-2åŠ é€Ÿ
def optimized_attention():
    # ç§»é™¤RPBï¼Œå¯ç”¨FlashAttention
    with torch.backends.cuda.sdp_kernel():
        attn_output = F.scaled_dot_product_attention(
            q, k, v, attn_mask=None
        )
```

### 3. æ­§ä¹‰å¤„ç†ç­–ç•¥

```python
def handle_temporal_ambiguity(masks, prev_masks, prompts):
    """å¤„ç†è·¨å¸§åˆ†å‰²æ­§ä¹‰"""
    
    if prompts:  # æœ‰æ–°æç¤ºï¼Œè§£å†³æ­§ä¹‰
        return resolve_ambiguity_with_prompts(masks, prompts)
    elif prev_masks:  # ä¼ æ’­å…ˆå‰é€‰æ‹©
        return propagate_previous_selection(masks, prev_masks)
    else:  # é€‰æ‹©å½“å‰æœ€ä½³
        return select_by_iou(masks)
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–åˆ†æ

### é€Ÿåº¦ä¼˜åŠ¿æ¥æº

1. **é«˜æ•ˆå›¾åƒç¼–ç å™¨**ï¼šHieraæ¯”ViTæ›´å¿«ä¸”æ€§èƒ½ç›¸å½“
2. **è®°å¿†é€šé“å‹ç¼©**ï¼š64ç»´vs 256ç»´ï¼Œ4å€å†…å­˜èŠ‚çœ
3. **æ³¨æ„åŠ›ä¼˜åŒ–**ï¼šç§»é™¤RPBï¼Œå¯ç”¨FlashAttention-2
4. **æµå¼å¤„ç†**ï¼šé¿å…é‡å¤ç¼–ç ï¼Œæ”¯æŒå®æ—¶åº”ç”¨

### ç²¾åº¦æå‡å› ç´ 

1. **æ—¶åºä¸€è‡´æ€§**ï¼šè®°å¿†æœºåˆ¶ä¿æŒè·¨å¸§ç¨³å®šæ€§
2. **é«˜åˆ†è¾¨ç‡ç»†èŠ‚**ï¼šè·³è·ƒè¿æ¥ä¿ç•™ç©ºé—´ç»†èŠ‚
3. **å¤šæ ·åŒ–è®­ç»ƒ**ï¼šSA-Væ•°æ®é›†è¦†ç›–å„ç§æŒ‘æˆ˜åœºæ™¯
4. **äº¤äº’å¼ä¼˜åŒ–**ï¼šæ¨¡æ‹Ÿè®­ç»ƒåŒ¹é…çœŸå®ä½¿ç”¨æ¨¡å¼

---

## ğŸ”® æ€»ç»“

SAM 2é€šè¿‡**è®°å¿†å¢å¼ºçš„æµå¼æ¶æ„**ã€**ç»Ÿä¸€çš„ä»»åŠ¡å®šä¹‰**å’Œ**å¤§è§„æ¨¡å¤šæ ·åŒ–æ•°æ®é›†**ï¼ŒæˆåŠŸå°†SAMçš„èƒ½åŠ›æ‰©å±•åˆ°è§†é¢‘é¢†åŸŸã€‚å…¶æ ¸å¿ƒç®—æ³•åˆ›æ–°åŒ…æ‹¬ï¼š

- **è®°å¿†æ³¨æ„åŠ›æœºåˆ¶**å®ç°æ—¶åºå»ºæ¨¡
- **ç‰©ä½“æŒ‡é’ˆå‘é‡**å¢å¼ºè¯­ä¹‰ä¸€è‡´æ€§  
- **å¤šæ©ç æ­§ä¹‰å¤„ç†**åº”å¯¹å¤æ‚åœºæ™¯
- **é«˜æ•ˆæµå¼æ¨ç†**æ”¯æŒå®æ—¶åº”ç”¨

è¿™å¥—ç®—æ³•æ¡†æ¶ä¸ºè§†é¢‘åˆ†å‰²å»ºç«‹äº†æ–°çš„æŠ€æœ¯æ ‡å‡†ï¼Œå¹¶ä¸ºæœªæ¥çš„è§†é¢‘ç†è§£ç ”ç©¶æä¾›äº†é‡è¦åŸºç¡€ã€‚
