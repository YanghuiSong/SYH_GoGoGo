# SYH_GoGoGo
The following is the code for reproducing recently read papers and the work currently in progress.
# Content
1. [[Pilot Study](#PilotStudy)]
2. [[Remote Sensing](#Remote_Sensing)]
3. [[Classification](#classification)]
4. [[Segmentation](#segmentation)]
       
-----------------------------------------------------------------------------------------------
<a name="PilotStudy"></a>  
## Pilot Study
1. [2024 ICLR] **FeatUp: A Model-Agnostic Framework for Features at Any Resolution** [[paper]](https://openreview.net/pdf?id=GkJiNn2QDF) [[code]](https://github.com/mhamilton723/FeatUp)[[Notes](#FeatUpLearning)]   
2. [2025 CVPR] **SegEarth-OV: Towards Training-Free Open-Vocabulary Segmentation for Remote Sensing Images** [[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images_CVPR_2025_paper.pdf) [[code]](https://github.com/likyoo/SegEarth-OV)[[Notes](#SegEarth)] 
3. [2025 NIPS] **InstructSAM: A Training-Free Framework for Instruction-Oriented Remote Sensing Object Recognition** [[paper]](https://arxiv.org/pdf/2505.15818) [[code]](https://github.com/VoyagerXvoyagerx/InstructSAM?tab=readme-ov-file)
4. [2025 arXiv] **AnyUp: Universal Feature Upsampling** [[paper]](https://arxiv.org/abs/2510.12764) [[code]](https://github.com/wimmerth/anyup)[[Notes](#AnyUp)] 
5. [2025 ICME] **LG-CD: Enhancing Language-Guided Change Detection through SAM2 Adaptation** [[paper]](https://arxiv.org/pdf/2509.21894)

<a name="Remote_Sensing"></a>  
## Remote Sensing
1. [2025 arXiv] **DynamicEarth: How Far are We from Open-Vocabulary Change Detection?** [[paper]](https://arXiv.org/abs/2501.12931) [[code]](https://github.com/likyoo/DynamicEarth)
2. [2025 TGRS] **A Unified Framework With Multimodal Fine-Tuning for Remote Sensing Semantic Segmentation.** [[paper]](https://ieeexplore.ieee.org/document/11063320) [[code]](https://github.com/sstary/SSRS)
3. [2025 ICASSP] **Enhancing Remote Sensing Vision-Language Models for Zero-Shot Scene Classification.** [[paper]](https://arXiv.org/abs/2409.00698) [[code]](https://github.com/elkhouryk/RS-TransCLIP)
4. [2025 ICCV] **https://github.com/mburges-cvl/ICCV_AL4FM.** [[paper]](https://openaccess.thecvf.com/content/ICCV2025/papers/Burges_Active_Learning_Meets_Foundation_Models_Fast_Remote_Sensing_Data_Annotation_ICCV_2025_paper.pdf) [[code]](https://github.com/mburges-cvl/ICCV_AL4FM)
5. [2025 ICCV] **Dynamic Dictionary Learning for Remote Sensing Image Segmentation.** [[paper]](https://arXiv.org/pdf/2503.06683) [[code]](https://github.com/XavierJiezou/D2LS)
6. [2025 ICCV] **GEOBench-VLM: Benchmarking Vision-Language Models for Geospatial Tasks.** [[paper]](https://arxiv.org/pdf/2411.19325) [[code]](https://github.com/The-AI-Alliance/GEO-Bench-VLM)
7. [2025 ICCV] **SCORE: Scene Context Matters in Open-Vocabulary Remote Sensing Instance Segmentation.** [[paper]](https://arXiv.org/abs/2507.12857) [[code]](https://github.com/HuangShiqi128/SCORE)
8. [2025 ICCV] **When Large Vision-Language Model Meets Large Remote Sensing Imagery: Coarse-to-Fine Text-Guided Token Pruning.** [[paper]](https://arXiv.org/pdf/2503.07588) [[code]](https://github.com/VisionXLab/LRS-VQA)
9. [2025 ICCV] **SMARTIES: Spectrum-Aware Multi-Sensor Auto-Encoder for Remote Sensing Images.** [[paper]](https://openaccess.thecvf.com/content/ICCV2025/papers/Sumbul_SMARTIES_Spectrum-Aware_Multi-Sensor_Auto-Encoder_for_Remote_Sensing_Images_ICCV_2025_paper.pdf) [[code]](https://github.com/gsumbul/SMARTIES)
10. [2025 ICCV] **Continuous Remote Sensing Image Super-Resolution via Neural Operator Diffusion** [[paper]](https://openaccess.thecvf.com/content/ICCV2025/papers/Xu_NeurOp-Diff_Continuous_Remote_Sensing_Image_Super-Resolution_via_Neural_Operator_Diffusion_ICCV_2025_paper.pdf) [[code]](https://github.com/zerono000/NeurOp-Diff)
11. [2025 ICCV] **HoliTracer: Holistic Vectorization of Geographic Objects from Large-Size Remote Sensing Imagery.** [[paper]](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_HoliTracer_Holistic_Vectorization_of_Geographic_Objects_from_Large-Size_Remote_Sensing_ICCV_2025_paper.pdf) [[code]](https://github.com/vvangfaye/HoliTracer)
12. [2025 arXiv] **SAR-KnowLIP: Towards Multimodal Foundation Models for Remote Sensing.** [[paper]](https://arxiv.org/pdf/2509.23927) [[code]](https://github.com/yangyifremad/SARKnowLIP)
13. [2025 AAAI] **ZoRI: Towards discriminative zero-shot remote sensing instance segmentation.** [[paper]](https://arXiv.org/abs/2412.12798) [[code]](https://github.com/HuangShiqi128/ZoRI)
14. [2024 NIPS] **Segment Any Change.** [[paper]](https://proceedings.NIPS.cc/paper_files/paper/2024/file/9415416201aa201902d1743c7e65787b-Paper-Conference.pdf) [[code]](https://github.com/Z-Zheng/pytorch-change-models)
15. [2025 CVPR] **SegEarth-OV: Towards Training-Free Open-Vocabulary Segmentation for Remote Sensing Images.** [[paper]](https://arXiv.org/abs/2410.01768) [[code]](https://github.com/likyoo/SegEarth-OV)
16. [2025 CVPR] **XLRS-Bench: Could Your Multimodal LLMs Understand Extremely Large Ultra-High-Resolution Remote Sensing Imagery?** [[paper]](https://arXiv.org/abs/2503.23771) [[code]](https://github.com/EvolvingLMMs-Lab/XLRS-Bench)
17. [2025 CVPR] **Exact: Exploring Space-Time Perceptive Clues for Weakly Supervised Satellite Image Time Series Semantic Segmentation.** [[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image_CVPR_2025_paper.pdf) [[code]](https://github.com/MiSsU-HH/Exact)
18. [2025 Arxiv] **SegEarth-OV-2: Annotation-Free Open-Vocabulary Segmentation for Remote-Sensing Images** [[paper]](https://arxiv.org/abs/2508.18067)  [[code]](https://github.com/earth-insights/SegEarth-OV-2)
19. [2025 AAAI] **Towards Open-Vocabulary Remote Sensing Image Semantic Segmentation** [[paper]](https://arxiv.org/abs/2412.19492) [[code]](https://github.com/yecy749/GSNet)
20. [2025 Arxiv] **InstructSAM: A Training-Free Framework for Instruction-Oriented Remote Sensing Object Recognition** [[paper]](https://arxiv.org/pdf/2505.15818) [[code]](https://github.com/VoyagerXvoyagerx/InstructSAM)
21. [2025 Arxiv] **DescribeEarth: Describe Anything for Remote Sensing Images** [[paper]](https://arxiv.org/pdf/2509.25654v1) [[code]](https://github.com/earth-insights/DescribeEarth)
22. [2025 NIPS] **GTPBD: A Fine-Grained Global Terraced Parcel and Boundary Dataset** [[paper]](https://arxiv.org/abs/2507.14697) [[code]](https://github.com/Z-ZW-WXQ/GTPBD)
23. [2025 Arxiv] **RS3DBench: A Comprehensive Benchmark for 3D Spatial Perception in Remote Sensing** [[paper]](https://arxiv.org/abs/2509.18897) [[code]](https://rs3dbench.github.io)
24. [2025 Arxiv] **DGL-RSIS: Decoupling Global Spatial Context and Local Class Semantics for Training-Free Remote Sensing Image Segmentation** [[paper]](https://arxiv.org/pdf/2509.00598) [[code]](https://github.com/designer1024/DGL-RSIS)
25. [2025 TGRS] **A Unified SAM-Guided Self-Prompt Learning Framework for Infrared Small Target Detection** [[paper]](https://ieeexplore.ieee.org/document/11172325) [[code]](https://github.com/fuyimin96/SAM-SPL)
26. [2025 TGRS] **Semantic Prototyping With CLIP for Few-Shot Object Detection in Remote Sensing Images** [[paper]](https://ieeexplore.ieee.org/document/10930588)
27. [2025 Arxiv] **ATRNet-STAR: A Large Dataset and Benchmark Towards Remote Sensing Object Recognition in the Wild** [[paper]](https://arxiv.org/abs/2501.13354) [[code]](https://github.com/waterdisappear/ATRNet-STAR)
28. [2025 Arxiv] **RSKT-Seg: Exploring Efficient Open-Vocabulary Segmentation in the Remote Sensing** [[paper]](https://arxiv.org/pdf/2509.12040) [[code]](https://github.com/LiBingyu01/RSKT-Seg)
29. [2025 ISPRS]  **AdaptVFMs-RSCD: Advancing Remote Sensing Change Detection from binary to semantic with SAM and CLIP** [[paper]](https://doi.org/10.1016/j.isprsjprs.2025.09.010) [[data]](https://github.com/Jiang-CHD-YunNan/RS-VFMs-Fine-tuning-Dataset)
30. [2025 Arxiv]**PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient Fine-Tuning for Remote Sensing Change Detection** [[paper]](https://arxiv.org/pdf/2509.09572) [[code]](https://github.com/dyzy41/PeftCD)
31. [2025 Arxiv] **AlignCLIP: Self-Guided Alignment for Remote Sensing Open-Vocabulary Semantic Segmentation** [[paper]](https://openreview.net/forum?id=hpD3tn7Xbp) [[code]](https://openreview.net/attachment?id=hpD3tn7Xbp&name=supplementary_material)
32. [2025 Arxiv] **Few-Shot Adaptation Benchmark for Remote Sensing Vision-Language Models** [[paper]](https://arxiv.org/pdf/2510.07135) [[code]](https://github.com/elkhouryk/fewshot_RSVLMs)
33. [2025 RSE] **Strategic sampling for training a semantic segmentation model in operational mapping: Case studies on cropland parcel extraction** [[paper]](https://doi.org/10.1016/j.rse.2025.115034) [[data]](https://doi.org/10.5281/zenodo.16595511) [[code]](https://github.com/Remote-Sensing-of-Land-Resource-Lab/Training-Sample-Selection)
34. [2025 TIP] **Universal Fine-Grained Visual Categorization by Concept Guided Learning** [[paper]](https://ieeexplore.ieee.org/document/10829548) [[data]](https://drive.google.com/file/d/11hYbdO32hyspucDKp5wwjwvCaD38AEKe/view?usp=sharing) [[code]](https://github.com/BiQiWHU/CGL)
35. [2025 TIP] **SARATR-X: Towards Building A Foundation Model for SAR Target Recognition** [[paper]](https://ieeexplore.ieee.org/document/10856784) [[code]](https://github.com/waterdisappear/SARATR-X)
36. [2025 TIP] **HSLabeling: Towards Efficient Labeling for Large-scale Remote Sensing Image Segmentation with Hybrid Sparse Labeling** [[paper]](https://ieeexplore.ieee.org/document/10829548) [[data]](https://drive.google.com/drive/folders/1CiYzJyBn1rV-xsrsYQ6o2HDQjdfnadHl) [[code]](https://github.com/linjiaxing99/HSLabeling)
37. [2025 CVM] **Remote sensing tuning: A survey** [[paper]](https://ieeexplore.ieee.org/document/11119145) [[code]](https://github.com/DongshuoYin/Remote-Sensing-Tuning-A-Survey/tree/main)
38. [2025 ISPRS]**Domain generalization for semantic segmentation of remote sensing images via vision foundation model fine-tuning**[[paper]](https://www.sciencedirect.com/science/article/pii/S0924271625003569)[[code]](https://github.com/mmmll23/GeoSA-BaSA)
39. [2025 ISPRS]**Meta Feature Disentanglement under continuous-valued domain modeling for generalizable remote sensing image segmentation on unseen domains**[[paper]](https://www.sciencedirect.com/science/article/pii/S0924271625003879)[[code]](https://github.com/LCB1970/MetaFD)


<a name="classification"></a>  
## Classification
...
<a name="segmentation"></a>  
## Segmentation
...

<a name="FeatUpLearning"></a>  
## FeatUp （Understand 40%→60%）
**FeatUp的核心思想与方法**
![FeatUp](https://raw.githubusercontent.com/YanghuiSong/SYH_GoGoGo/main/FeatUp.png)
FeatUp的核心灵感来自NeRF的多视图一致性原理：通过观察同一图像经微小变换（如裁剪、翻转、缩放）后的多个低分辨率特征视图，学习高分辨率特征的空间一致性。

本文介绍了一个名为 FEATUP 的框架，它可以帮助深度学习模型恢复低分辨率特征的空间信息，从而提高模型在一些需要高分辨率特征的任务（如语义分割和深度估计）上的性能表现。这个框架有两个版本，一个是通过一次前向传播来引导特征，另一个则是通过拟合一个隐含模型来重建任意分辨率的特征。这两个方法都使用了多视一致性损失，并且可以直接替换到现有的应用程序中，而不需要重新训练模型。

在遥感图像中，由于模型的池化操作导致特征图分辨率降低，从而影响了下游任务的表现。而通过使用 FEATUP 技术，可以对深度特征进行上采样，增加其空间分辨率，使得它们能够更好地应用于密集预测任务，如语义分割和深度估计等。因此，FEATUP 技术可以在遥感图像处理中提高特征的空间分辨率，并改善下游任务的性能表现。

具体包括以下关键设计：

1. 多视图一致性损失（核心监督信号）
   
对输入图像施加随机微小变换（如填充、缩放、水平翻转），得到多个“抖动”版本的低分辨率特征。FeatUp学习一个高分辨率特征图，使其经下采样后能匹配所有抖动视图的低分辨率特征，通过高斯似然损失（含自适应不确定性）监督这一过程，确保高分辨率特征的空间一致性。

多视一致性损失是一种用于训练深度学习模型的技术，在多个视角下观察同一场景并计算它们之间的差异，以此来增加模型对不同视角下的数据的鲁棒性和泛化能力。在本文中，多视一致性损失被用来训练 FeatUp 框架中的两个版本，即引导特征和重建特征的方法。具体来说，多视一致性损失是通过比较模型输出的低分辨率特征和经过多次变换后的高分辨率特征来计算的，这些变换包括图像的平移、缩放和翻转等操作。通过最小化这种差异，模型可以更好地适应不同的输入数据，并生成更准确的结果。

与对比学习相比，多视一致性损失的主要区别在于它不是通过直接比较不同样本之间的相似性来进行训练的，而是通过对同一场景的不同视角进行比较来提高模型的鲁棒性和泛化能力。相比之下，对比学习更加注重对不同类别之间的区分度的训练，通常需要大量的负样本来表示不同的类别。因此，虽然两种技术都是基于比较不同样本之间的相似性的思想，但其应用场景和目的略有不同。

2. 两种下采样器（模拟模型池化行为）
   
为匹配不同模型的特征降采样机制，设计两种下采样器：

简单下采样器：学习非负归一化模糊核，通过卷积实现特征平滑下采样，适用于固定感受野模型（如CNN）。

注意力下采样器：通过1×1卷积预测显著性图，动态调整下采样核权重，适应动态感受野或对象显著性（如ViT的 patch 注意力机制）。

在FeatuP中，有两种下采样器被设计出来来匹配不同模型的特征降采样机制。第一种是简单下采样器，它通过学习非负归一化模糊核并利用卷积操作对特征进行平滑下采样。这种方法适用于固定感受野模型，比如CNN等。第二种是注意力下采样器，它通过1×1卷积预测显著性图，动态地调整下采样核权重，以适应动态感受野或对象显著性。这种方法适用于具有动态感受野或对象显著性的模型，例如ViT中的patch注意力机制。这两种下采样器都能够有效地降低特征的空间分辨率，从而提高模型的计算效率，并且不会影响特征的语义信息。

传统的池化方法会将输入图像的空间分辨率降低，从而减少计算量并提高模型的鲁棒性。而FeatuUp则通过增加空间分辨率来恢复深度特征中的丢失信息，从而提高了下游任务的性能。因此，与传统的池化方法相比，FeatuUp能够更好地保留原始图像的语义信息，并且可以在不重新训练的情况下直接应用于现有的应用程序中。

3. 两种上采样器（核心创新）
   
FeatUp提供两种即插即用的上采样变体，可直接替换现有特征：

JBU FeatUp（通用前向传播上采样）
基于联合双边滤波（JBU） 的改进，通过堆叠参数化JBU层，利用输入图像的高分辨率信号引导特征上采样。关键优化：

设计高效CUDA内核，比标准PyTorch实现快10倍、内存占用低2个数量级；

用MLP替代传统JBU的固定高斯核，学习特征与图像高频细节的关联，保留语义的同时恢复边缘信息。

Implicit FeatUp（单图像隐式上采样）

过拟合一个小型隐式网络到单图像特征，通过傅里叶特征编码（含颜色信息）实现任意分辨率的特征重建。优势：

参数仅为显式特征存储的1/100，支持超高分辨率输出；

结合总变差正则化避免噪声，适合需要精细细节的场景。

第一种是基于联合双边滤波（JBU）的改进，称为JBU FeatUp。它通过堆叠参数化的JBU层，利用输入图像的高分辨率信号来引导特征上采样。具体来说，它采用了一个高效的CUDA内核，比标准PyTorch实现快10倍，内存占用也低了两个数量级。此外，它还用多层感知机（MLP）代替了传统JBU的固定高斯核，以便学习特征与图像高频细节之间的关系，同时保留语义信息并恢复边缘信息。JBU的原理是利用高分辨率信号作为指导，通过一种叫做双边滤波的技术来对低分辨率图像进行平滑处理，并在保持图像边缘清晰的情况下提高其分辨率。具体来说，它会先定义一个高斯核函数来计算每个像素点周围的邻域内像素值的加权平均值，然后根据这个平均值来更新当前像素点的值。这样就可以实现对图像的平滑处理和分辨率提升。同时，由于双边滤波还考虑了像素之间的距离和颜色差异等因素，因此可以有效地保留图像中的细节和纹理信息。


第二种是Implicit FeatUp，它是通过过拟合一个小型隐式网络到单张图像的特征来进行任意分辨率的特征重建。这种方法的优势在于，它的参数仅为显式特征存储的1/100，可以支持超高的分辨率输出。此外，它还结合了总变差正则化，以避免噪声，并适用于需要精细细节的场景。

mplicit FeatUp是一种通过训练一个小型隐式网络来实现任意分辨率特征重建的方法。它的基本思想是在给定一张输入图像及其对应的低分辨率特征图时，训练一个小型神经网络来预测该图像的高分辨率特征图。这个神经网络通常由多个卷积层和反卷积层组成，其中卷积层用于提取图像的局部特征，而反卷积层则用于将这些局部特征组合成高分辨率的全局特征。在训练过程中，神经网络的目标是最小化预测的高分辨率特征图与真实高分辨率特征图之间的差异。最终得到的隐式网络可以用来对任意图像的特征进行重建，从而实现任意分辨率的特征映射。
特征映射是指将原始数据（如图像）中的每个像素点表示为一个向量，并将这些向量组织成一个矩阵的过程。在这个矩阵中，每一行代表了一个像素点的向量表示，每一列则代表了不同的特征。通过这种方式，我们可以将图像转换为一个更易于处理的形式，以便于后续的计算机视觉任务，例如分类、检测、分割等。

关键贡献

模型无关框架：适用于任意视觉 backbone（CNN、ViT、自监督模型如DINO等），无需修改原模型结构。

高效JBU实现：提出首个高效CUDA版联合双边滤波，解决传统JBU计算瓶颈，支持大规模模型部署。

即插即用提升：上采样特征可直接替换现有特征，在不重新训练下游模型的情况下提升性能（如分割mIoU、深度估计精度）。



<a name="AnyUp"></a>  
## AnyUp （Understand 60%）
**AnyUp的核心思想与方法**

[**基于已经开源的AnyUp初体验**](https://github.com/wimmerth/anyup)

**原图**
![image](https://raw.githubusercontent.com/YanghuiSong/SYH_GoGoGo/main/image1.png)

[**DINOV2**](https://huggingface.co/facebook/dinov2-base)
![dinov2](https://raw.githubusercontent.com/YanghuiSong/SYH_GoGoGo/main/dinov2.png)

[**DINO**](https://huggingface.co/facebook/dino-vitb16)
![dino](https://raw.githubusercontent.com/YanghuiSong/SYH_GoGoGo/main/dino.png)

[**CLIP**](https://huggingface.co/openai/clip-vit-base-patch32)
![clip](https://raw.githubusercontent.com/YanghuiSong/SYH_GoGoGo/main/clip.png)
这篇论文介绍了一种名为AnyUp的方法，用于对任何视觉特征在任意分辨率下进行上采样。现有的学习型上采样器需要针对每个特征提取器重新训练，而AnyUp则不需要，因此可以在不同特征类型之间通用，并且可以应用于各种下游任务。该方法采用一种推理时的特征非特定性上采样架构来提高上采样的质量，并在实验中达到了新的最佳水平。

与FeatUp相比，AnyUp的优点是可以应用于多种不同的特征类型，并且可以用于各种下游任务，具有较高的灵活性和可扩展性。然而，AnyUp需要手动调整超参数以获得最佳性能，这可能比较困难。另外，由于AnyUp是一个通用的上采样方法，因此其性能可能会受到特定任务的影响。

相比之下，FeatUp是一种基于深度神经网络的图像上采样方法，可以通过在低分辨率图像上进行训练来生成高分辨率图像。FeatUp的优点是在某些情况下可以获得更好的图像质量，特别是在对细节要求较高的任务中。然而，FeatUp需要大量的计算资源和时间来进行训练，并且需要针对每个任务进行微调，因此其适用范围相对较窄。

总的来说，AnyUp和FeatUp都是有效的图像上采样方法，但它们的适用场景略有不同。如果需要在多个任务之间共享模型，则AnyUp可能是更好的选择；如果需要在特定任务中获得更好的图像质量，则FeatUp可能更适合。

AnyUp和FeatUp是两种不同的图像上采样方法，它们的原理有所不同。

AnyUp使用了一种基于卷积神经网络的方法，通过将低分辨率图像作为输入，在卷积层中逐步增加特征图的空间维度，最终得到高分辨率图像作为输出。具体来说，AnyUp使用了类似于图像放大（super-resolution）的技术，通过学习低分辨率图像到高分辨率图像之间的映射关系，从而实现图像上采样的目的。AnyUp还可以应用于多种不同的特征类型，并且可以用于各种下游任务，具有较高的灵活性和可扩展性。

FeatUp则是一种基于深度神经网络的图像上采样方法，它通过在低分辨率图像上进行训练来生成高分辨率图像。FeatUp使用了一个类似于编码器-解码器（encoder-decoder）的结构，其中编码器部分用于提取低分辨率图像中的特征，而解码器部分则用于将这些特征映射回高分辨率图像。FeatUp的优点是在某些情况下可以获得更好的图像质量，特别是在对细节要求较高的任务中。然而，FeatUp需要大量的计算资源和时间来进行训练，并且需要针对每个任务进行微调，因此其适用范围相对较小。

总的来说，AnyUp和FeatUp都是有效的图像上采样方法，但它们的原理和应用场景略有不同。

论文方法

该论文提出了一种轻量级、低参数化的模型来实现特征上采样，旨在防止内存和计算瓶颈。该模型基于注意力机制，并使用自定义的特征无关卷积层（feature-agnostic convolution layer）来处理来自不同源模型的不同维度的特征图。此外，他们还通过限制注意力计算范围在像素周围的局部窗口内来简化任务，从而进一步提高了效率。

方法改进

与之前的注意力机制上采样模型相比，该模型的主要改进在于采用了特征无关卷积层和局部窗口注意力计算策略。这些改进使得模型更加高效且具有更好的泛化能力。

解决的问题

该论文解决了如何设计一种轻量级、低参数化的模型来进行特征上采样的问题。同时，通过使用特征无关卷积层和局部窗口注意力计算策略，该模型也有效地解决了全局注意力机制可能导致的过拟合和计算复杂度高的问题。

论文实验

本文主要介绍了作者提出的特征插值方法AnyUp，并进行了多项对比实验来验证其性能和优越性。以下是每个对比实验的详细介绍：

质量比较实验（Sec. 5.1）：该实验主要与之前的工作进行比较，包括FeatuUp、LoftUp和JAFAR等方法。作者使用了COCO-Stuff、ADE20k和PASCAL VOC等数据集进行测试，并使用了像素准确率和交并比（IoU）作为评估指标。结果表明，AnyUp在语义分割任务中提供了最先进的上采样性能。

功能迁移实验（Sec. 5.2）：该实验旨在验证AnyUp是否能够适应不同的功能，并且不需要重新训练。作者使用了DINOv2 ViT-S模型进行训练和测试，并在不同分辨率下对输入图像进行上采样。结果表明，AnyUp具有很好的泛化能力，在不同分辨率下的上采样效果都很好。

特征空间保留实验（Sec. 5.3）：该实验旨在验证AnyUp是否能够在保持原始低分辨率特征分布的同时提高上采样的质量。作者使用了线性探针来检测特征空间的转移情况，并在深度估计和语义分割两个任务上进行了测试。结果表明，AnyUp能够很好地保留原始特征分布，并提高了上采样的质量。

其他实验（Sec. 5.4）：该实验包括了其他一些对比实验，如不同分辨率下的上采样效果、数据采样方式的影响以及信息流的去除对性能的影响等。这些实验进一步证明了AnyUp的有效性和优越性。

综上所述，通过多项对比实验，作者证明了AnyUp在特征插值方面具有很高的性能和优越性。


论文总结

该论文提出了一种名为AnyUp的方法，用于从任意分辨率到任意分辨率的特征上采样，并且能够应用于各种不同的特征类型和分辨率。该方法通过引入一个特征无关层来处理不同类型的特征，并使用窗口注意力和基于图像部分的损失训练模型以提高上采样质量。实验结果表明，AnyUp在各种下游任务中均取得了最先进的性能，并具有高度通用性和泛化能力。


该论文的主要贡献在于提出了一种通用的特征上采样方法，可以应用于任何类型的特征和分辨率。其主要创新点包括：

特征无关层：该层可以处理任何类型的特征，而不需要特定于某种特征的处理方式。

窗口注意力：该方法可以在保持输入特征空间的同时有效地训练模型。

基于图像部分的损失：该损失函数可以使模型更加关注重要的区域，从而提高上采样的质量。


该论文提出的AnyUp方法为特征上采样提供了一个新的解决方案，但仍然存在一些挑战需要解决。例如，如何更好地利用更大的数据集和更复杂的模型来进一步提高上采样的质量。此外，该方法也可以扩展到其他计算机视觉任务中，如图像分割和目标检测等。因此，在未来的研究中，我们将继续探索这些方向并改进现有的方法。

<a name="上采样与遥感"></a>  
## 上采样在遥感图像中的应用
**AnyUp和FeatUp这两种图像上采样方法都可以应用于遥感图像处理领域，包括遥感图像分割、检测、分类以及变化检测等任务。**

**对于遥感图像分割任务，这两种方法可以帮助提高分割精度和效率，特别是当原始图像分辨率较低或者存在噪声和遮挡等情况时。例如，可以通过将低分辨率的遥感图像上采样为高分辨率图像，然后利用深度学习模型进行像素级别的分类或语义分割，以获得更准确的结果。**

**对于遥感图像检测和分类任务，这两种方法也可以帮助提高识别率和鲁棒性。例如，可以通过将低分辨率的遥感图像上采样为高分辨率图像，然后利用深度学习模型进行目标检测或分类，以获得更高的准确性和可靠性。**

**对于遥感图像变化检测任务，这两种方法可以帮助捕捉地表的变化情况。例如，可以通过将历史遥感图像与当前遥感图像进行比较，利用图像上采样技术增强图像细节，然后利用深度学习模型进行变化检测，以发现地表的变化和演变趋势。**

**综上所述，AnyUp和FeatUp这两种图像上采样方法在遥感图像处理领域有着广泛的应用前景，可以帮助提高遥感图像处理的效率和准确性。**

<a name="SegEarth"></a>  
## SegEarth-OV （Understand 70%）
**框架流程**
![SegEarth-OV框架流程图](https://raw.githubusercontent.com/YanghuiSong/SYH_GoGoGo/main/UploadImage/graphviz%20-%202025-10-29T222844.294.png)


**方法描述**

该论文提出了一个名为SimFeatUp的方法，用于在远程感知图像上进行高质量的超分辨率重建。该方法基于CLIP模型中的图像编码器，并使用FeatuUp模型进行学习无监督的超分辨率重建。具体来说，该方法采用了以下三个关键步骤：

● 使用CLIP模型中图像编码器的最后一层作为输入，而不是传统的最后一层输出。

● 在FeatuUp模型中引入了内容保留网络（CRN），以确保重构的高分辨率图像与原始图像保持一致。

● 修改了FeatuUp模型中的卷积核大小，使其能够更好地适应远程感知图像的特点。

**方法改进**

与传统的FeatuUp模型相比，SimFeatUp模型具有以下两个主要改进：

● 更好的图像内容保留：通过添加内容保留网络，SimFeatUp模型可以确保重构的高分辨率图像与原始图像保持一致，从而提高了图像的质量。

● 更好的超分辨率重建效果：通过修改卷积核大小，SimFeatUp模型可以更好地适应远程感知图像的特点，从而实现了更好的超分辨率重建效果。

**解决的问题**

SimFeatUp方法的主要目的是为了解决远程感知图像上的高质量超分辨率重建问题。传统的FeatuUp模型虽然也可以实现超分辨率重建，但是在处理远程感知图像时存在一些局限性，例如图像内容保留不足、重建效果不佳等问题。因此，SimFeatUp方法通过对FeatuUp模型进行改进，解决了这些问题，实现了更好的超分辨率重建效果。


**论文实验**

本文主要介绍了作者在远程遥感图像场景下的多类语义分割和单类提取任务中所提出的SegEarth-OV方法，并与现有的自然图像OVSS模型进行了比较实验。具体来说，本文采用了以下对比实验：

● 对比了SegEarth-OV与其他5种训练免费的OVSS模型（包括vanilla CLIP、MaskCLIP、SCLIP、GEM和ClearCLIP）在8个远程遥感图像语义分割数据集上的表现，结果表明SegEarth-OV在所有数据集上都取得了最佳性能，相比其他方法平均提高了5.8%的mIoU分数。

● 在建筑提取任务中，将SegEarth-OV与其他4种训练免费的OVSS模型（包括CHN6-CUG、DeepGlobe、Massachusetts和SpaceNet）进行了比较实验，在四个数据集中SegEarth-OV的表现优于其他方法。

● 在道路提取任务中，将SegEarth-OV与其他4种训练免费的OVSS模型（包括iSAID、Potsdam、UAVid和VDD）进行了比较实验，但该任务的整体表现不如其他任务，可能是因为道路形状特殊或者标签生成方式不够精确等原因。

● 在洪水检测任务中，将SegEarth-OV与其他4种训练免费的OVSS模型（包括WHUAerial、WHUSat.II、Inria和xBED）进行了比较实验，结果表明SegEarth-OV在这项任务中的表现最好，提高了15.3%的IoU分数。

此外，文章还对SegEarth-OV的组成部分进行了详细的分析和评估，包括插件式模块SimFeatUp和全局偏差缓解等。同时，作者还对SimFeatUp进行了进一步的测试，将其应用于三个自然图像数据集（PASCAL Context、COCOStuff和Cityscapes），证明了其在不同领域的通用性。最后，作者还评估了几种基于遥感数据的CLIP模型在遥感图像上的表现，发现SegEarth-OV的效果优于这些模型。

**论文总结**

文章优点

本文提出了一种针对遥感图像的无监督视觉语言模型（OVSS）方法——SegEarth-OV。该方法通过使用SimFeatUp特征上采样器和全局偏移消除技术来提高遥感图像上的性能，并在17个遥感数据集上实现了显著的改进。此外，SegEarth-OV是第一个探索训练免费OVSS方法的遥感场景，证明了即使预训练在自然图像上的视觉语言模型也可以用于地球感知任务。

方法创新点

本文提出了两个关键思想：SimFeatUp和全局偏移消除。其中，SimFeatUp是一种通用的特征上采样器，可以在几个未标记的图像上训练以重建内容不变的高分辨率特征。全局偏移消除则是一种简单而直接的方法，可以通过执行局部和全局令牌之间的减法操作来有效地减少CLIP中的全局属性对局部特征的影响。

**一些细节问题**

CLS是CLIP模型中的特殊token，它在训练阶段被优化以包含整个图像的全局信息，并在推断阶段用于计算文本嵌入与图像特征之间的相似度。在本文中，CLS的作用是在训练阶段对整个图像进行优化，但在推断阶段却只关注局部区域，导致了“全局偏倚”的问题。因此，本文提出了一种简单的减法操作来消除这种全局偏倚。

在本文中，他们通过观察到patch tokens对[CLS] token的异常响应来提出了一种简单的方法来减轻全局偏差。具体来说，他们在执行任何任务之前，从输入图像中提取了所有可能的局部区域，并计算每个区域的[CLS] token表示。然后，他们使用这些局部[CLS] token表示来减去全局[CLS] token表示，从而得到一个局部偏移量。最后，他们将这个局部偏移量应用于所有的patch tokens，以减轻全局偏差的影响。这种方法可以在训练过程中自动学习，不需要手动调整超参数或设计复杂的网络结构。

自注意力机制（Self-Attention）是一种用于序列数据的神经网络模型，它能够对输入序列中的每个元素与其他所有元素进行交互，并根据这些交互结果计算出每个元素的重要性权重。自注意力机制在自然语言处理、语音识别等领域有广泛应用。

而自自注意力机制（Self-Self Attention）是自注意力机制的一种变体，它是在同一层中同时应用了两次自注意力机制，即先通过一次自注意力机制计算出每个元素的重要性和其与其他元素的关系，然后再将得到的结果作为输入再次进行自注意力机制计算，以此来增强模型对输入序列的理解能力。自自注意力机制已经在图像分类、目标检测等计算机视觉任务中取得了很好的效果。

在CLIP模型中，每个局部视觉标记都会关注整个图像中的许多位置，并且它们之间的注意力图案通常是相似的。这意味着在CLIP模型中，一些全局属性会附着在局部标记上。但在远程感测等应用中，这些全局属性可能会影响模型对局部特征变化的准确性，从而降低模型性能。这是因为当模型在处理具有复杂结构的对象时，例如建筑物或道路，它可能会受到全局属性的影响，导致预测结果不准确。因此，需要解决这个全局属性带来的问题，以提高模型在远程感测等领域的表现。

当模型在处理具有复杂结构的对象时，例如建筑物或道路，它可能会受到全局属性的影响，导致预测结果不准确。这是因为全局属性可能会影响模型对局部特征变化的准确性，从而降低模型性能。例如，在处理具有复杂结构的建筑物时，模型可能会将某些区域错误地识别为其他类别，因为其受到全局属性的影响而无法正确区分不同的局部特征。


减去全局偏置提升性能的原因

● 减少干扰：通过减去全局偏差，可以减少全局信息对局部特征的干扰，使得局部图像块标记更专注于自身的局部特征信息。这样在后续基于局部特征的任务（如目标检测、图像分割等）中，模型能够更好地捕捉和利用真正的局部细节，而不是被全局信息所主导。
● 增强特征区分度：去除全局偏差后，不同图像块之间的特征差异可能会更加明显，有助于提高模型对不同局部区域的区分能力，进而提升整体性能。例如，在目标检测任务中，能够更准确地定位和识别不同的目标物体，而不会因为全局信息的干扰导致误判或漏判。
