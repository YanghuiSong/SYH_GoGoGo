# SYH_GoGoGo
The following is the code for reproducing recently read papers and the work currently in progress.
# Content
1. [[Pilot Study](#PilotStudy)]
2. [[Remote Sensing](#Remote_Sensing)]
3. [[Classification](#classification)]
4. [[Segmentation](#segmentation)]    
       
-----------------------------------------------------------------------------------------------
<a name="PilotStudy"></a>  
## Pilot Study
1. [2024 ICLR] **FeatUp: A Model-Agnostic Framework for Features at Any Resolution** [[paper]](https://openreview.net/pdf?id=GkJiNn2QDF) [[code]](https://github.com/mhamilton723/FeatUp)[[Notes](#FeatUpLearning)]   
2. [2025 CVPR] **SegEarth-OV: Towards Training-Free Open-Vocabulary Segmentation for Remote Sensing Images** [[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images_CVPR_2025_paper.pdf) [[code]](https://github.com/likyoo/SegEarth-OV)
3. [2025 NIPS] **InstructSAM: A Training-Free Framework for Instruction-Oriented Remote Sensing Object Recognition** [[paper]](https://arxiv.org/pdf/2505.15818) [[code]](https://github.com/VoyagerXvoyagerx/InstructSAM?tab=readme-ov-file)
4. [2025 arXiv] **AnyUp: Universal Feature Upsampling** [[paper]](https://arxiv.org/abs/2510.12764) [[code]](https://github.com/wimmerth/anyup)[[Notes](#AnyUp)] 
5. [2025 ICME] **LG-CD: Enhancing Language-Guided Change Detection through SAM2 Adaptation** [[paper]](https://arxiv.org/pdf/2509.21894)

<a name="Remote_Sensing"></a>  
## Remote Sensing
1. [2025 arXiv] **DynamicEarth: How Far are We from Open-Vocabulary Change Detection?** [[paper]](https://arXiv.org/abs/2501.12931) [[code]](https://github.com/likyoo/DynamicEarth)
2. [2025 TGRS] **A Unified Framework With Multimodal Fine-Tuning for Remote Sensing Semantic Segmentation.** [[paper]](https://ieeexplore.ieee.org/document/11063320) [[code]](https://github.com/sstary/SSRS)
3. [2025 ICASSP] **Enhancing Remote Sensing Vision-Language Models for Zero-Shot Scene Classification.** [[paper]](https://arXiv.org/abs/2409.00698) [[code]](https://github.com/elkhouryk/RS-TransCLIP)
4. [2025 ICCV] **https://github.com/mburges-cvl/ICCV_AL4FM.** [[paper]](https://openaccess.thecvf.com/content/ICCV2025/papers/Burges_Active_Learning_Meets_Foundation_Models_Fast_Remote_Sensing_Data_Annotation_ICCV_2025_paper.pdf) [[code]](https://github.com/mburges-cvl/ICCV_AL4FM)
5. [2025 ICCV] **Dynamic Dictionary Learning for Remote Sensing Image Segmentation.** [[paper]](https://arXiv.org/pdf/2503.06683) [[code]](https://github.com/XavierJiezou/D2LS)
6. [2025 ICCV] **GEOBench-VLM: Benchmarking Vision-Language Models for Geospatial Tasks.** [[paper]](https://arxiv.org/pdf/2411.19325) [[code]](https://github.com/The-AI-Alliance/GEO-Bench-VLM)
7. [2025 ICCV] **SCORE: Scene Context Matters in Open-Vocabulary Remote Sensing Instance Segmentation.** [[paper]](https://arXiv.org/abs/2507.12857) [[code]](https://github.com/HuangShiqi128/SCORE)
8. [2025 ICCV] **When Large Vision-Language Model Meets Large Remote Sensing Imagery: Coarse-to-Fine Text-Guided Token Pruning.** [[paper]](https://arXiv.org/pdf/2503.07588) [[code]](https://github.com/VisionXLab/LRS-VQA)
9. [2025 ICCV] **SMARTIES: Spectrum-Aware Multi-Sensor Auto-Encoder for Remote Sensing Images.** [[paper]](https://openaccess.thecvf.com/content/ICCV2025/papers/Sumbul_SMARTIES_Spectrum-Aware_Multi-Sensor_Auto-Encoder_for_Remote_Sensing_Images_ICCV_2025_paper.pdf) [[code]](https://github.com/gsumbul/SMARTIES)
10. [2025 ICCV] **Continuous Remote Sensing Image Super-Resolution via Neural Operator Diffusion** [[paper]](https://openaccess.thecvf.com/content/ICCV2025/papers/Xu_NeurOp-Diff_Continuous_Remote_Sensing_Image_Super-Resolution_via_Neural_Operator_Diffusion_ICCV_2025_paper.pdf) [[code]](https://github.com/zerono000/NeurOp-Diff)
11. [2025 ICCV] **HoliTracer: Holistic Vectorization of Geographic Objects from Large-Size Remote Sensing Imagery.** [[paper]](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_HoliTracer_Holistic_Vectorization_of_Geographic_Objects_from_Large-Size_Remote_Sensing_ICCV_2025_paper.pdf) [[code]](https://github.com/vvangfaye/HoliTracer)
12. [2025 arXiv] **SAR-KnowLIP: Towards Multimodal Foundation Models for Remote Sensing.** [[paper]](https://arxiv.org/pdf/2509.23927) [[code]](https://github.com/yangyifremad/SARKnowLIP)
13. [2025 AAAI] **ZoRI: Towards discriminative zero-shot remote sensing instance segmentation.** [[paper]](https://arXiv.org/abs/2412.12798) [[code]](https://github.com/HuangShiqi128/ZoRI)
14. [2024 NIPS] **Segment Any Change.** [[paper]](https://proceedings.NIPS.cc/paper_files/paper/2024/file/9415416201aa201902d1743c7e65787b-Paper-Conference.pdf) [[code]](https://github.com/Z-Zheng/pytorch-change-models)
15. [2025 CVPR] **SegEarth-OV: Towards Training-Free Open-Vocabulary Segmentation for Remote Sensing Images.** [[paper]](https://arXiv.org/abs/2410.01768) [[code]](https://github.com/likyoo/SegEarth-OV)
16. [2025 CVPR] **XLRS-Bench: Could Your Multimodal LLMs Understand Extremely Large Ultra-High-Resolution Remote Sensing Imagery?** [[paper]](https://arXiv.org/abs/2503.23771) [[code]](https://github.com/EvolvingLMMs-Lab/XLRS-Bench)
17. [2025 CVPR] **Exact: Exploring Space-Time Perceptive Clues for Weakly Supervised Satellite Image Time Series Semantic Segmentation.** [[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image_CVPR_2025_paper.pdf) [[code]](https://github.com/MiSsU-HH/Exact)
18. [2025 Arxiv] **SegEarth-OV-2: Annotation-Free Open-Vocabulary Segmentation for Remote-Sensing Images** [[paper]](https://arxiv.org/abs/2508.18067)  [[code]](https://github.com/earth-insights/SegEarth-OV-2)
19. [2025 AAAI] **Towards Open-Vocabulary Remote Sensing Image Semantic Segmentation** [[paper]](https://arxiv.org/abs/2412.19492) [[code]](https://github.com/yecy749/GSNet)
20. [2025 Arxiv] **InstructSAM: A Training-Free Framework for Instruction-Oriented Remote Sensing Object Recognition** [[paper]](https://arxiv.org/pdf/2505.15818) [[code]](https://github.com/VoyagerXvoyagerx/InstructSAM)
21. [2025 Arxiv] **DescribeEarth: Describe Anything for Remote Sensing Images** [[paper]](https://arxiv.org/pdf/2509.25654v1) [[code]](https://github.com/earth-insights/DescribeEarth)
22. [2025 NIPS] **GTPBD: A Fine-Grained Global Terraced Parcel and Boundary Dataset** [[paper]](https://arxiv.org/abs/2507.14697) [[code]](https://github.com/Z-ZW-WXQ/GTPBD)
23. [2025 Arxiv] **RS3DBench: A Comprehensive Benchmark for 3D Spatial Perception in Remote Sensing** [[paper]](https://arxiv.org/abs/2509.18897) [[code]](https://rs3dbench.github.io)
24. [2025 Arxiv] **DGL-RSIS: Decoupling Global Spatial Context and Local Class Semantics for Training-Free Remote Sensing Image Segmentation** [[paper]](https://arxiv.org/pdf/2509.00598) [[code]](https://github.com/designer1024/DGL-RSIS)
25. [2025 TGRS] **A Unified SAM-Guided Self-Prompt Learning Framework for Infrared Small Target Detection** [[paper]](https://ieeexplore.ieee.org/document/11172325) [[code]](https://github.com/fuyimin96/SAM-SPL)
26. [2025 TGRS] **Semantic Prototyping With CLIP for Few-Shot Object Detection in Remote Sensing Images** [[paper]](https://ieeexplore.ieee.org/document/10930588)
27. [2025 Arxiv] **ATRNet-STAR: A Large Dataset and Benchmark Towards Remote Sensing Object Recognition in the Wild** [[paper]](https://arxiv.org/abs/2501.13354) [[code]](https://github.com/waterdisappear/ATRNet-STAR)
28. [2025 Arxiv] **RSKT-Seg: Exploring Efficient Open-Vocabulary Segmentation in the Remote Sensing** [[paper]](https://arxiv.org/pdf/2509.12040) [[code]](https://github.com/LiBingyu01/RSKT-Seg)
29. [2025 ISPRS]  **AdaptVFMs-RSCD: Advancing Remote Sensing Change Detection from binary to semantic with SAM and CLIP** [[paper]](https://doi.org/10.1016/j.isprsjprs.2025.09.010) [[data]](https://github.com/Jiang-CHD-YunNan/RS-VFMs-Fine-tuning-Dataset)
30. **PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient Fine-Tuning for Remote Sensing Change Detection** [[paper]](https://arxiv.org/pdf/2509.09572) [[code]](https://github.com/dyzy41/PeftCD)
31. [2025 Arxiv] **AlignCLIP: Self-Guided Alignment for Remote Sensing Open-Vocabulary Semantic Segmentation** [[paper]](https://openreview.net/forum?id=hpD3tn7Xbp) [[code]](https://openreview.net/attachment?id=hpD3tn7Xbp&name=supplementary_material)
32. [2025 Arxiv] **Few-Shot Adaptation Benchmark for Remote Sensing Vision-Language Models** [[paper]](https://arxiv.org/pdf/2510.07135) [[code]](https://github.com/elkhouryk/fewshot_RSVLMs)
33. [2025 RSE] **Strategic sampling for training a semantic segmentation model in operational mapping: Case studies on cropland parcel extraction** [[paper]](https://doi.org/10.1016/j.rse.2025.115034) [[data]](https://doi.org/10.5281/zenodo.16595511) [[code]](https://github.com/Remote-Sensing-of-Land-Resource-Lab/Training-Sample-Selection)
34. [2025 TIP] **Universal Fine-Grained Visual Categorization by Concept Guided Learning** [[paper]](https://ieeexplore.ieee.org/document/10829548) [[data]](https://drive.google.com/file/d/11hYbdO32hyspucDKp5wwjwvCaD38AEKe/view?usp=sharing) [[code]](https://github.com/BiQiWHU/CGL)
35. [2025 TIP] **SARATR-X: Towards Building A Foundation Model for SAR Target Recognition** [[paper]](https://ieeexplore.ieee.org/document/10856784) [[code]](https://github.com/waterdisappear/SARATR-X)
36. [2025 TIP] **HSLabeling: Towards Efficient Labeling for Large-scale Remote Sensing Image Segmentation with Hybrid Sparse Labeling** [[paper]](https://ieeexplore.ieee.org/document/10829548) [[data]](https://drive.google.com/drive/folders/1CiYzJyBn1rV-xsrsYQ6o2HDQjdfnadHl) [[code]](https://github.com/linjiaxing99/HSLabeling)
37. [2025 CVM] **Remote sensing tuning: A survey** [[paper]](https://ieeexplore.ieee.org/document/11119145) [[code]](https://github.com/DongshuoYin/Remote-Sensing-Tuning-A-Survey/tree/main)

<a name="classification"></a>  
## Classification
...
<a name="segmentation"></a>  
## Segmentation
...

<a name="FeatUpLearning"></a>  
## FeatUp （Understand 40%→60%）
**FeatUp的核心思想与方法**

FeatUp的核心灵感来自NeRF的多视图一致性原理：通过观察同一图像经微小变换（如裁剪、翻转、缩放）后的多个低分辨率特征视图，学习高分辨率特征的空间一致性。

本文介绍了一个名为 FEATUP 的框架，它可以帮助深度学习模型恢复低分辨率特征的空间信息，从而提高模型在一些需要高分辨率特征的任务（如语义分割和深度估计）上的性能表现。这个框架有两个版本，一个是通过一次前向传播来引导特征，另一个则是通过拟合一个隐含模型来重建任意分辨率的特征。这两个方法都使用了多视一致性损失，并且可以直接替换到现有的应用程序中，而不需要重新训练模型。

在遥感图像中，由于模型的池化操作导致特征图分辨率降低，从而影响了下游任务的表现。而通过使用 FEATUP 技术，可以对深度特征进行上采样，增加其空间分辨率，使得它们能够更好地应用于密集预测任务，如语义分割和深度估计等。因此，FEATUP 技术可以在遥感图像处理中提高特征的空间分辨率，并改善下游任务的性能表现。

具体包括以下关键设计：

1. 多视图一致性损失（核心监督信号）
   
对输入图像施加随机微小变换（如填充、缩放、水平翻转），得到多个“抖动”版本的低分辨率特征。FeatUp学习一个高分辨率特征图，使其经下采样后能匹配所有抖动视图的低分辨率特征，通过高斯似然损失（含自适应不确定性）监督这一过程，确保高分辨率特征的空间一致性。

多视一致性损失是一种用于训练深度学习模型的技术，在多个视角下观察同一场景并计算它们之间的差异，以此来增加模型对不同视角下的数据的鲁棒性和泛化能力。在本文中，多视一致性损失被用来训练 FeatUp 框架中的两个版本，即引导特征和重建特征的方法。具体来说，多视一致性损失是通过比较模型输出的低分辨率特征和经过多次变换后的高分辨率特征来计算的，这些变换包括图像的平移、缩放和翻转等操作。通过最小化这种差异，模型可以更好地适应不同的输入数据，并生成更准确的结果。

与对比学习相比，多视一致性损失的主要区别在于它不是通过直接比较不同样本之间的相似性来进行训练的，而是通过对同一场景的不同视角进行比较来提高模型的鲁棒性和泛化能力。相比之下，对比学习更加注重对不同类别之间的区分度的训练，通常需要大量的负样本来表示不同的类别。因此，虽然两种技术都是基于比较不同样本之间的相似性的思想，但其应用场景和目的略有不同。

2. 两种下采样器（模拟模型池化行为）
   
为匹配不同模型的特征降采样机制，设计两种下采样器：

简单下采样器：学习非负归一化模糊核，通过卷积实现特征平滑下采样，适用于固定感受野模型（如CNN）。

注意力下采样器：通过1×1卷积预测显著性图，动态调整下采样核权重，适应动态感受野或对象显著性（如ViT的 patch 注意力机制）。

在FeatuP中，有两种下采样器被设计出来来匹配不同模型的特征降采样机制。第一种是简单下采样器，它通过学习非负归一化模糊核并利用卷积操作对特征进行平滑下采样。这种方法适用于固定感受野模型，比如CNN等。第二种是注意力下采样器，它通过1×1卷积预测显著性图，动态地调整下采样核权重，以适应动态感受野或对象显著性。这种方法适用于具有动态感受野或对象显著性的模型，例如ViT中的patch注意力机制。这两种下采样器都能够有效地降低特征的空间分辨率，从而提高模型的计算效率，并且不会影响特征的语义信息。

传统的池化方法会将输入图像的空间分辨率降低，从而减少计算量并提高模型的鲁棒性。而FeatuUp则通过增加空间分辨率来恢复深度特征中的丢失信息，从而提高了下游任务的性能。因此，与传统的池化方法相比，FeatuUp能够更好地保留原始图像的语义信息，并且可以在不重新训练的情况下直接应用于现有的应用程序中。

3. 两种上采样器（核心创新）
   
FeatUp提供两种即插即用的上采样变体，可直接替换现有特征：

JBU FeatUp（通用前向传播上采样）
基于联合双边滤波（JBU） 的改进，通过堆叠参数化JBU层，利用输入图像的高分辨率信号引导特征上采样。关键优化：

设计高效CUDA内核，比标准PyTorch实现快10倍、内存占用低2个数量级；

用MLP替代传统JBU的固定高斯核，学习特征与图像高频细节的关联，保留语义的同时恢复边缘信息。

Implicit FeatUp（单图像隐式上采样）

过拟合一个小型隐式网络到单图像特征，通过傅里叶特征编码（含颜色信息）实现任意分辨率的特征重建。优势：

参数仅为显式特征存储的1/100，支持超高分辨率输出；

结合总变差正则化避免噪声，适合需要精细细节的场景。

第一种是基于联合双边滤波（JBU）的改进，称为JBU FeatUp。它通过堆叠参数化的JBU层，利用输入图像的高分辨率信号来引导特征上采样。具体来说，它采用了一个高效的CUDA内核，比标准PyTorch实现快10倍，内存占用也低了两个数量级。此外，它还用多层感知机（MLP）代替了传统JBU的固定高斯核，以便学习特征与图像高频细节之间的关系，同时保留语义信息并恢复边缘信息。JBU的原理是利用高分辨率信号作为指导，通过一种叫做双边滤波的技术来对低分辨率图像进行平滑处理，并在保持图像边缘清晰的情况下提高其分辨率。具体来说，它会先定义一个高斯核函数来计算每个像素点周围的邻域内像素值的加权平均值，然后根据这个平均值来更新当前像素点的值。这样就可以实现对图像的平滑处理和分辨率提升。同时，由于双边滤波还考虑了像素之间的距离和颜色差异等因素，因此可以有效地保留图像中的细节和纹理信息。


第二种是Implicit FeatUp，它是通过过拟合一个小型隐式网络到单张图像的特征来进行任意分辨率的特征重建。这种方法的优势在于，它的参数仅为显式特征存储的1/100，可以支持超高的分辨率输出。此外，它还结合了总变差正则化，以避免噪声，并适用于需要精细细节的场景。

mplicit FeatUp是一种通过训练一个小型隐式网络来实现任意分辨率特征重建的方法。它的基本思想是在给定一张输入图像及其对应的低分辨率特征图时，训练一个小型神经网络来预测该图像的高分辨率特征图。这个神经网络通常由多个卷积层和反卷积层组成，其中卷积层用于提取图像的局部特征，而反卷积层则用于将这些局部特征组合成高分辨率的全局特征。在训练过程中，神经网络的目标是最小化预测的高分辨率特征图与真实高分辨率特征图之间的差异。最终得到的隐式网络可以用来对任意图像的特征进行重建，从而实现任意分辨率的特征映射。
特征映射是指将原始数据（如图像）中的每个像素点表示为一个向量，并将这些向量组织成一个矩阵的过程。在这个矩阵中，每一行代表了一个像素点的向量表示，每一列则代表了不同的特征。通过这种方式，我们可以将图像转换为一个更易于处理的形式，以便于后续的计算机视觉任务，例如分类、检测、分割等。

关键贡献

模型无关框架：适用于任意视觉 backbone（CNN、ViT、自监督模型如DINO等），无需修改原模型结构。

高效JBU实现：提出首个高效CUDA版联合双边滤波，解决传统JBU计算瓶颈，支持大规模模型部署。

即插即用提升：上采样特征可直接替换现有特征，在不重新训练下游模型的情况下提升性能（如分割mIoU、深度估计精度）。



<a name="AnyUp"></a>  
## AnyUp （Understand 10%）
**AnyUp的核心思想与方法**

这篇论文介绍了一种名为AnyUp的方法，用于对任何视觉特征在任意分辨率下进行上采样。现有的学习型上采样器需要针对每个特征提取器重新训练，而AnyUp则不需要，因此可以在不同特征类型之间通用，并且可以应用于各种下游任务。该方法采用一种推理时的特征非特定性上采样架构来提高上采样的质量，并在实验中达到了新的最佳水平。

论文方法

该论文提出了一种轻量级、低参数化的模型来实现特征上采样，旨在防止内存和计算瓶颈。该模型基于注意力机制，并使用自定义的特征无关卷积层（feature-agnostic convolution layer）来处理来自不同源模型的不同维度的特征图。此外，他们还通过限制注意力计算范围在像素周围的局部窗口内来简化任务，从而进一步提高了效率。

方法改进

与之前的注意力机制上采样模型相比，该模型的主要改进在于采用了特征无关卷积层和局部窗口注意力计算策略。这些改进使得模型更加高效且具有更好的泛化能力。

解决的问题

该论文解决了如何设计一种轻量级、低参数化的模型来进行特征上采样的问题。同时，通过使用特征无关卷积层和局部窗口注意力计算策略，该模型也有效地解决了全局注意力机制可能导致的过拟合和计算复杂度高的问题。

论文实验

本文主要介绍了作者提出的特征插值方法AnyUp，并进行了多项对比实验来验证其性能和优越性。以下是每个对比实验的详细介绍：

质量比较实验（Sec. 5.1）：该实验主要与之前的工作进行比较，包括FeatuUp、LoftUp和JAFAR等方法。作者使用了COCO-Stuff、ADE20k和PASCAL VOC等数据集进行测试，并使用了像素准确率和交并比（IoU）作为评估指标。结果表明，AnyUp在语义分割任务中提供了最先进的上采样性能。

功能迁移实验（Sec. 5.2）：该实验旨在验证AnyUp是否能够适应不同的功能，并且不需要重新训练。作者使用了DINOv2 ViT-S模型进行训练和测试，并在不同分辨率下对输入图像进行上采样。结果表明，AnyUp具有很好的泛化能力，在不同分辨率下的上采样效果都很好。

特征空间保留实验（Sec. 5.3）：该实验旨在验证AnyUp是否能够在保持原始低分辨率特征分布的同时提高上采样的质量。作者使用了线性探针来检测特征空间的转移情况，并在深度估计和语义分割两个任务上进行了测试。结果表明，AnyUp能够很好地保留原始特征分布，并提高了上采样的质量。

其他实验（Sec. 5.4）：该实验包括了其他一些对比实验，如不同分辨率下的上采样效果、数据采样方式的影响以及信息流的去除对性能的影响等。这些实验进一步证明了AnyUp的有效性和优越性。

综上所述，通过多项对比实验，作者证明了AnyUp在特征插值方面具有很高的性能和优越性。


论文总结

该论文提出了一种名为AnyUp的方法，用于从任意分辨率到任意分辨率的特征上采样，并且能够应用于各种不同的特征类型和分辨率。该方法通过引入一个特征无关层来处理不同类型的特征，并使用窗口注意力和基于图像部分的损失训练模型以提高上采样质量。实验结果表明，AnyUp在各种下游任务中均取得了最先进的性能，并具有高度通用性和泛化能力。


该论文的主要贡献在于提出了一种通用的特征上采样方法，可以应用于任何类型的特征和分辨率。其主要创新点包括：

特征无关层：该层可以处理任何类型的特征，而不需要特定于某种特征的处理方式。

窗口注意力：该方法可以在保持输入特征空间的同时有效地训练模型。

基于图像部分的损失：该损失函数可以使模型更加关注重要的区域，从而提高上采样的质量。


该论文提出的AnyUp方法为特征上采样提供了一个新的解决方案，但仍然存在一些挑战需要解决。例如，如何更好地利用更大的数据集和更复杂的模型来进一步提高上采样的质量。此外，该方法也可以扩展到其他计算机视觉任务中，如图像分割和目标检测等。因此，在未来的研究中，我们将继续探索这些方向并改进现有的方法。
